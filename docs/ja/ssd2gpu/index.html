<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="PG-Strom Development Team" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>GPUダイレクトSQL - PG-Strom Manual</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="//fonts.googleapis.com/earlyaccess/notosansjp.css" rel="stylesheet" />
        <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet" />
        <link href="../custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL";
        var mkdocs_page_input_path = "ssd2gpu.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> PG-Strom Manual
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
  [<strong>Japanese</strong> | <a href="../../ssd2gpu/"    style="color: #cccccc">English</a>]
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">はじめに</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../install/">インストール</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">利用ガイド</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../operations/">基本的な操作</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../brin/">BRINインデックス</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../partition/">パーティション</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../postgis/">GPU版PostGIS</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../troubles/">トラブルシューティング</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">先進機能</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">GPUダイレクトSQL</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#_1">概要</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_2">初期設定</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#_3">ドライバのインストール</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_4">テーブルスペースの設計</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_5">運用</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#gpunvme-ssd">GPUとNVME-SSD間の距離</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#guc">GUCパラメータによる制御</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#gpusql_1">GPUダイレクトSQL実行の利用を確認する</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#visibility-map">Visibility Mapに関する注意事項</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../arrow_fdw/">Apache Arrow</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../gpucache/">GPUキャッシュ</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../fluentd/">Fluentd連携</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">リファレンス</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_types/">データ型</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_devfuncs/">関数と演算子</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_sqlfuncs/">SQLオブジェクト</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_params/">GUCパラメータ</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">リリースノート</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.2/">PG-Strom v5.2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.1/">PG-Strom v5.1</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.0/">PG-Strom v5.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v3.0/">PG-Strom v3.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.3/">PG-Strom v2.3</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.2/">PG-Strom v2.2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.0/">PG-Strom v2.0</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">PG-Strom Manual</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">先進機能</li>
      <li class="breadcrumb-item active">GPUダイレクトSQL</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="gpusql">GPUダイレクトSQL実行</h1>
<h2 id="_1">概要</h2>
<p>SQLワークロードを高速に処理するには、プロセッサが効率よく処理を行うのと同様に、ストレージやメモリからプロセッサへ高速にデータを供給する事が重要です。処理すべきデータがプロセッサに届いていなければ、プロセッサは手持ち無沙汰になってしまいます。</p>
<p>GPUダイレクトSQL実行機能は、PCIeバスに直結する事で高速なI/O処理を実現するNVMe-SSDと、同じPCIeバス上に接続されたGPUをダイレクトに接続し、ハードウェア限界に近い速度でデータをプロセッサに供給する事でSQLワークロードを高速に処理するための機能です。</p>
<p>通常、ストレージ上に格納されたPostgreSQLデータブロックは、PCIeバスを通していったんCPU/RAMへとロードされます。その後、クエリ実行計画にしたがってWHERE句によるフィルタリングやJOIN/GROUP BYといった処理を行うわけですが、集計系ワークロードの特性上、入力するデータ件数より出力するデータ件数の方がはるかに少ない件数となります。例えば数十億行を読み出した結果をGROUP BYで集約した結果が高々数百行という事も珍しくありません。</p>
<p>言い換えれば、我々はゴミデータを運ぶためにPCIeバス上の帯域を消費しているとも言えますが、CPUがレコードの中身を調べるまでは、その要不要を判断できないため、一般的な実装ではこれは不可避と言えます。</p>
<p><img alt="SSD2GPU Direct SQL Execution Overview" src="../img/ssd2gpu-overview.png" /></p>
<p>GPUダイレクトSQL実行はデータの流れを変え、ストレージ上のデータブロックをPCIeバス上のP2P DMAを用いてGPUに直接転送し、GPUでSQLワークロードを処理する事でCPUが処理すべきレコード数を減らすための機能です。いわば、ストレージとCPU/RAMの間に位置してSQLを処理するためのプリプロセッサとしてGPUを活用し、結果としてI/O処理を高速化するためのアプローチです。</p>
<p>本機能は、内部的にNVIDIA GPUDirect Storageモジュール（<code>nvidia-fs</code>）を使用して、GPUデバイスメモリとNVMEストレージとの間でP2Pのデータ転送を行います。
したがって、本機能を利用するには、PostgreSQLの拡張モジュールであるPG-Stromだけではなく、上記のLinux kernelモジュールが必要です。</p>
<p>また、本機能が対応しているのはNVME仕様のSSDや、NVME-oFで接続されたリモートデバイスのみです。
SASやSATAといったインターフェースで接続された旧式のストレージには対応していません。
今までに動作実績のあるNVME-SSDについては <a href="https://github.com/heterodb/pg-strom/wiki/002:-HW-Validation-List#nvme-ssd-validation-list">002: HW Validation List</a> が参考になるでしょう。</p>
<h2 id="_2">初期設定</h2>
<h3 id="_3">ドライバのインストール</h3>
<p>以前のPG-Stromでは、GPUダイレクトSQLの利用にはHeteroDB社の開発した独自のLinux kernelドライバが必要でしたが、v3.0以降ではNVIDIAの提供するGPUDirect Storageを利用するように設計を変更しています。GPUDirect Storage用のLinux kernelドライバ（<code>nvidia-fs</code>）はCUDA Toolkitのインストールプロセスに統合され、本マニュアルの「インストール」の章に記載の手順でシステムをセットアップした場合、特に追加の設定は必要ではありません。</p>
<p>必要なLinux kernelドライバがインストールされているかどうか、<code>modinfo</code>コマンドや<code>lsmod</code>コマンドを利用して確認する事ができます。</p>
<pre><code>$ modinfo nvidia-fs
filename:       /lib/modules/5.14.0-427.18.1.el9_4.x86_64/extra/nvidia-fs.ko.xz
description:    NVIDIA GPUDirect Storage
license:        GPL v2
version:        2.20.5
rhelversion:    9.4
srcversion:     096A726CAEC0A059E24049E
depends:
retpoline:      Y
name:           nvidia_fs
vermagic:       5.14.0-427.18.1.el9_4.x86_64 SMP preempt mod_unload modversions
sig_id:         PKCS#7
signer:         DKMS module signing key
sig_key:        18:B4:AE:27:B8:7D:74:4F:C2:27:68:2A:EB:E0:6A:F0:84:B2:94:EE
sig_hashalgo:   sha512
   :              :

$ lsmod | grep nvidia
nvidia_fs             323584  32
nvidia_uvm           6877184  4
nvidia               8822784  43 nvidia_uvm,nvidia_fs
drm                   741376  2 drm_kms_helper,nvidia
</code></pre>
<h3 id="_4">テーブルスペースの設計</h3>
<p>GPUダイレクトSQL実行は以下の条件で発動します。</p>
<ul>
<li>スキャン対象のテーブルがNVMe-SSDで構成された区画に配置されている。<ul>
<li><code>/dev/nvmeXXXX</code>ブロックデバイス、または<code>/dev/nvmeXXXX</code>ブロックデバイスのみから構成されたmd-raid0区画が対象です。</li>
</ul>
</li>
<li>テーブルサイズが<code>pg_strom.gpudirect_threshold</code>よりも大きい事。<ul>
<li>この設定値は任意に変更可能ですが、デフォルト値は本体搭載物理メモリに<code>shared_buffers</code>の設定値の1/3を加えた大きさです。</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>md-raid0を用いて複数のNVMe-SSD区画からストライピング読出しを行うには、HeteroDB社の提供するエンタープライズサブスクリプションの適用が必要です。</p>
</div>
<p>テーブルをNVMe-SSDで構成された区画に配置するには、データベースクラスタ全体をNVMe-SSDボリュームに格納する以外にも、PostgreSQLのテーブルスペース機能を用いて特定のテーブルや特定のデータベースのみをNVMe-SSDボリュームに配置する事ができます。</p>
<p>例えば <code>/opt/nvme</code> にNVMe-SSDボリュームがマウントされている場合、以下のようにテーブルスペースを作成する事ができます。
PostgreSQLのサーバプロセスの権限で当該ディレクトリ配下のファイルを読み書きできるようパーミッションが設定されている必要がある事に留意してください。</p>
<pre><code>CREATE TABLESPACE my_nvme LOCATION '/opt/nvme';
</code></pre>
<p>このテーブルスペース上にテーブルを作成するには、<code>CREATE TABLE</code>構文で以下のように指定します。</p>
<pre><code>CREATE TABLE my_table (...) TABLESPACE my_nvme;
</code></pre>
<p>あるいは、データベースのデフォルトテーブルスペースを変更するには、<code>ALTER DATABASE</code>構文で以下のように指定します。
この場合、既存テーブルの配置されたテーブルスペースは変更されない事に留意してください。</p>
<pre><code>ALTER DATABASE my_database SET TABLESPACE my_nvme;
</code></pre>
<h2 id="_5">運用</h2>
<h3 id="gpunvme-ssd">GPUとNVME-SSD間の距離</h3>
<p>サーバの選定とGPUおよびNVME-SSDの搭載にあたり、デバイスの持つ性能を最大限に引き出すには、デバイス間の距離を意識したコンフィグが必要です。</p>
<p>GPUダイレクトSQL機能がその基盤として使用している<a href="https://docs.nvidia.com/cuda/gpudirect-rdma/">NVIDIA GPUDirect RDMA</a>は、P2P DMAを実行するには互いのデバイスが同じPCIe root complexの配下に接続されている事を要求しています。つまり、デュアルCPUシステムでNVME-SSDがCPU1に、GPUがCPU2に接続されており、P2P DMAがCPU間のQPIを横切るよう構成する事はできません。</p>
<p>また、性能の観点からはCPU内蔵のPCIeコントローラよりも、専用のPCIeスイッチを介して互いのデバイスを接続する方が推奨されています。</p>
<p>以下の写真はHPC向けサーバのマザーボードで、8本のPCIe x16スロットがPCIeスイッチを介して互いに対となるスロットと接続されています。また、写真の左側のスロットはCPU1に、右側のスロットはCPU2に接続されています。</p>
<p>例えば、SSD-2上に構築されたテーブルをGPUダイレクトSQLを用いてスキャンする場合、最適なGPUの選択はGPU-2でしょう。またGPU-1を使用する事も可能ですが、GPUDirect RDMAの制約から、GPU-3とGPU-4の使用は避けねばなりません。</p>
<p><img alt="Motherboard of HPC Server" src="../img/pcie-hpc-server.png" /></p>
<p>PG-Stromは起動時にシステムのPCIeバストポロジ情報を取得し、GPUとNVME-SSD間の論理的な距離を算出します。
これは以下のように起動時のログに記録されており、例えば<code>/dev/nvme2</code>をスキャンする時はGPU1といった具合に、各NVME-SSDごとに最も距離の近いGPUを優先して使用するようになります。</p>
<pre><code>$ pg_ctl restart
     :
LOG:  PG-Strom: GPU0 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.50GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0
LOG:  [0000:41:00:0] GPU0 (NVIDIA A100-PCIE-40GB; GPU-13943bfd-5b30-38f5-0473-78&gt;
LOG:  [0000:81:00:0] nvme0 (NGD-IN2500-080T4-C) --&gt; GPU0 [dist=9]
LOG:  [0000:82:00:0] nvme2 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=9]
LOG:  [0000:c2:00:0] nvme3 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=9]
LOG:  [0000:c6:00:0] nvme5 (Corsair MP600 CORE) --&gt; GPU0 [dist=9]
LOG:  [0000:c3:00:0] nvme4 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=9]
LOG:  [0000:c1:00:0] nvme1 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=9]
LOG:  [0000:c4:00:0] nvme6 (NGD-IN2500-080T4-C) --&gt; GPU0 [dist=9]
</code></pre>
<p>通常は自動設定で問題ありません。
ただ、NVME-over-Fabric(RDMA)を使用する場合はPCIeバス上のnvmeデバイスの位置を取得できないため、手動でNVME-SSDとGPUの位置関係を設定する必要があります。</p>
<p>例えば<code>nvme1</code>には<code>gpu2</code>を、<code>nvme2</code>と<code>nvme3</code>には<code>gpu1</code>を割り当てる場合、以下の設定を<code>postgresql.conf</code>へ記述します。この手動設定は、自動設定よりも優先する事に留意してください。</p>
<pre><code>pg_strom.nvme_distance_map = 'nvme1=gpu2,nvme2=gpu1,nvme3=gpu1'
</code></pre>
<p>ローカルのNVME-SSDデバイス以外、例えば100Gbイーサネットで接続されたストレージサーバからGPU-Direct SQLを実行する場合など、PCI-Eバス上の距離の概念が当てはまらない場合は、ストレージがマウントされたディレクトリと、そこに関連付けるGPUを指定する事もできます。
以下は設定例です。</p>
<pre><code>pg_strom.nvme_distance_map = '/mnt/0=gpu0,/mnt/1=gpu1'
</code></pre>
<h3 id="guc">GUCパラメータによる制御</h3>
<p>GPUダイレクトSQL実行に関連するGUCパラメータは2つあります。</p>
<p>一つは<code>pg_strom.gpudirect_enabled</code>で、GPUダイレクト機能の有効/無効を単純にon/offします。
本パラメータが<code>off</code>になっていると、テーブルのサイズや物理配置とは無関係にGPUダイレクトSQL実行は使用されません。デフォルト値は<code>on</code>です。</p>
<p>もう一つのパラメータは<code>pg_strom.gpudirect_threshold</code>で、GPUダイレクトSQL実行が使われるべき最小のテーブルサイズを指定します。</p>
<p>テーブルの物理配置がNVME-SSD区画（または、NVME-SSDのみで構成されたmd-raid0区画）上に存在し、かつ、テーブルのサイズが本パラメータの指定値よりも大きな場合、PG-StromはGPUダイレクトSQL実行を選択します。
本パラメータのデフォルト値は<code>2GB</code>です。つまり、明らかに小さなテーブルに対してはGPUダイレクトSQLではなく、PostgreSQLのバッファから読み出す事を優先します。</p>
<p>これは、一回の読み出しであればGPUダイレクトSQL実行に優位性があったとしても、オンメモリ処理ができる程度のテーブルに対しては、二回目以降のディスクキャッシュ利用を考慮すると、必ずしも優位とは言えないという仮定に立っているという事です。</p>
<p>ワークロードの特性によっては必ずしもこの設定が正しいとは限りません。</p>
<h3 id="gpusql_1">GPUダイレクトSQL実行の利用を確認する</h3>
<p><code>EXPLAIN</code>コマンドを実行すると、当該クエリでGPUダイレクトSQL実行が利用されるのかどうかを確認する事ができます。</p>
<p>以下のクエリの例では、<code>Custom Scan (GpuJoin)</code>による<code>lineorder</code>テーブルに対するスキャンに<code>NVMe-Strom: enabled</code>との表示が出ています。この場合、<code>lineorder</code>テーブルからの読出しにはGPUダイレクトSQL実行が利用されます。</p>
<pre><code># explain (costs off)
select sum(lo_revenue), d_year, p_brand1
from lineorder, date1, part, supplier
where lo_orderdate = d_datekey
and lo_partkey = p_partkey
and lo_suppkey = s_suppkey
and p_category = 'MFGR#12'
and s_region = 'AMERICA'
  group by d_year, p_brand1
  order by d_year, p_brand1;
                                          QUERY PLAN
----------------------------------------------------------------------------------------------
 GroupAggregate
   Group Key: date1.d_year, part.p_brand1
   -&gt;  Sort
         Sort Key: date1.d_year, part.p_brand1
         -&gt;  Custom Scan (GpuPreAgg)
               Reduction: Local
               GPU Projection: pgstrom.psum((lo_revenue)::double precision), d_year, p_brand1
               Combined GpuJoin: enabled
               -&gt;  Custom Scan (GpuJoin) on lineorder
                     GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue
                     Outer Scan: lineorder
                     Depth 1: GpuHashJoin  (nrows 2406009600...97764190)
                              HashKeys: lineorder.lo_partkey
                              JoinQuals: (lineorder.lo_partkey = part.p_partkey)
                              KDS-Hash (size: 10.67MB)
                     Depth 2: GpuHashJoin  (nrows 97764190...18544060)
                              HashKeys: lineorder.lo_suppkey
                              JoinQuals: (lineorder.lo_suppkey = supplier.s_suppkey)
                              KDS-Hash (size: 131.59MB)
                     Depth 3: GpuHashJoin  (nrows 18544060...18544060)
                              HashKeys: lineorder.lo_orderdate
                              JoinQuals: (lineorder.lo_orderdate = date1.d_datekey)
                              KDS-Hash (size: 461.89KB)
                     NVMe-Strom: enabled
                     -&gt;  Custom Scan (GpuScan) on part
                           GPU Projection: p_brand1, p_partkey
                           GPU Filter: (p_category = 'MFGR#12'::bpchar)
                     -&gt;  Custom Scan (GpuScan) on supplier
                           GPU Projection: s_suppkey
                           GPU Filter: (s_region = 'AMERICA'::bpchar)
                     -&gt;  Seq Scan on date1
(31 rows)
</code></pre>
<h3 id="visibility-map">Visibility Mapに関する注意事項</h3>
<p>現在のところ、PG-StromのGPU側処理では行単位のMVCC可視性チェックを行う事ができません。これは、可視性チェックを行うために必要なデータ構造がホスト側だけに存在するためですが、ストレージ上のブロックを直接GPUに転送する場合、少々厄介な問題が生じます。</p>
<p>NVMe-SSDにP2P DMAを要求する時点では、ストレージブロックの内容はまだCPU/RAMへと読み出されていないため、具体的にどの行が可視であるのか、どの行が不可視であるのかを判別する事ができません。これは、PostgreSQLがレコードをストレージへ書き出す際にMVCC関連の属性と共に書き込んでいるためで、似たような問題がIndexOnlyScanを実装する際に表面化しました。</p>
<p>これに対処するため、PostgreSQLはVisibility Mapと呼ばれるインフラを持っています。これは、あるデータブロック中に存在するレコードが全てのトランザクションから可視である事が明らかであれば、該当するビットを立てる事で、データブロックを読むことなく当該ブロックにMVCC不可視なレコードが存在するか否かを判定する事を可能とするものです。</p>
<p>GPUダイレクトSQL実行はこのインフラを利用しています。つまり、Visibility Mapがセットされており、"all-visible"であるブロックだけがP2P DMAで読み出すようリクエストが送出されます。</p>
<p>Visibility MapはVACUUMのタイミングで作成されるため、以下のように明示的にVACUUMを実行する事で強制的にVisibility Mapを構築する事ができます。</p>
<pre><code>VACUUM ANALYZE linerorder;
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../troubles/" class="btn btn-neutral float-left" title="トラブルシューティング"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../arrow_fdw/" class="btn btn-neutral float-right" title="Apache Arrow">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../troubles/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../arrow_fdw/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
