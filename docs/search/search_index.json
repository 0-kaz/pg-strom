{
    "docs": [
        {
            "location": "/", 
            "text": "Preface\n\n\nThis chapter introduces the overview of PG-Strom, and developer's community.\n\n\nWhat is PG-Strom?\n\n\nPG-Strom is an extension module of PostgreSQL designed for version 9.6 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set.\n\n\nIts core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications.\n\n\nUnlike some DWH systems, PG-Strom shares the storage system of PostgreSQL which saves data in row-format. It is not always best choice for summary or analytics workloads, however, it is also an advantage as well. Users don't need to export and transform the data from transactional database for processing.\n\n\nPG-Strom v2.0 enhanced the capability to read from the storage. SSD-to-GPU Direct SQL Execution and in-memory columnar cache make up for the slowness of storage devices, and enable to provide massive data blocks to GPU fast which runs SQL workloads.\n\n\nOn the other hands, the feature of PL/CUDA and gstore_fdw allows to run highly computing density problems, like advanced statistical analytics or machine learning, on the database management system, and to return only results to users.\n\n\nLicense and Copyright\n\n\nPG-Strom is an open source software distributed under the GPL(GNU Public License) v2.\nSee \nLICENSE\n for the license details.\n\n\nPG-Strom Development Team reserves the copyright of the software.\nPG-Strom Development Team is an international, unincorporated association of individuals and companies who have contributed to the PG-Strom project, but not a legal entity.\n\n\nCommunity\n\n\nWe have a community mailing-list at: \nPG-Strom community ML\n It is a right place to post questions, requests, troubles and etc, related to PG-Strom project.\n\n\nPlease pay attention it is a public list for world wide. So, it is your own responsibility not to disclose confidential information.\n\n\nThe primary language of the mailing-list is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach \n(JP)\n prefix on the subject like, for non-Japanese speakers to skip messages.\n\n\nBug or troubles report\n\n\nIf you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue with \nbug\n tag at the \nPG-Strom Issue Tracker\n.\n\n\nPlease ensure the items below on bug reports.\n\n\n\n\nWhether you can reproduce the same problem on the latest revision?\n\n\nHopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software.\n\n\n\n\n\n\nWhether you can reproduce the same problem if PG-Strom is disabled?\n\n\nGUC option pg_strom.enabled can turn on/off PG-Strom.\n\n\n\n\n\n\nIs there any known issues on the issue tracker of GitHub?\n\n\nPlease don't forget to search \nclosed\n issues\n\n\n\n\n\n\n\n\nThe information below are helpful for bug-reports.\n\n\n\n\nOutput of \nEXPLAIN VERBOSE\n for the queries in trouble.\n\n\nData structure of the tables involved with \n\\d+ \ntable name\n on psql command.\n\n\nLog messages (verbose messages are more helpful)\n\n\nStatus of GUC options you modified from the default configurations.\n\n\nHardware configuration - GPU model and host RAM size especially.\n\n\n\n\nIf you are not certain whether the strange behavior on your site is bug or not, please report it to the mailing-list prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information.\n\n\nNew features proposition\n\n\nIf you have any ideas of new features, please open a new issue with \nfeature\n tag at the \nPG-Strom Issue Tracker\n, then have a discussion with other developers.\n\n\nA preferable design proposal will contain the items below.\n\n\n\n\nWhat is your problem to solve / improve?\n\n\nHow much serious is it on your workloads / user case?\n\n\nWay to implement your idea?\n\n\nExpected downside, if any.\n\n\n\n\nOnce we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed.\n\n\nOnce a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again.\n\n\nOn the development stage, please attach patch file on the issue ticket. We don't use pull request.\n\n\nSupport Policy\n\n\nThe PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release.\n\n\nPlease note that it is volunteer based community support policy, so our support is best effort and no SLA definition.\n\n\nIf you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom).\n\n\nVersioning Policy\n\n\nPG-Strom's version number is consists of two portion; major and minor version. \nmajor\n.\nminor\n\n\nIts minor version shall be incremented for each release; including bug fixes and new features.\nIts major version shall be incremented in the following situation.\n\n\n\n\nSome of supported PostgreSQL version gets deprecated.\n\n\nSome of supported GPU devices gets deprecated.\n\n\nNew version adds epoch making features.", 
            "title": "Home"
        }, 
        {
            "location": "/#preface", 
            "text": "This chapter introduces the overview of PG-Strom, and developer's community.", 
            "title": "Preface"
        }, 
        {
            "location": "/#what-is-pg-strom", 
            "text": "PG-Strom is an extension module of PostgreSQL designed for version 9.6 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set.  Its core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications.  Unlike some DWH systems, PG-Strom shares the storage system of PostgreSQL which saves data in row-format. It is not always best choice for summary or analytics workloads, however, it is also an advantage as well. Users don't need to export and transform the data from transactional database for processing.  PG-Strom v2.0 enhanced the capability to read from the storage. SSD-to-GPU Direct SQL Execution and in-memory columnar cache make up for the slowness of storage devices, and enable to provide massive data blocks to GPU fast which runs SQL workloads.  On the other hands, the feature of PL/CUDA and gstore_fdw allows to run highly computing density problems, like advanced statistical analytics or machine learning, on the database management system, and to return only results to users.", 
            "title": "What is PG-Strom?"
        }, 
        {
            "location": "/#license-and-copyright", 
            "text": "PG-Strom is an open source software distributed under the GPL(GNU Public License) v2.\nSee  LICENSE  for the license details.  PG-Strom Development Team reserves the copyright of the software.\nPG-Strom Development Team is an international, unincorporated association of individuals and companies who have contributed to the PG-Strom project, but not a legal entity.", 
            "title": "License and Copyright"
        }, 
        {
            "location": "/#community", 
            "text": "We have a community mailing-list at:  PG-Strom community ML  It is a right place to post questions, requests, troubles and etc, related to PG-Strom project.  Please pay attention it is a public list for world wide. So, it is your own responsibility not to disclose confidential information.  The primary language of the mailing-list is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach  (JP)  prefix on the subject like, for non-Japanese speakers to skip messages.", 
            "title": "Community"
        }, 
        {
            "location": "/#bug-or-troubles-report", 
            "text": "If you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue with  bug  tag at the  PG-Strom Issue Tracker .  Please ensure the items below on bug reports.   Whether you can reproduce the same problem on the latest revision?  Hopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software.    Whether you can reproduce the same problem if PG-Strom is disabled?  GUC option pg_strom.enabled can turn on/off PG-Strom.    Is there any known issues on the issue tracker of GitHub?  Please don't forget to search  closed  issues     The information below are helpful for bug-reports.   Output of  EXPLAIN VERBOSE  for the queries in trouble.  Data structure of the tables involved with  \\d+  table name  on psql command.  Log messages (verbose messages are more helpful)  Status of GUC options you modified from the default configurations.  Hardware configuration - GPU model and host RAM size especially.   If you are not certain whether the strange behavior on your site is bug or not, please report it to the mailing-list prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information.", 
            "title": "Bug or troubles report"
        }, 
        {
            "location": "/#new-features-proposition", 
            "text": "If you have any ideas of new features, please open a new issue with  feature  tag at the  PG-Strom Issue Tracker , then have a discussion with other developers.  A preferable design proposal will contain the items below.   What is your problem to solve / improve?  How much serious is it on your workloads / user case?  Way to implement your idea?  Expected downside, if any.   Once we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed.  Once a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again.  On the development stage, please attach patch file on the issue ticket. We don't use pull request.", 
            "title": "New features proposition"
        }, 
        {
            "location": "/#support-policy", 
            "text": "The PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release.  Please note that it is volunteer based community support policy, so our support is best effort and no SLA definition.  If you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom).", 
            "title": "Support Policy"
        }, 
        {
            "location": "/#versioning-policy", 
            "text": "PG-Strom's version number is consists of two portion; major and minor version.  major . minor  Its minor version shall be incremented for each release; including bug fixes and new features.\nIts major version shall be incremented in the following situation.   Some of supported PostgreSQL version gets deprecated.  Some of supported GPU devices gets deprecated.  New version adds epoch making features.", 
            "title": "Versioning Policy"
        }, 
        {
            "location": "/install/", 
            "text": "This chapter introduces the steps to install PG-Strom.\n\n\nChecklist\n\n\n\n\nServer Hardware\n\n\nIt requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices.\n\n\nnote002:HW Validation List\n may help you to choose the hardware.\n\n\nSSD-to-GPU Direct SQL Execution needs SSD devices which support NVMe specification, and to be installed under the same PCIe Root Complex where GPU is located on.\n\n\n\n\n\n\nGPU Device\n\n\nPG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later;\n\n\nnote001:GPU Availability Matrix\n shows more detailed information. Check this list for the support status of SSD-to-GPU Direct SQL Execution.\n\n\n\n\n\n\nOperating System\n\n\nPG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or CentOS version 7.x series.    - SSD-to-GPU Direct SQL Execution needs Red Hat Enterprise Linux or CentOS version 7.3 or later.\n\n\n\n\n\n\nPostgreSQL\n\n\nPG-Strom requires PostgreSQL version 9.6 or later. PostgreSQL v9.6 renew the custom-scan interface for CPU-parallel execution or \nGROUP BY\n planning, thus, it allows cooperation of custom-plans provides by extension modules.\n\n\n\n\n\n\nCUDA Toolkit\n\n\nPG-Strom requires CUDA Toolkit version 9.1 or later.\n\n\nPG-Strom provides half-precision floating point type (\nfloat2\n), and it internally use \nhalf_t\n type of CUDA C, so we cannot build it with older CUDA Toolkit.\n\n\n\n\n\n\n\n\nOS Installation\n\n\nPost OS Installation Configuration\n\n\nDKMS Installation\n\n\n\n\nFedora Project Public FTP Site\n\n\nhttps://dl.fedoraproject.org/pub/epel/7/x86_64/\n\n\n\n\n\n\n\n\nHeteroDB-SWDC Installation\n\n\nCUDA Toolkit Installation\n\n\n\n\n$ sudo rpm -i cuda-repo-\ndistribution\n-\nversion\n.x86_64.rpm\n$ sudo yum clean all\n$ sudo yum install cuda\n\n\n\n\n$ ls /usr/local/cuda\nbin     include  libnsight         nvml       samples  tools\ndoc     jre      libnvvp           nvvm       share    version.txt\nextras  lib64    nsightee_plugins  pkgconfig  src\n\n\n\n\n$ nvidia-smi\nWed Feb 14 09:43:48 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   41C    P0    37W / 250W |      0MiB / 16152MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\nPostgreSQL Installation\n\n\n$ sudo rpm -ivh pgdg-redhat10-10-2.noarch.rpm\n$ sudo yum install -y postgresql10-server postgresql10-devel\n            :\n            :\n================================================================================\n Package                  Arch        Version                 Repository   Size\n================================================================================\nInstalling:\n postgresql10-devel       x86_64      10.2-1PGDG.rhel7        pgdg10      2.0 M\n postgresql10-server      x86_64      10.2-1PGDG.rhel7        pgdg10      4.4 M\nInstalling for dependencies:\n postgresql10             x86_64      10.2-1PGDG.rhel7        pgdg10      1.5 M\n postgresql10-libs        x86_64      10.2-1PGDG.rhel7        pgdg10      354 k\n\nTransaction Summary\n================================================================================\nInstall  2 Packages (+2 Dependent packages)\n\nTotal download size: 8.3 M\nInstalled size: 35 M\n            :\n            :\nInstalled:\n  postgresql10-devel.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-server.x86_64 0:10.2-1PGDG.rhel7\n\nDependency Installed:\n  postgresql10.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-libs.x86_64 0:10.2-1PGDG.rhel7\n\nComplete!\n\n\n\n\nPG-Strom Installation\n\n\nRPM Installation\n\n\n$ wget https://heterodb.github.io/swdc/yum/rhel7-noarch/heterodb-swdc-1.0-1.el7.noarch.rpm\n$ sudo rpm -ivh heterodb-swdc-1.0-1.el7.noarch.rpm\nPreparing...                          ################################# [100%]\nUpdating / installing...\n   1:heterodb-swdc-1.0-1.el7          ################################# [100%]\n\n\n\n\n\n\n\n\n\n\nBuild from the source\n\n\nGetting the source code\n\n\n$ git clone https://github.com/heterodb/pg-strom.git\nCloning into 'pg-strom'...\nremote: Counting objects: 13797, done.\nremote: Compressing objects: 100% (215/215), done.\nremote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400\nReceiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done.\nResolving deltas: 100% (10504/10504), done.\n\n\n\n\nBuilding the PG-Strom\n\n\n$ cd pg-strom\n$ make PG_CONFIG=/usr/pgsql-10/bin/pg_config\n$ sudo make install PG_CONFIG=/usr/pgsql-10/bin/pg_config\n\n\n\n\nPost Installation Setup", 
            "title": "Install"
        }, 
        {
            "location": "/install/#checklist", 
            "text": "Server Hardware  It requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices.  note002:HW Validation List  may help you to choose the hardware.  SSD-to-GPU Direct SQL Execution needs SSD devices which support NVMe specification, and to be installed under the same PCIe Root Complex where GPU is located on.    GPU Device  PG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later;  note001:GPU Availability Matrix  shows more detailed information. Check this list for the support status of SSD-to-GPU Direct SQL Execution.    Operating System  PG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or CentOS version 7.x series.    - SSD-to-GPU Direct SQL Execution needs Red Hat Enterprise Linux or CentOS version 7.3 or later.    PostgreSQL  PG-Strom requires PostgreSQL version 9.6 or later. PostgreSQL v9.6 renew the custom-scan interface for CPU-parallel execution or  GROUP BY  planning, thus, it allows cooperation of custom-plans provides by extension modules.    CUDA Toolkit  PG-Strom requires CUDA Toolkit version 9.1 or later.  PG-Strom provides half-precision floating point type ( float2 ), and it internally use  half_t  type of CUDA C, so we cannot build it with older CUDA Toolkit.", 
            "title": "Checklist"
        }, 
        {
            "location": "/install/#os-installation", 
            "text": "", 
            "title": "OS Installation"
        }, 
        {
            "location": "/install/#post-os-installation-configuration", 
            "text": "", 
            "title": "Post OS Installation Configuration"
        }, 
        {
            "location": "/install/#dkms-installation", 
            "text": "Fedora Project Public FTP Site  https://dl.fedoraproject.org/pub/epel/7/x86_64/", 
            "title": "DKMS Installation"
        }, 
        {
            "location": "/install/#heterodb-swdc-installation", 
            "text": "", 
            "title": "HeteroDB-SWDC Installation"
        }, 
        {
            "location": "/install/#cuda-toolkit-installation", 
            "text": "$ sudo rpm -i cuda-repo- distribution - version .x86_64.rpm\n$ sudo yum clean all\n$ sudo yum install cuda  $ ls /usr/local/cuda\nbin     include  libnsight         nvml       samples  tools\ndoc     jre      libnvvp           nvvm       share    version.txt\nextras  lib64    nsightee_plugins  pkgconfig  src  $ nvidia-smi\nWed Feb 14 09:43:48 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   41C    P0    37W / 250W |      0MiB / 16152MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+", 
            "title": "CUDA Toolkit Installation"
        }, 
        {
            "location": "/install/#postgresql-installation", 
            "text": "$ sudo rpm -ivh pgdg-redhat10-10-2.noarch.rpm\n$ sudo yum install -y postgresql10-server postgresql10-devel\n            :\n            :\n================================================================================\n Package                  Arch        Version                 Repository   Size\n================================================================================\nInstalling:\n postgresql10-devel       x86_64      10.2-1PGDG.rhel7        pgdg10      2.0 M\n postgresql10-server      x86_64      10.2-1PGDG.rhel7        pgdg10      4.4 M\nInstalling for dependencies:\n postgresql10             x86_64      10.2-1PGDG.rhel7        pgdg10      1.5 M\n postgresql10-libs        x86_64      10.2-1PGDG.rhel7        pgdg10      354 k\n\nTransaction Summary\n================================================================================\nInstall  2 Packages (+2 Dependent packages)\n\nTotal download size: 8.3 M\nInstalled size: 35 M\n            :\n            :\nInstalled:\n  postgresql10-devel.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-server.x86_64 0:10.2-1PGDG.rhel7\n\nDependency Installed:\n  postgresql10.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-libs.x86_64 0:10.2-1PGDG.rhel7\n\nComplete!", 
            "title": "PostgreSQL Installation"
        }, 
        {
            "location": "/install/#pg-strom-installation", 
            "text": "", 
            "title": "PG-Strom Installation"
        }, 
        {
            "location": "/install/#rpm-installation", 
            "text": "$ wget https://heterodb.github.io/swdc/yum/rhel7-noarch/heterodb-swdc-1.0-1.el7.noarch.rpm\n$ sudo rpm -ivh heterodb-swdc-1.0-1.el7.noarch.rpm\nPreparing...                          ################################# [100%]\nUpdating / installing...\n   1:heterodb-swdc-1.0-1.el7          ################################# [100%]", 
            "title": "RPM Installation"
        }, 
        {
            "location": "/install/#build-from-the-source", 
            "text": "", 
            "title": "Build from the source"
        }, 
        {
            "location": "/install/#getting-the-source-code", 
            "text": "$ git clone https://github.com/heterodb/pg-strom.git\nCloning into 'pg-strom'...\nremote: Counting objects: 13797, done.\nremote: Compressing objects: 100% (215/215), done.\nremote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400\nReceiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done.\nResolving deltas: 100% (10504/10504), done.", 
            "title": "Getting the source code"
        }, 
        {
            "location": "/install/#building-the-pg-strom", 
            "text": "$ cd pg-strom\n$ make PG_CONFIG=/usr/pgsql-10/bin/pg_config\n$ sudo make install PG_CONFIG=/usr/pgsql-10/bin/pg_config", 
            "title": "Building the PG-Strom"
        }, 
        {
            "location": "/install/#post-installation-setup", 
            "text": "", 
            "title": "Post Installation Setup"
        }, 
        {
            "location": "/features/", 
            "text": "This chapter introduces advanced features of PG-Strom.\n\n\nSSD-to-GPU Direct SQL Execution\n\n\nOverview\n\n\n\n\nSystem Setup\n\n\nDriver Installation\n\n\n$ sudo yum install nvme-strom\nLoaded plugins: fastestmirror\n      :\nResolving Dependencies\n--\n Running transaction check\n---\n Package nvme-strom.x86_64 0:0.6-1.el7 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch            Version            Repository         Size\n================================================================================\nInstalling:\n nvme-strom          x86_64          0.6-1.el7          heterodb           58 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n\nTotal download size: 58 k\nInstalled size: 217 k\nIs this ok [y/d/N]: y\nDownloading packages:\nPackage nvme-strom-0.6-1.el7.x86_64.rpm is not signed0 B/s |    0 B   --:-- ETA\nnvme-strom-0.6-1.el7.x86_64.rpm                            |  58 kB   00:00\n\n\nPackage nvme-strom-0.6-1.el7.x86_64.rpm is not signed\n\n\n\n\n$ lsmod | grep nvme\nnvme_strom             12625  0\nnvme                   27722  4\nnvme_core              52964  9 nvme\n\n\n\n\nDesigning Tablespace\n\n\nCREATE TABLESPACE my_nvme LOCATION '/opt/nvme';\n\n\n\n\nCREATE TABLE my_table ()\n\n\n\n\nALTER DATABASE my_database SET TABLESPACE my_nvme;\n\n\n\n\nOperations\n\n\nControls using GUC parameters\n\n\n{\n\u3082\u3046\u4e00\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\npg_strom.nvme_strom_threshold\n\u3067\u3001SSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u304c\u4f7f\u308f\u308c\u308b\u3079\u304d\u6700\u5c0f\u306e\u30c6\u30fc\u30d6\u30eb\u30b5\u30a4\u30ba\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n\n\n\u30c6\u30fc\u30d6\u30eb\u306e\u7269\u7406\u914d\u7f6e\u304cNVMe-SSD\u533a\u753b\uff08\u307e\u305f\u306f\u3001NVMe-SSD\u306e\u307f\u3067\u69cb\u6210\u3055\u308c\u305fmd-raid0\u533a\u753b\uff09\u4e0a\u306b\u5b58\u5728\u3057\u3001\u304b\u3064\u3001\u30c6\u30fc\u30d6\u30eb\u306e\u30b5\u30a4\u30ba\u304c\u672c\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6307\u5b9a\u5024\u3088\u308a\u3082\u5927\u304d\u306a\u5834\u5408\u3001PG-Strom\u306fSSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u3092\u9078\u629e\u3057\u307e\u3059\u3002\n\u672c\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306f\u3001\u30b7\u30b9\u30c6\u30e0\u306e\u7269\u7406\u30e1\u30e2\u30ea\u30b5\u30a4\u30ba\u3068\nshared_buffers\n\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6307\u5b9a\u5024\u306e1/3\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u521d\u671f\u8a2d\u5b9a\u3067\u306f\u9593\u9055\u3044\u306a\u304f\u30aa\u30f3\u30e1\u30e2\u30ea\u3067\u51e6\u7406\u3057\u304d\u308c\u306a\u3044\u30b5\u30a4\u30ba\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u5bfe\u3057\u3066\u3060\u3051SSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u3092\u884c\u3046\u3088\u3046\u8abf\u6574\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\n\u3053\u308c\u306f\u3001\u4e00\u56de\u306e\u8aad\u307f\u51fa\u3057\u3067\u3042\u308c\u3070SSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u306b\u512a\u4f4d\u6027\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u3082\u3001\u30aa\u30f3\u30e1\u30e2\u30ea\u51e6\u7406\u304c\u3067\u304d\u308b\u7a0b\u5ea6\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u5bfe\u3057\u3066\u306f\u3001\u4e8c\u56de\u76ee\u4ee5\u964d\u306e\u30c7\u30a3\u30b9\u30af\u30ad\u30e3\u30c3\u30b7\u30e5\u5229\u7528\u3092\u8003\u616e\u3059\u308b\u3068\u3001\u5fc5\u305a\u3057\u3082\u512a\u4f4d\u3068\u306f\u8a00\u3048\u306a\u3044\u3068\u3044\u3046\u4eee\u5b9a\u306b\u7acb\u3063\u3066\u3044\u308b\u3068\u3044\u3046\u4e8b\u3067\u3059\u3002\n\n\n\u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u306e\u7279\u6027\u306b\u3088\u3063\u3066\u306f\u5fc5\u305a\u3057\u3082\u3053\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u3044\u3068\u306f\u9650\u308a\u307e\u305b\u3093\u3002\n}\n\n\nEnsure usage of SSD-to-GPU Direct SQL Execution\n\n\n# explain (costs off)\nselect sum(lo_revenue), d_year, p_brand1\nfrom lineorder, date1, part, supplier\nwhere lo_orderdate = d_datekey\nand lo_partkey = p_partkey\nand lo_suppkey = s_suppkey\nand p_category = 'MFGR#12'\nand s_region = 'AMERICA'\n  group by d_year, p_brand1\n  order by d_year, p_brand1;\n                                          QUERY PLAN\n----------------------------------------------------------------------------------------------\n GroupAggregate\n   Group Key: date1.d_year, part.p_brand1\n   -\n  Sort\n         Sort Key: date1.d_year, part.p_brand1\n         -\n  Custom Scan (GpuPreAgg)\n               Reduction: Local\n               GPU Projection: pgstrom.psum((lo_revenue)::double precision), d_year, p_brand1\n               Combined GpuJoin: enabled\n               -\n  Custom Scan (GpuJoin) on lineorder\n                     GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue\n                     Outer Scan: lineorder\n                     Depth 1: GpuHashJoin  (nrows 2406009600...97764190)\n                              HashKeys: lineorder.lo_partkey\n                              JoinQuals: (lineorder.lo_partkey = part.p_partkey)\n                              KDS-Hash (size: 10.67MB)\n                     Depth 2: GpuHashJoin  (nrows 97764190...18544060)\n                              HashKeys: lineorder.lo_suppkey\n                              JoinQuals: (lineorder.lo_suppkey = supplier.s_suppkey)\n                              KDS-Hash (size: 131.59MB)\n                     Depth 3: GpuHashJoin  (nrows 18544060...18544060)\n                              HashKeys: lineorder.lo_orderdate\n                              JoinQuals: (lineorder.lo_orderdate = date1.d_datekey)\n                              KDS-Hash (size: 461.89KB)\n                     NVMe-Strom: enabled\n                     -\n  Custom Scan (GpuScan) on part\n                           GPU Projection: p_brand1, p_partkey\n                           GPU Filter: (p_category = 'MFGR#12'::bpchar)\n                     -\n  Custom Scan (GpuScan) on supplier\n                           GPU Projection: s_suppkey\n                           GPU Filter: (s_region = 'AMERICA'::bpchar)\n                     -\n  Seq Scan on date1\n(31 rows)\n\n\n\n\nAttension for visibility map\n\n\nVACUUM ANALYZE linerorder;\n\n\n\n\nIn-memory Columnar Cache\n\n\nOverview\n\n\nSystem Setup\n\n\nLocation of the columnar cache\n\n\nColumnar Cache Builder Configuration\n\n\nSource Table Configuration\n\n\npostgres=# select pgstrom_ccache_enabled('t0');\n pgstrom_ccache_enabled\n------------------------\n enabled\n(1 row)\n\n\n\n\nOperations\n\n\nCheck status of columnar cache\n\n\ncontrib_regression_pg_strom=# SELECT * FROM pgstrom.ccache_info ;\n database_id | table_id | block_nr | nitems  |  length   |             ctime             |             atime\n-------------+----------+----------+---------+-----------+-------------------------------+-------------------------------\n       13323 | 25887    |   622592 | 1966080 | 121897472 | 2018-02-18 14:31:30.898389+09 | 2018-02-18 14:38:43.711287+09\n       13323 | 25887    |   425984 | 1966080 | 121897472 | 2018-02-18 14:28:39.356952+09 | 2018-02-18 14:38:43.514788+09\n       13323 | 25887    |    98304 | 1966080 | 121897472 | 2018-02-18 14:28:01.542261+09 | 2018-02-18 14:38:42.930281+09\n         :       :             :         :          :                :                               :\n       13323 | 25887    |    16384 | 1963079 | 121711472 | 2018-02-18 14:28:00.647021+09 | 2018-02-18 14:38:42.909112+09\n       13323 | 25887    |   737280 | 1966080 | 121897472 | 2018-02-18 14:34:32.249899+09 | 2018-02-18 14:38:43.882029+09\n       13323 | 25887    |   770048 | 1966080 | 121897472 | 2018-02-18 14:28:57.321121+09 | 2018-02-18 14:38:43.90157+09\n(50 rows)\n\n\n\n\nCheck usage of columnar cache\n\n\npostgres=# EXPLAIN SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid \n 1000;\n\n                                  QUERY PLAN\n-------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n   Outer Scan Filter: (aid \n 1000)\n   Depth 1: GpuHashJoin  (nrows 1029348...1029348)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size: 10.78MB)\n   CCache: enabled\n   -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(10 rows)\n\n\n\n\npostgres=# EXPLAIN ANALYZE SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid \n 1000;\n\n                                    QUERY PLAN\n\n-------------------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n                              (actual time=91.766..723.549 rows=1000224 loops=1)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n                   (actual time=7.129..398.270 rows=100000000 loops=1)\n   Outer Scan Filter: (aid \n 1000)\n   Rows Removed by Outer Scan Filter: 98999776\n   Depth 1: GpuHashJoin  (plan nrows: 1029348...1029348, actual nrows: 1000224...1000224)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size plan: 10.78MB, exec: 64.00MB)\n   CCache Hits: 50\n   -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n                       (actual time=0.011..13.542 rows=100000 loops=1)\n Planning time: 23.390 ms\n Execution time: 1409.073 ms\n(13 rows)\n\n\n\n\nAttension for \nDROP DATABASE\n command\n\n\nGPU Memory Store(gstore_fdw)\n\n\nOverview\n\n\nSetup\n\n\nCREATE FOREIGN TABLE ft (\n    id int,\n    x0 real,\n    x1 real,\n    x2 real,\n    x3 real,\n    x4 real,\n    x5 real,\n    x6 real,\n    x7 real,\n    x8 real,\n    x9 real\n) SERVER gstore_fdw OPTIONS (pinning '0', format 'pgstrom');\n\n\n\n\nOperations\n\n\n\u30c7\u30fc\u30bf\u306e\u30ed\u30fc\u30c9\n\n\n\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30ca\u30eb\u3067\u306f\u3042\u308b\u304c\u3001\u3001\u3001\n\n\n\u30c7\u30fc\u30bf\u5bb9\u91cf\u306e\u78ba\u8a8d\n\n\npreserved memory\u306e\u78ba\u8a8d\n\n\n\u6ce8\u610f\u4e8b\u9805\n\n\nInternal Data Format (pgstrom format)\n\n\nRelated Features\n\n\n\u30e9\u30fc\u30b8\u30aa\u30d6\u30b8\u30a7\u30af\u30c8", 
            "title": "Advanced Features"
        }, 
        {
            "location": "/features/#ssd-to-gpu-direct-sql-execution", 
            "text": "", 
            "title": "SSD-to-GPU Direct SQL Execution"
        }, 
        {
            "location": "/features/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/features/#system-setup", 
            "text": "", 
            "title": "System Setup"
        }, 
        {
            "location": "/features/#driver-installation", 
            "text": "$ sudo yum install nvme-strom\nLoaded plugins: fastestmirror\n      :\nResolving Dependencies\n--  Running transaction check\n---  Package nvme-strom.x86_64 0:0.6-1.el7 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch            Version            Repository         Size\n================================================================================\nInstalling:\n nvme-strom          x86_64          0.6-1.el7          heterodb           58 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n\nTotal download size: 58 k\nInstalled size: 217 k\nIs this ok [y/d/N]: y\nDownloading packages:\nPackage nvme-strom-0.6-1.el7.x86_64.rpm is not signed0 B/s |    0 B   --:-- ETA\nnvme-strom-0.6-1.el7.x86_64.rpm                            |  58 kB   00:00\n\n\nPackage nvme-strom-0.6-1.el7.x86_64.rpm is not signed  $ lsmod | grep nvme\nnvme_strom             12625  0\nnvme                   27722  4\nnvme_core              52964  9 nvme", 
            "title": "Driver Installation"
        }, 
        {
            "location": "/features/#designing-tablespace", 
            "text": "CREATE TABLESPACE my_nvme LOCATION '/opt/nvme';  CREATE TABLE my_table ()  ALTER DATABASE my_database SET TABLESPACE my_nvme;", 
            "title": "Designing Tablespace"
        }, 
        {
            "location": "/features/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/features/#controls-using-guc-parameters", 
            "text": "{\n\u3082\u3046\u4e00\u3064\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f pg_strom.nvme_strom_threshold \u3067\u3001SSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u304c\u4f7f\u308f\u308c\u308b\u3079\u304d\u6700\u5c0f\u306e\u30c6\u30fc\u30d6\u30eb\u30b5\u30a4\u30ba\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002  \u30c6\u30fc\u30d6\u30eb\u306e\u7269\u7406\u914d\u7f6e\u304cNVMe-SSD\u533a\u753b\uff08\u307e\u305f\u306f\u3001NVMe-SSD\u306e\u307f\u3067\u69cb\u6210\u3055\u308c\u305fmd-raid0\u533a\u753b\uff09\u4e0a\u306b\u5b58\u5728\u3057\u3001\u304b\u3064\u3001\u30c6\u30fc\u30d6\u30eb\u306e\u30b5\u30a4\u30ba\u304c\u672c\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6307\u5b9a\u5024\u3088\u308a\u3082\u5927\u304d\u306a\u5834\u5408\u3001PG-Strom\u306fSSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u3092\u9078\u629e\u3057\u307e\u3059\u3002\n\u672c\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u306f\u3001\u30b7\u30b9\u30c6\u30e0\u306e\u7269\u7406\u30e1\u30e2\u30ea\u30b5\u30a4\u30ba\u3068 shared_buffers \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6307\u5b9a\u5024\u306e1/3\u3067\u3059\u3002\u3064\u307e\u308a\u3001\u521d\u671f\u8a2d\u5b9a\u3067\u306f\u9593\u9055\u3044\u306a\u304f\u30aa\u30f3\u30e1\u30e2\u30ea\u3067\u51e6\u7406\u3057\u304d\u308c\u306a\u3044\u30b5\u30a4\u30ba\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u5bfe\u3057\u3066\u3060\u3051SSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u3092\u884c\u3046\u3088\u3046\u8abf\u6574\u3055\u308c\u3066\u3044\u307e\u3059\u3002  \u3053\u308c\u306f\u3001\u4e00\u56de\u306e\u8aad\u307f\u51fa\u3057\u3067\u3042\u308c\u3070SSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\u306b\u512a\u4f4d\u6027\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u3082\u3001\u30aa\u30f3\u30e1\u30e2\u30ea\u51e6\u7406\u304c\u3067\u304d\u308b\u7a0b\u5ea6\u306e\u30c6\u30fc\u30d6\u30eb\u306b\u5bfe\u3057\u3066\u306f\u3001\u4e8c\u56de\u76ee\u4ee5\u964d\u306e\u30c7\u30a3\u30b9\u30af\u30ad\u30e3\u30c3\u30b7\u30e5\u5229\u7528\u3092\u8003\u616e\u3059\u308b\u3068\u3001\u5fc5\u305a\u3057\u3082\u512a\u4f4d\u3068\u306f\u8a00\u3048\u306a\u3044\u3068\u3044\u3046\u4eee\u5b9a\u306b\u7acb\u3063\u3066\u3044\u308b\u3068\u3044\u3046\u4e8b\u3067\u3059\u3002  \u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u306e\u7279\u6027\u306b\u3088\u3063\u3066\u306f\u5fc5\u305a\u3057\u3082\u3053\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u3044\u3068\u306f\u9650\u308a\u307e\u305b\u3093\u3002\n}", 
            "title": "Controls using GUC parameters"
        }, 
        {
            "location": "/features/#ensure-usage-of-ssd-to-gpu-direct-sql-execution", 
            "text": "# explain (costs off)\nselect sum(lo_revenue), d_year, p_brand1\nfrom lineorder, date1, part, supplier\nwhere lo_orderdate = d_datekey\nand lo_partkey = p_partkey\nand lo_suppkey = s_suppkey\nand p_category = 'MFGR#12'\nand s_region = 'AMERICA'\n  group by d_year, p_brand1\n  order by d_year, p_brand1;\n                                          QUERY PLAN\n----------------------------------------------------------------------------------------------\n GroupAggregate\n   Group Key: date1.d_year, part.p_brand1\n   -   Sort\n         Sort Key: date1.d_year, part.p_brand1\n         -   Custom Scan (GpuPreAgg)\n               Reduction: Local\n               GPU Projection: pgstrom.psum((lo_revenue)::double precision), d_year, p_brand1\n               Combined GpuJoin: enabled\n               -   Custom Scan (GpuJoin) on lineorder\n                     GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue\n                     Outer Scan: lineorder\n                     Depth 1: GpuHashJoin  (nrows 2406009600...97764190)\n                              HashKeys: lineorder.lo_partkey\n                              JoinQuals: (lineorder.lo_partkey = part.p_partkey)\n                              KDS-Hash (size: 10.67MB)\n                     Depth 2: GpuHashJoin  (nrows 97764190...18544060)\n                              HashKeys: lineorder.lo_suppkey\n                              JoinQuals: (lineorder.lo_suppkey = supplier.s_suppkey)\n                              KDS-Hash (size: 131.59MB)\n                     Depth 3: GpuHashJoin  (nrows 18544060...18544060)\n                              HashKeys: lineorder.lo_orderdate\n                              JoinQuals: (lineorder.lo_orderdate = date1.d_datekey)\n                              KDS-Hash (size: 461.89KB)\n                     NVMe-Strom: enabled\n                     -   Custom Scan (GpuScan) on part\n                           GPU Projection: p_brand1, p_partkey\n                           GPU Filter: (p_category = 'MFGR#12'::bpchar)\n                     -   Custom Scan (GpuScan) on supplier\n                           GPU Projection: s_suppkey\n                           GPU Filter: (s_region = 'AMERICA'::bpchar)\n                     -   Seq Scan on date1\n(31 rows)", 
            "title": "Ensure usage of SSD-to-GPU Direct SQL Execution"
        }, 
        {
            "location": "/features/#attension-for-visibility-map", 
            "text": "VACUUM ANALYZE linerorder;", 
            "title": "Attension for visibility map"
        }, 
        {
            "location": "/features/#in-memory-columnar-cache", 
            "text": "", 
            "title": "In-memory Columnar Cache"
        }, 
        {
            "location": "/features/#overview_1", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/features/#system-setup_1", 
            "text": "", 
            "title": "System Setup"
        }, 
        {
            "location": "/features/#location-of-the-columnar-cache", 
            "text": "", 
            "title": "Location of the columnar cache"
        }, 
        {
            "location": "/features/#columnar-cache-builder-configuration", 
            "text": "", 
            "title": "Columnar Cache Builder Configuration"
        }, 
        {
            "location": "/features/#source-table-configuration", 
            "text": "postgres=# select pgstrom_ccache_enabled('t0');\n pgstrom_ccache_enabled\n------------------------\n enabled\n(1 row)", 
            "title": "Source Table Configuration"
        }, 
        {
            "location": "/features/#operations_1", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/features/#check-status-of-columnar-cache", 
            "text": "contrib_regression_pg_strom=# SELECT * FROM pgstrom.ccache_info ;\n database_id | table_id | block_nr | nitems  |  length   |             ctime             |             atime\n-------------+----------+----------+---------+-----------+-------------------------------+-------------------------------\n       13323 | 25887    |   622592 | 1966080 | 121897472 | 2018-02-18 14:31:30.898389+09 | 2018-02-18 14:38:43.711287+09\n       13323 | 25887    |   425984 | 1966080 | 121897472 | 2018-02-18 14:28:39.356952+09 | 2018-02-18 14:38:43.514788+09\n       13323 | 25887    |    98304 | 1966080 | 121897472 | 2018-02-18 14:28:01.542261+09 | 2018-02-18 14:38:42.930281+09\n         :       :             :         :          :                :                               :\n       13323 | 25887    |    16384 | 1963079 | 121711472 | 2018-02-18 14:28:00.647021+09 | 2018-02-18 14:38:42.909112+09\n       13323 | 25887    |   737280 | 1966080 | 121897472 | 2018-02-18 14:34:32.249899+09 | 2018-02-18 14:38:43.882029+09\n       13323 | 25887    |   770048 | 1966080 | 121897472 | 2018-02-18 14:28:57.321121+09 | 2018-02-18 14:38:43.90157+09\n(50 rows)", 
            "title": "Check status of columnar cache"
        }, 
        {
            "location": "/features/#check-usage-of-columnar-cache", 
            "text": "postgres=# EXPLAIN SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid   1000;\n\n                                  QUERY PLAN\n-------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n   Outer Scan Filter: (aid   1000)\n   Depth 1: GpuHashJoin  (nrows 1029348...1029348)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size: 10.78MB)\n   CCache: enabled\n   -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(10 rows)  postgres=# EXPLAIN ANALYZE SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid   1000;\n\n                                    QUERY PLAN\n\n-------------------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n                              (actual time=91.766..723.549 rows=1000224 loops=1)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n                   (actual time=7.129..398.270 rows=100000000 loops=1)\n   Outer Scan Filter: (aid   1000)\n   Rows Removed by Outer Scan Filter: 98999776\n   Depth 1: GpuHashJoin  (plan nrows: 1029348...1029348, actual nrows: 1000224...1000224)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size plan: 10.78MB, exec: 64.00MB)\n   CCache Hits: 50\n   -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n                       (actual time=0.011..13.542 rows=100000 loops=1)\n Planning time: 23.390 ms\n Execution time: 1409.073 ms\n(13 rows)", 
            "title": "Check usage of columnar cache"
        }, 
        {
            "location": "/features/#attension-for-drop-database-command", 
            "text": "", 
            "title": "Attension for DROP DATABASE command"
        }, 
        {
            "location": "/features/#gpu-memory-storegstore_fdw", 
            "text": "", 
            "title": "GPU Memory Store(gstore_fdw)"
        }, 
        {
            "location": "/features/#overview_2", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/features/#setup", 
            "text": "CREATE FOREIGN TABLE ft (\n    id int,\n    x0 real,\n    x1 real,\n    x2 real,\n    x3 real,\n    x4 real,\n    x5 real,\n    x6 real,\n    x7 real,\n    x8 real,\n    x9 real\n) SERVER gstore_fdw OPTIONS (pinning '0', format 'pgstrom');", 
            "title": "Setup"
        }, 
        {
            "location": "/features/#operations_2", 
            "text": "\u30c7\u30fc\u30bf\u306e\u30ed\u30fc\u30c9  \u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30ca\u30eb\u3067\u306f\u3042\u308b\u304c\u3001\u3001\u3001  \u30c7\u30fc\u30bf\u5bb9\u91cf\u306e\u78ba\u8a8d  preserved memory\u306e\u78ba\u8a8d  \u6ce8\u610f\u4e8b\u9805", 
            "title": "Operations"
        }, 
        {
            "location": "/features/#internal-data-format-pgstrom-format", 
            "text": "", 
            "title": "Internal Data Format (pgstrom format)"
        }, 
        {
            "location": "/features/#related-features", 
            "text": "\u30e9\u30fc\u30b8\u30aa\u30d6\u30b8\u30a7\u30af\u30c8", 
            "title": "Related Features"
        }, 
        {
            "location": "/plcuda/", 
            "text": "PL/CUDA\n\n\nThis chapter introduces the way to implement GPU executable native program as SQL functions, using PL/CUDA procedural language.\n\n\nPL/CUDA Overview\n\n\nPG-Strom internally constructs GPU programs by CUDA language, according to the supplied SQL, then generates GPU's native binary using just-in-time compile. CUDA is a programming environment provided by NVIDIA. It allows implementing parallel program which is executable on GPU device, using C-like statement. This transformation process from SQL statement to CUDA program is an internal process, thus, no need to pay attention what GPU programs are generated and executed from the standpoint of users.\n\n\nOn the other hands, PostgreSQL supports to add programming language to implement SQL functions by \nCREATE LANGUAGE\n statement. PL/CUDA is a language handler to supports \nCREATE LANGUAGE\n command. It also allows users to run arbitrary GPU programs manually implemented as SQL functions, but not only GPU programs automatically generated by PG-Strom based on SQL.\n\n\nIts argument can take the data types supported by PG-Strom, like numeric, text, or array-matrix data type. These arguments are implicitly loaded onto GPU device memory by the PL/CUDA infrastructure, so users don't need to pay attention for data loading between the database and GPU devices. In a similar fashion, the return value of PL/CUDA function (including the case of variable length data type) will be written back to CPU from GPU, then decode to the result of SQL function.\n\n\nYou can also use foreign tables defined with \ngstore_fdw\n as arguments of PL/CUDA function. In this case, no need to load the data onto GPU for each invocation because foreign table already keeps the data, and available to use larger data than 1GB which is a restriction of variable length data in PostgreSQL.\n\n\nTherefore, users can focus on productive tasks like implementation of statistical analysis, code optimization and so on, without routine process like data input/output between GPU and databases.\n\n\n\u3053\u3053\u306b\u7d75\n\n\nOnce a PL/CUDA function is declared with CREATE FUNCTION statement, it generates a CUDA program that embeds the definition of this function on the GPU's kernel function at the execution time. This kernel function contains initialization code to reference this PL/CUDA functions and auxiliary code to return run-time error to CPU side. Also, it can include some run-time functions to support execution of PG-Strom.\n\n\nHere is no special memory protection mechanism on the native CUDA program made with PL/CUDA function, thus, execution of buggy PL/CUDA function can crash GPU execution environment or PostreSQL infrastructure in some cases. Thus, only database superuser can define PL/CUDA function.\n\n\nBelow is an example of simple PL/CUDA function. This function takes two int arguments, and then returns the sum of them with int data type.\n\n\npostgres=# CREATE FUNCTION gpu_add(int, int)\nRETURNS int\nAS $$\n#plcuda_include \ncuda_mathlib.h\n\n#plcuda_begin\n  if (get_global_id() == 0)\n    *retval = pgfn_int4pl(kcxt, arg1, arg2);\n#plcuda_end\n$$ LANGUAGE plcuda;\nCREATE FUNCTION\n\n\n\n\nThe code block enclosed by \n#plcuda_begin\n and \n#plcuda_end\n is main portion of PL/CUDA function. This kernel function can reference the \nint\n type argument as \narg1\n and \narg2\n which are \npg_int4_t\n variables, and can return the result values written on the region pointed by retval variable which is a pointer of \npg_int4_t *\n data type, as result of PL/CUDA function. \npgfn_int4pl()\n is a runtime function of PG-Strom, declared at \ncuda_mathlib.h\n, which adds two \npg_int4_t\n variables.\n\n\nBelow is an example of execution of this PL/CUDA function. Its two integer arguments (100 and 200) were sent to GPU device, then it wrote back the calculated result (300) from the GPU device. As like normal SQL functions, PL/CUDA function can be used as a part of SQL expression.\n\n\npostgres=# SELECT gpu_add(100,200);\n gpu_add\n---------\n     300\n(1 row)\n\n\n\n\nThe plcuda_function_source function allows showing the source of kernel function generated by the PL/CUDA function. The code block enclosed by the comment: \n/* ---- code by pl/cuda function ---- */\n is the portion injected from the declaration of PL/CUDA function\n\n\npostgres=# SELECT pgstrom.plcuda_function_source('gpu_add'::regproc);\n                     plcuda_function_source\n----------------------------------------------------------------\n #include \ncuda_device_runtime_api.h\n                          +\n                                                               +\n #define HOSTPTRLEN 8                                          +\n #define DEVICEPTRLEN 8                                        +\n #define BLCKSZ 8192                                           +\n #define MAXIMUM_ALIGNOF 8                                     +\n #define MAXIMUM_ALIGNOF_SHIFT 3                               +\n #define PGSTROM_KERNEL_DEBUG 1                                +\n #include \ncuda_common.h\n                                      +\n                                                               +\n #define PG_BOOLOID 16                                         +\n #define PG_INT2OID 21                                         +\n #define PG_INT4OID 23                                         +\n #define PG_INT8OID 20                                         +\n #define PG_FLOAT2OID 237809                                   +\n #define PG_FLOAT4OID 700                                      +\n #define PG_FLOAT8OID 701                                      +\n #define PG_CASHOID 790                                        +\n #define PG_UUIDOID 2950                                       +\n #define PG_MACADDROID 829                                     +\n #define PG_INETOID 869                                        +\n #define PG_CIDROID 650                                        +\n #define PG_DATEOID 1082                                       +\n #define PG_TIMEOID 1083                                       +\n #define PG_TIMETZOID 1266                                     +\n #define PG_TIMESTAMPOID 1114                                  +\n #define PG_TIMESTAMPTZOID 1184                                +\n #define PG_INTERVALOID 1186                                   +\n #define PG_BPCHAROID 1042                                     +\n #define PG_VARCHAROID 1043                                    +\n #define PG_NUMERICOID 1700                                    +\n #define PG_BYTEAOID 17                                        +\n #define PG_TEXTOID 25                                         +\n #define PG_INT4RANGEOID 3904                                  +\n #define PG_INT8RANGEOID 3926                                  +\n #define PG_TSRANGEOID 3908                                    +\n #define PG_TSTZRANGEOID 3910                                  +\n #define PG_DATERANGEOID 3912                                  +\n                                                               +\n #include \ncuda_mathlib.h\n                                     +\n typedef union {                                               +\n     pg_varlena_t     varlena_v;                               +\n     pg_bool_t        bool_v;                                  +\n     pg_int2_t        int2_v;                                  +\n     pg_int4_t        int4_v;                                  +\n     pg_int8_t        int8_v;                                  +\n     pg_float2_t      float2_v;                                +\n     pg_float4_t      float4_v;                                +\n     pg_float8_t      float8_v;                                +\n #ifdef CUDA_NUMERIC_H                                         +\n     pg_numeric_t     numeric_v;                               +\n #endif                                                        +\n #ifdef CUDA_MISC_H                                            +\n     pg_money_t       money_v;                                 +\n     pg_uuid_t        uuid_v;                                  +\n     pg_macaddr_t     macaddr_v;                               +\n     pg_inet_t        inet_v;                                  +\n     pg_cidr_t        cidr_t;                                  +\n #endif                                                        +\n #ifdef CUDA_TIMELIB_H                                         +\n     pg_date_t        date_v;                                  +\n     pg_time_t        time_v;                                  +\n     pg_timestamp_t   timestamp_v;                             +\n     pg_timestamptz_t timestamptz_v;                           +\n #endif                                                        +\n #ifdef CUDA_TEXTLIB_H                                         +\n     pg_bpchar_t      bpchar_v;                                +\n     pg_text_t        text_v;                                  +\n     pg_varchar_t     varchar_v;                               +\n #endif                                                        +\n #ifdef CUDA_RANGETYPE_H                                       +\n     pg_int4range_t   int4range_v;                             +\n     pg_int8range_t   int8range_v;                             +\n #ifdef CUDA_TIMELIB_H                                         +\n     pg_tsrange_t     tsrange_v;                               +\n     pg_tstzrange_t   tstzrange_v;                             +\n     pg_daterange_t   daterange_v;                             +\n #endif                                                        +\n #endif                                                        +\n   } pg_anytype_t;                                             +\n                                                               +\n                                                               +\n #include \ncuda_plcuda.h\n                                      +\n STATIC_INLINE(void)                                           +\n __plcuda_main_kernel(kern_plcuda *kplcuda,                    +\n                    void *workbuf,                             +\n                    void *results,                             +\n                    kern_context *kcxt)                        +\n {                                                             +\n   pg_int4_t *retval __attribute__ ((unused));                 +\n   pg_int4_t arg1 __attribute__((unused));                     +\n   pg_int4_t arg2 __attribute__((unused));                     +\n   assert(sizeof(*retval) \n= sizeof(kplcuda-\n__retval));       +\n   retval = (pg_int4_t *)kplcuda-\n__retval;                    +\n   arg1 = pg_int4_param(kcxt,0);                               +\n   arg2 = pg_int4_param(kcxt,1);                               +\n                                                               +\n   /* ---- code by pl/cuda function ---- */                    +\n   if (get_global_id() == 0)                                   +\n     *retval = pgfn_int4pl(kcxt, arg1, arg2);                  +\n   /* ---- code by pl/cuda function ---- */                    +\n }                                                             +\n                                                               +\n KERNEL_FUNCTION(void)                                         +\n plcuda_main_kernel_entrypoint(kern_plcuda *kplcuda,           +\n             void *workbuf,                                    +\n             void *results)                                    +\n {                                                             +\n   kern_parambuf *kparams = KERN_PLCUDA_PARAMBUF(kplcuda);     +\n   kern_context kcxt;                                          +\n                                                               +\n   assert(kplcuda-\nnargs \n= kparams-\nnparams);                 +\n   INIT_KERNEL_CONTEXT(\nkcxt,plcuda_main_kernel,kparams);      +\n   __plcuda_main_kernel(kplcuda, workbuf, results, \nkcxt);     +\n   kern_writeback_error_status(\nkplcuda-\nkerror_main, \nkcxt.e);+\n }                                                             +\n                                                               +\n                                                               +\n #include \ncuda_terminal.h\n                                    +\n\n(1 row)\n\n\n\n\nPL/CUDA Structure\n\n\nFunction declaration with PL/CUDA is consists of several code blocks split by directives that begin from \n#plcuda_...\n. Only the code block start with \n#plcuda_begin\n is the minimum requirement, and you can add some other code block on demand.\n\n\n#plcuda_decl\n  [...any declarations...]\n#plcuda_prep\n  [...function body of prep kernel...]\n#plcuda_begin\n  [...function body of main kernel...]\n#plcuda_post\n  [...function body of post kernel...]\n#plcuda_end\n\n\n\n\nThe declaration block, which begins with \n#plcuda_decl\n, can have declaration of static functions we can call from other code blocks. Unlike other code blocks, the contents of the code block won't be injected into a particular kernel function, and you need to declare complete static functions. When a kernel function is executed with parallel threads larger than block size on a GPU device, the only way to synchronize between multiple execution units is synchronization of kernel function exit. For example, in case when algorithm is implemented under the assumption of correct initialization of the result buffer, you have to initialize the results buffer first, then you cannot execute the core of algorithm until completion of the initialization. If a part of threads would be executed towards uninitialized buffer, it easily leads incorrect calculation results or crash of execution environment, you always need to avoid.\n\n\nEvery content of user defined code blocks, the preparation block begins from \n#plcuda_prep\n, the main block begins from \n#plcuda_begin\n, and the post-process block begins from \n#plcuda_post\n, shall be injected to the relevant kernel functions. Even though implementation of the preparation block and the post-process block are optional, we will ensure the order to launch the preparation kernel function, the main kernel function, then the post-process kernel function when these code blocks are defined. We intend to use these functions to initialize the results buffer or working buffer prior to execution of the main kernel function, or to summarize the final results next to execution of the main kernel.\n\n\nAn invocation of PL/CUDA function internall contains several SQL functions and launch GPU kernel functions. Prior to the GPU kernel functions, we have to determine the parameters when GPU kernel functions like number of threads, amount of results and working buffer. These parameters depend on the arguments, so PL/CUDA handler determines with other SQL functions that take identical argument signature.\n\n\nOnce we could determine the parameters to call GPU kernel function, PL/CUDA handler loads the arguments of PL/CUDA function onto the argument buffer on GPUs, by DMA copy, on demand.\n\n\nThen, it launches the preparation kernel function (if any), the main kernel function, and the post-process kernel function (if any). Please note that we cannot synchronize GPU threads across the block size boundary, except for the timing of GPU kernel function begin/end. It means, if you expect a particular state exists on the working buffer or results buffer, buffer initialization by preparation kernel then reference of this data structure by the main kernel are required.\n\n\nFinally, PL/CUDA handler writes back the contents of result buffer into the host side. In case when PL/CUDA function returns a fixed-length datum, the code block updates the area pointed by the \nretval\n variable which is initialized prior to execution of the user defined block. In case when PL/CUDA function returns a variable-length datum, \nretval\n points to the area of \npg_varlena_t\n, and its value has to be a reference to the results buffer (\nvoid *results\n), if it is not a \nNULL\n. Please note that it shall not be written back if \nretval\n points out of the results buffer.\n\n\ntypedef struct {\n    varlena    *value;      /* reference to the results buffer */\n    cl_bool     isnull;     /* true, if NULL */\n} pg_varlena_t;\n\n\n\n\n\n\n#plcuda_num_threads\n directive allows specifying the number of threads to execute GPU kernel function. This directive can be used inside of the code block, and takes either a constant value or a SQL function. This SQL function has to be declared to take identical argument types and return bigint type.\n\n\nIn a similar fashion, \n#plcuda_shmem_unitsz\n allows to specify the amount of shared memory per thread, to be acquired on GPU kernel function launch. For example, when a GPU kernel function that consumes 8bytes per thread is launched with 384 threads per streaming-multiprocessor, 3KB of shared memory shall be available. Please note that the number of threads per streaming-multiprocessor shall be automatically calculated during the code optimization, a different concept from what we specify with \n#plcuda_num_threads\n directive.\n\n\n#plcuda_kernel_maxthreads\n directive allows switching optimization policy of the kernel function for the current code block, from maximization of execution efficiency to maximization of number of threads per streaming-multiprocessor (usually 1024). Increase of number of threads per streaming-multiprocessor will improve the performance of workloads which heavily use inter-threads synchronization using shared memory, like reduction operation. On the other hands, it reduces number of registers per thread, needs a right policy in the right place.\n\n\n#plcuda_num_threads (\nvalue\n|\nfunction name\n)\n#plcuda_shmem_unitsz  (\nvalue\n|\nfunction name\n)\n#plcuda_kernel_maxthreads\n\n\n\n\nPL/CUDA\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\n\n\nPL/CUDA Directives\n\n\nThis section is a reference for PL/CUDA function's directives and related SQL functions.\n\n\n#plcuda_begin\n\n\nIt marks beginning of the main kernel function code block. This directive is always required. Prior to execution of the code block on GPU, the arguments of PL/CUDA function are initialized for references by variable names like \narg1\n, \narg2\n, ... These variables have same representation with what PG-Strom represents SQL data types on GPU, for example, an argument of the \nreal\n data type (that is single precision floating point type) is shown as a \npg_float4_t\n type variable as declared below.\n\n\ntypedef struct {\n    cl_float    value;\n    cl_bool     isnull;\n} pg_float4_t;\n\n\n\n\nThese variables are kept in private area of each threads, thus, update of these variables are not reflected on execution of the kernel function on the next step. If you want to share the state between kernel functions, value shall be kept in either the working buffer referenced by the \nvoid *workbuf\n pointer or the results buffer referenced by the \nvoid *results\n pointer.\n\n\n#plcuda_end\n\n\nIt marks end of the kernel function code block. By the way, if a directive to start code block was put inside of the different code block, the current code block is implicitly closed by the \n#plcuda_end\n directive.\n\n\n#plcuda_decl\n\n\nUse of this directive is optional. It marks beginning of the declaration code block that contains the raw code to be declared prior to the definition of any kernel functions. Unlike other code blocks, the contents of this code block shall not be applied as a kernel function, thus, you have to put complete definition of functions.\n\n\n#plcuda_prep\n\n\nUse of this directive is optional. It marks beginning of the preparation code block that shall be executed on GPU prior to the main kernel function; begins from \n#plcuda_begin\n directive. We expect the preparation kernel initializes the results and working buffer. The main kernel shall not be kicked until completion of the preparation kernel. Arguments of PL/CUDA functions can be referenced like as the main kernel function doing.\n\n\n#plcuda_post\n\n\nYou can optionally use this directive. It marks beginning of the post-process code block that shall be executed on GPU next to the main kernel function; begins from \n#plcuda_begin\n directive. We expect the post-process kernel set up the final results to be returned to the CPU side. The post-process kernel shall not be kicked until completion of the preparation kernel. Arguments of PL/CUDA functions can be referenced like as the main kernel function doing.\n\n\n#plcuda_num_threads (\nvalue\n|\nfunction\n)\n\n\nUse of this directive is optional. If not specified, the default is a constant value \n1\n.\nThis directive allows specifying the number of threads to execute the GPU kernel function if it is used in the code block of \n#plcuda_prep\n, \n#plcuda_begin\n, or \n#plcuda_post\n.\nIf a constant value is specified, PL/CUDA runtime kicks the specified number of GPU threads to run the GPU kernel function. If a SQL function name is specified, PL/CUDA runtime call the specified SQL function, and then result of the function shall be applied as the number of GPU threads to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.\n\n\n#plcuda_shmem_unitsz (\nvalue\n|\nfunction\n)\n\n\nUse of this directive is optional. If not specified, the default is a constant value \n0\n.\n\n\nThis directive allows specifying amount of the shared memory per thread to be dinamically allocated on GPU kernel execution, if it is used in the code block of \n#plcuda_prep\n, \n#plcuda_begin\n, or \n#plcuda_post\n.\n\n\nIf a constant value is specified, PL/CUDA runtime kicks GPU kernel function with the specified amount of the shared memory per thread.\n\n\nIf a SQL function name is specified, PL/CUDA runtime call the specified SQL function, and then result of the function shall be applied as the amount of the shared memory per thread to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.\n\n\nPlease note that amount of the shared memory actually acquired on execution of GPU kernel function depends on the number of threads per streaming-multiprocessor, not only the amount of shared memory per thread specified by this directive. (Also note that the number of threads per streaming-multiprocessor is a different concept what we specified using #plcuda_num_threads.) For example, if amount of shared memory per thread is 8 bytes and the number of streaming-multiprocessor is 384, 3KB of shared memory shall be allocated per streaming-multiprocessor. At that time, if the number of total threads specified by #plcuda_num_threads is 32768, this GPU kernel shall be executed with 86 streaming-multiprocessor. However, it is the role of scheduler to determine the timing to put kernels into, so it does not mean that 86 x 3KB = 256KB of the shared memory is consumed at once.\n\n\n#plcuda_shmem_blocksz (\nvalue\n|\nfunction\n)\n\n\nUse of this directive is optional. If not specified, the default is a constant value \n0\n.\n\n\nThis directive allows specifying amount of the shared memory per block to be dinamically allocated on GPU kernel execution, if it is used in the code block of \n#plcuda_prep\n, \n#plcuda_begin\n, or \n#plcuda_post\n.\n\n\nIf a constant value is specified, PL/CUDA runtime kicks GPU kernel function with the specified amount of the shared memory per block.\n\n\nIf a SQL function name is specified, PL/CUDA runtime call the specified SQL function, and then result of the function shall be applied as the amount of the shared memory per block to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.\n\n\n#plcuda_kernel_blocksz (\nvalue\n|\nfunction\n)\n\n\nUse of this directive is optional.\n\n\nThis directive allows specifying the number of threads per streaming-multiprocessor, if it is used in the code block of \n#plcuda_prep\n, \n#plcuda_begin\n, or \n#plcuda_post\n. It is usually a multiple number of the warp value of the device, and equal to or less than \n1024\n. In the default, an optimal value is applied according to the resource consumption of the GPU kernel function, therefore, this directive shall not be used unless you have no special reason; a larger block size is preferable due to characteristics of the algorithm for example.\n\n\nIf a constant value is specified, PL/CUDA runtime kicks GPU kernel function with the specified amount of the shared memory per block.\nIf a SQL function name is specified, PL/CUDA runtime calls the specified SQL function, and then result of the function shall be applied as the amount of the shared memory per block to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns \nbigint\n data type.\n\n\nIncrease the number of threads per streaming-multiprocessor allows more threads to synchronize other threads using the shared memory, on the other hands, it leads decrease of the amount of registers a thread can use, thus, it may have performance degradation by private variables allocation on the (slow) global memory for example.\n\n\n#plcuda_include (\"library name\"|\nfunction name\n)\n\n\nThis directive includes the static GPU library of PG-Strom, or a user defined code block, for use in PL/CUDA functions.\nPlease note that it is NOT a feature to include arbitrary header files on the server system.\n\n\nIf any of the static library name below is specified, PL/CUDA runtime injects the library on the head of the generated CUDA C program. Honestlly, it is a legacy manner, so we expect limited use cases.\n\n\nIf a SQL function name is specified, PL/CUDA runtime calls the specified SQL function, and then result of the function shall be injected to the CUDA C code where \n#plcuda_include\n directive exists. This SQL function takes identical arguments with PL/CUDA function, and returns \ntext\n data type.\n\n\n\n\n\n\n\n\nLibrary name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\"cuda_dynpara.h\"\n\n\nA collection of GPU runtime functions related to dynamic parallelism; that launch kernel functions on GPU. Include of this file also links the device runtime library of CUDA.\n\n\n\n\n\n\n\"cuda_matrix.h\"\n\n\nA collection of GPU runtime functions to process the array type of SQL as if vector/matrix.\n\n\n\n\n\n\n\"cuda_timelib.h\"\n\n\nA collection of GPU runtime functions to process the date and time data type of SQL.\n\n\n\n\n\n\n\"cuda_textlib.h\"\n\n\nA collection of GPU runtime functions to process the text data type and LIKE operator.\n\n\n\n\n\n\n\"cuda_numeric.h\"\n\n\nA a collection of GPU runtime functions to process the \nnumeric\n data type of SQL.\n\n\n\n\n\n\n\"cuda_mathlib.h\"\n\n\nA collection of GPU runtime functions to process the arithmetic operators and mathematic functions of SQL.\n\n\n\n\n\n\n\"cuda_money.h\"\n\n\nA collection of GPU runtime functions to process the currency data type of SQL.\n\n\n\n\n\n\n\"cuda_curand.h\"\n\n\nA collection of GPU runtime functions to use \ncurand\n library which supports random number generation, provided by CUDA.\n\n\n\n\n\n\n\n\n#plcuda_results_bufsz (\nvalue\n|\nfunction\n)\n\n\nUse of this directive is optional. If not specified, the default is a constant value \n0\n.\n\n\nThis directive allows specifying amount of the results buffer in bytes, to be acquired on execution of PL/CUDA function. If PL/CUDA function is declared to return variable length datum, allocation of the results buffer is needed.\n\n\nIf a constant value is specified, PL/CUDA language handler acquires the specified amount of GPU RAM as the results buffer, then launch the GPU kernel functions. If a SQL function name is specified, PL/CUDA language handler call the specified SQL function, then result of the function shall be applied as the amount of GPU RAM for the results buffer and launch the GPU kernel functions. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.\n\n\nGPU kernel functions can access the results buffer as the region pointed by the \nvoid *results\n argument. If \n0\n bytes were specified, \nNULL\n shall be set on the \nvoid *results\n.\n\n\n#plcuda_working_bufsz (\nvalue\n|\nfunction\n)\n\n\nUse of this directive is optional. If not specified, the default is a constant value \n0\n.\n\n\nThis directive allows specifying amount of the working buffer in bytes, to be acquired on execution of PL/CUDA function.\n\n\nIf a constant value is specified, PL/CUDA language handler acquires the specified amount of GPU RAM as the working buffer, and then launch the GPU kernel functions. If a SQL function name is specified, PL/CUDA language handler call the specified SQL function, then result of the function shall be applied as the amount of GPU RAM for the working buffer and launch the GPU kernel functions. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.\n\n\nGPU kernel functions can access the working buffer as the region pointed by the void \nresults argument. If 0 bytes were specified, NULL shall be set on the void \nresults.\n\n\n#plcuda_sanity_checl \nfunction\n\n\nIt allows to specify the sanity check function that preliminary checks adequacy of the supplied arguments, prior to GPU kernel launch.\nNo sanity check function is configured on the default.\nUsually, launch of GPU kernel function is heavier task than call of another function on CPU, because it also involves initialization of GPU devices. If supplied arguments have unacceptable values from the specification of the PL/CUDA function, a few thousands or millions (or more in some cases) of GPU kernel threads shall be launched just to check the arguments and return an error status. If sanity check can be applied prior to the launch of GPU kernel function with enough small cost, it is a valuable idea to raise an error using sanity check function prior to the GPU kernel function. The sanity check function takes identical arguments with PL/CUDA function, and returns \nbool\n data type.\n\n\n#plcuda_cpu_fallback \nfunction\n\n\nIt allows to specify the CPU fallback function that performs as like GPU kernel function. No CPU fallback function is configured on the default.\n\n\nIf GPU kernel function returns StromError_CpuReCheck error and the CPU fallback function is configured, the PL/CUDA language handler discards the results of processing on GPU side, then call the CPU fallback function. It is valuable to implement an alternative remedy, in case when GPU kernel function is not always executable for all possible input; for example, data size may be too large to load onto GPU RAM. Also note that we must have a trade-off of the performance because CPU fallback function shall be executed in CPU single thread.\n\n\nPL/CUDA Related Functions\n\n\n\n\n\n\n\n\nDefinition\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nplcuda_function_source(regproc)\n\n\ntext\n\n\nIt returns source code of the GPU kernel generated from the PL/CUDA function, towards the OID input of PL/CUDA function as argument.\n\n\n\n\n\n\n\n\nArray-Matrix Functions\n\n\nThis section introduces the SQL functions that supports array-based matrix types provided by PG-Strom.\n\n\n\n\n2-dimensional Array\n\n\nElement of array begins from 1 for each dimension\n\n\nNo NULL value is contained\n\n\nLength of the array is less than 1GB, due to the restriction of variable length datum in PostgreSQL\n\n\nArray with \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data type\n\n\n\n\nIf and when the array satisfies the above terms, we can determine the location of (i,j) element of the array by the index uniquely, and it enables GPU thread to fetch the datum to be processed very efficiently. Also, array-based matrix packs only the data to be used for calculation, unlike usual row-based format, so it has advantaged on memory consumption and data transfer.\n\n\n\n\n\n\n\n\nDefinition\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\narray_matrix(variadic arg, ...)\n\n\narray\n\n\nIt is an aggregate function that combines all the rows supplied. For example, when 3 \nfloat\n arguments were supplied by 1000 rows, it returns an array-based matrix of 3 columns X 1000 rows, with \nfloat\n data type.\nThis function is declared to take variable length arguments. The \narg\n takes one or more scalar values of either \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n. All the arg must have same data types.\n\n\n\n\n\n\nmatrix_unnest(array)\n\n\nrecord\n\n\nIt is a set function that extracts the array-based matrix to set of records. \narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. It returns \nrecord\n type which consists of more than one columns according to the width of matrix. For example, in case of a matrix of 10 columns X 500 rows, each records contains 10 columns with element type of the matrix, then it generates 500 of the records. \nIt is similar to the standard \nunnest\n function, but generates \nrecord\n type, thus, it requires to specify the record type to be returned using \nAS (colname1 type[, ...])\n clause.\n\n\n\n\n\n\nrbind(array, array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function combines the supplied two matrices vertically. Both matrices needs to have same element data type. If width of matrices are not equivalent, it fills up the padding area by zero.\n\n\n\n\n\n\nrbind(array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function is similar to \nrbind(array, array)\n, but performs as an aggregate function, then combines all the input matrices into one result vertically.\n\n\n\n\n\n\ncbind(array, array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function combines the supplied two matrices horizontally. Both matrices needs to have same element data type. If height of matrices are not equivalent, it fills up the padding area by zero.\n\n\n\n\n\n\ncbind(array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function is similar to cbind(array, array), but performs as an aggregate function, then combines all the input matrices into one result horizontally.\n\n\n\n\n\n\ntranspose(array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function makes a transposed matrix that swaps height and width of the supplied matrix.\n\n\n\n\n\n\narray_matrix_validation(anyarray)\n\n\nbool\n\n\nIt validates whether the supplied array (\nanyarray\n) is adequate for the array-based matrix. It is intended to use for sanity check prior to invocation of PL/CUDA function, or check constraint on domain type definition.\n\n\n\n\n\n\narray_matrix_height(array)\n\n\nint\n\n\narray\n is an array of either \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function returns the height of the supplied matrix.\n\n\n\n\n\n\narray_matrix_width(array)\n\n\nint\n\n\narray\n is an array of either \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function returns the width of the supplied matrix.\n\n\n\n\n\n\narray_vector_rawsize(regtype,int)\n\n\nbigint\n\n\nIt returns required bytesize to store an array-based vector (1-dimensional array) with data type specified by the 1st argument and height by the 2nd argument. It is intended to use for \n#plcuda_results_bufsz\n and \n#plcuda_working_bufsz\n.\n\n\n\n\n\n\narray_matrix_rawsize(regtype,int,int)\n\n\nbigint\n\n\nIt returns required bytesize to store an array-based matrix with data type specified by the 1st argument, height by the 2nd argument and width by the 3rd argument. It is intended to use for \n#plcuda_results_bufsz\n and \n#plcuda_working_bufsz\n.\n\n\n\n\n\n\narray_cube_rawsize(regtype,int,int,int)\n\n\nbigint\n\n\nIt returns required bytesize to store an array-based cube (3-dimensional array) with data type specified by the 1st argument, height by the 2nd argument, width by the 3rd argument, and depth by the 4th argument. It is intended to use for \n#plcuda_results_bufsz\n and \n#plcuda_working_bufsz\n.", 
            "title": "PL/CUDA"
        }, 
        {
            "location": "/plcuda/#plcuda", 
            "text": "This chapter introduces the way to implement GPU executable native program as SQL functions, using PL/CUDA procedural language.", 
            "title": "PL/CUDA"
        }, 
        {
            "location": "/plcuda/#plcuda-overview", 
            "text": "PG-Strom internally constructs GPU programs by CUDA language, according to the supplied SQL, then generates GPU's native binary using just-in-time compile. CUDA is a programming environment provided by NVIDIA. It allows implementing parallel program which is executable on GPU device, using C-like statement. This transformation process from SQL statement to CUDA program is an internal process, thus, no need to pay attention what GPU programs are generated and executed from the standpoint of users.  On the other hands, PostgreSQL supports to add programming language to implement SQL functions by  CREATE LANGUAGE  statement. PL/CUDA is a language handler to supports  CREATE LANGUAGE  command. It also allows users to run arbitrary GPU programs manually implemented as SQL functions, but not only GPU programs automatically generated by PG-Strom based on SQL.  Its argument can take the data types supported by PG-Strom, like numeric, text, or array-matrix data type. These arguments are implicitly loaded onto GPU device memory by the PL/CUDA infrastructure, so users don't need to pay attention for data loading between the database and GPU devices. In a similar fashion, the return value of PL/CUDA function (including the case of variable length data type) will be written back to CPU from GPU, then decode to the result of SQL function.  You can also use foreign tables defined with  gstore_fdw  as arguments of PL/CUDA function. In this case, no need to load the data onto GPU for each invocation because foreign table already keeps the data, and available to use larger data than 1GB which is a restriction of variable length data in PostgreSQL.  Therefore, users can focus on productive tasks like implementation of statistical analysis, code optimization and so on, without routine process like data input/output between GPU and databases.  \u3053\u3053\u306b\u7d75  Once a PL/CUDA function is declared with CREATE FUNCTION statement, it generates a CUDA program that embeds the definition of this function on the GPU's kernel function at the execution time. This kernel function contains initialization code to reference this PL/CUDA functions and auxiliary code to return run-time error to CPU side. Also, it can include some run-time functions to support execution of PG-Strom.  Here is no special memory protection mechanism on the native CUDA program made with PL/CUDA function, thus, execution of buggy PL/CUDA function can crash GPU execution environment or PostreSQL infrastructure in some cases. Thus, only database superuser can define PL/CUDA function.  Below is an example of simple PL/CUDA function. This function takes two int arguments, and then returns the sum of them with int data type.  postgres=# CREATE FUNCTION gpu_add(int, int)\nRETURNS int\nAS $$\n#plcuda_include  cuda_mathlib.h \n#plcuda_begin\n  if (get_global_id() == 0)\n    *retval = pgfn_int4pl(kcxt, arg1, arg2);\n#plcuda_end\n$$ LANGUAGE plcuda;\nCREATE FUNCTION  The code block enclosed by  #plcuda_begin  and  #plcuda_end  is main portion of PL/CUDA function. This kernel function can reference the  int  type argument as  arg1  and  arg2  which are  pg_int4_t  variables, and can return the result values written on the region pointed by retval variable which is a pointer of  pg_int4_t *  data type, as result of PL/CUDA function.  pgfn_int4pl()  is a runtime function of PG-Strom, declared at  cuda_mathlib.h , which adds two  pg_int4_t  variables.  Below is an example of execution of this PL/CUDA function. Its two integer arguments (100 and 200) were sent to GPU device, then it wrote back the calculated result (300) from the GPU device. As like normal SQL functions, PL/CUDA function can be used as a part of SQL expression.  postgres=# SELECT gpu_add(100,200);\n gpu_add\n---------\n     300\n(1 row)  The plcuda_function_source function allows showing the source of kernel function generated by the PL/CUDA function. The code block enclosed by the comment:  /* ---- code by pl/cuda function ---- */  is the portion injected from the declaration of PL/CUDA function  postgres=# SELECT pgstrom.plcuda_function_source('gpu_add'::regproc);\n                     plcuda_function_source\n----------------------------------------------------------------\n #include  cuda_device_runtime_api.h                           +\n                                                               +\n #define HOSTPTRLEN 8                                          +\n #define DEVICEPTRLEN 8                                        +\n #define BLCKSZ 8192                                           +\n #define MAXIMUM_ALIGNOF 8                                     +\n #define MAXIMUM_ALIGNOF_SHIFT 3                               +\n #define PGSTROM_KERNEL_DEBUG 1                                +\n #include  cuda_common.h                                       +\n                                                               +\n #define PG_BOOLOID 16                                         +\n #define PG_INT2OID 21                                         +\n #define PG_INT4OID 23                                         +\n #define PG_INT8OID 20                                         +\n #define PG_FLOAT2OID 237809                                   +\n #define PG_FLOAT4OID 700                                      +\n #define PG_FLOAT8OID 701                                      +\n #define PG_CASHOID 790                                        +\n #define PG_UUIDOID 2950                                       +\n #define PG_MACADDROID 829                                     +\n #define PG_INETOID 869                                        +\n #define PG_CIDROID 650                                        +\n #define PG_DATEOID 1082                                       +\n #define PG_TIMEOID 1083                                       +\n #define PG_TIMETZOID 1266                                     +\n #define PG_TIMESTAMPOID 1114                                  +\n #define PG_TIMESTAMPTZOID 1184                                +\n #define PG_INTERVALOID 1186                                   +\n #define PG_BPCHAROID 1042                                     +\n #define PG_VARCHAROID 1043                                    +\n #define PG_NUMERICOID 1700                                    +\n #define PG_BYTEAOID 17                                        +\n #define PG_TEXTOID 25                                         +\n #define PG_INT4RANGEOID 3904                                  +\n #define PG_INT8RANGEOID 3926                                  +\n #define PG_TSRANGEOID 3908                                    +\n #define PG_TSTZRANGEOID 3910                                  +\n #define PG_DATERANGEOID 3912                                  +\n                                                               +\n #include  cuda_mathlib.h                                      +\n typedef union {                                               +\n     pg_varlena_t     varlena_v;                               +\n     pg_bool_t        bool_v;                                  +\n     pg_int2_t        int2_v;                                  +\n     pg_int4_t        int4_v;                                  +\n     pg_int8_t        int8_v;                                  +\n     pg_float2_t      float2_v;                                +\n     pg_float4_t      float4_v;                                +\n     pg_float8_t      float8_v;                                +\n #ifdef CUDA_NUMERIC_H                                         +\n     pg_numeric_t     numeric_v;                               +\n #endif                                                        +\n #ifdef CUDA_MISC_H                                            +\n     pg_money_t       money_v;                                 +\n     pg_uuid_t        uuid_v;                                  +\n     pg_macaddr_t     macaddr_v;                               +\n     pg_inet_t        inet_v;                                  +\n     pg_cidr_t        cidr_t;                                  +\n #endif                                                        +\n #ifdef CUDA_TIMELIB_H                                         +\n     pg_date_t        date_v;                                  +\n     pg_time_t        time_v;                                  +\n     pg_timestamp_t   timestamp_v;                             +\n     pg_timestamptz_t timestamptz_v;                           +\n #endif                                                        +\n #ifdef CUDA_TEXTLIB_H                                         +\n     pg_bpchar_t      bpchar_v;                                +\n     pg_text_t        text_v;                                  +\n     pg_varchar_t     varchar_v;                               +\n #endif                                                        +\n #ifdef CUDA_RANGETYPE_H                                       +\n     pg_int4range_t   int4range_v;                             +\n     pg_int8range_t   int8range_v;                             +\n #ifdef CUDA_TIMELIB_H                                         +\n     pg_tsrange_t     tsrange_v;                               +\n     pg_tstzrange_t   tstzrange_v;                             +\n     pg_daterange_t   daterange_v;                             +\n #endif                                                        +\n #endif                                                        +\n   } pg_anytype_t;                                             +\n                                                               +\n                                                               +\n #include  cuda_plcuda.h                                       +\n STATIC_INLINE(void)                                           +\n __plcuda_main_kernel(kern_plcuda *kplcuda,                    +\n                    void *workbuf,                             +\n                    void *results,                             +\n                    kern_context *kcxt)                        +\n {                                                             +\n   pg_int4_t *retval __attribute__ ((unused));                 +\n   pg_int4_t arg1 __attribute__((unused));                     +\n   pg_int4_t arg2 __attribute__((unused));                     +\n   assert(sizeof(*retval)  = sizeof(kplcuda- __retval));       +\n   retval = (pg_int4_t *)kplcuda- __retval;                    +\n   arg1 = pg_int4_param(kcxt,0);                               +\n   arg2 = pg_int4_param(kcxt,1);                               +\n                                                               +\n   /* ---- code by pl/cuda function ---- */                    +\n   if (get_global_id() == 0)                                   +\n     *retval = pgfn_int4pl(kcxt, arg1, arg2);                  +\n   /* ---- code by pl/cuda function ---- */                    +\n }                                                             +\n                                                               +\n KERNEL_FUNCTION(void)                                         +\n plcuda_main_kernel_entrypoint(kern_plcuda *kplcuda,           +\n             void *workbuf,                                    +\n             void *results)                                    +\n {                                                             +\n   kern_parambuf *kparams = KERN_PLCUDA_PARAMBUF(kplcuda);     +\n   kern_context kcxt;                                          +\n                                                               +\n   assert(kplcuda- nargs  = kparams- nparams);                 +\n   INIT_KERNEL_CONTEXT( kcxt,plcuda_main_kernel,kparams);      +\n   __plcuda_main_kernel(kplcuda, workbuf, results,  kcxt);     +\n   kern_writeback_error_status( kplcuda- kerror_main,  kcxt.e);+\n }                                                             +\n                                                               +\n                                                               +\n #include  cuda_terminal.h                                     +\n\n(1 row)", 
            "title": "PL/CUDA Overview"
        }, 
        {
            "location": "/plcuda/#plcuda-structure", 
            "text": "Function declaration with PL/CUDA is consists of several code blocks split by directives that begin from  #plcuda_... . Only the code block start with  #plcuda_begin  is the minimum requirement, and you can add some other code block on demand.  #plcuda_decl\n  [...any declarations...]\n#plcuda_prep\n  [...function body of prep kernel...]\n#plcuda_begin\n  [...function body of main kernel...]\n#plcuda_post\n  [...function body of post kernel...]\n#plcuda_end  The declaration block, which begins with  #plcuda_decl , can have declaration of static functions we can call from other code blocks. Unlike other code blocks, the contents of the code block won't be injected into a particular kernel function, and you need to declare complete static functions. When a kernel function is executed with parallel threads larger than block size on a GPU device, the only way to synchronize between multiple execution units is synchronization of kernel function exit. For example, in case when algorithm is implemented under the assumption of correct initialization of the result buffer, you have to initialize the results buffer first, then you cannot execute the core of algorithm until completion of the initialization. If a part of threads would be executed towards uninitialized buffer, it easily leads incorrect calculation results or crash of execution environment, you always need to avoid.  Every content of user defined code blocks, the preparation block begins from  #plcuda_prep , the main block begins from  #plcuda_begin , and the post-process block begins from  #plcuda_post , shall be injected to the relevant kernel functions. Even though implementation of the preparation block and the post-process block are optional, we will ensure the order to launch the preparation kernel function, the main kernel function, then the post-process kernel function when these code blocks are defined. We intend to use these functions to initialize the results buffer or working buffer prior to execution of the main kernel function, or to summarize the final results next to execution of the main kernel.  An invocation of PL/CUDA function internall contains several SQL functions and launch GPU kernel functions. Prior to the GPU kernel functions, we have to determine the parameters when GPU kernel functions like number of threads, amount of results and working buffer. These parameters depend on the arguments, so PL/CUDA handler determines with other SQL functions that take identical argument signature.  Once we could determine the parameters to call GPU kernel function, PL/CUDA handler loads the arguments of PL/CUDA function onto the argument buffer on GPUs, by DMA copy, on demand.  Then, it launches the preparation kernel function (if any), the main kernel function, and the post-process kernel function (if any). Please note that we cannot synchronize GPU threads across the block size boundary, except for the timing of GPU kernel function begin/end. It means, if you expect a particular state exists on the working buffer or results buffer, buffer initialization by preparation kernel then reference of this data structure by the main kernel are required.  Finally, PL/CUDA handler writes back the contents of result buffer into the host side. In case when PL/CUDA function returns a fixed-length datum, the code block updates the area pointed by the  retval  variable which is initialized prior to execution of the user defined block. In case when PL/CUDA function returns a variable-length datum,  retval  points to the area of  pg_varlena_t , and its value has to be a reference to the results buffer ( void *results ), if it is not a  NULL . Please note that it shall not be written back if  retval  points out of the results buffer.  typedef struct {\n    varlena    *value;      /* reference to the results buffer */\n    cl_bool     isnull;     /* true, if NULL */\n} pg_varlena_t;   #plcuda_num_threads  directive allows specifying the number of threads to execute GPU kernel function. This directive can be used inside of the code block, and takes either a constant value or a SQL function. This SQL function has to be declared to take identical argument types and return bigint type.  In a similar fashion,  #plcuda_shmem_unitsz  allows to specify the amount of shared memory per thread, to be acquired on GPU kernel function launch. For example, when a GPU kernel function that consumes 8bytes per thread is launched with 384 threads per streaming-multiprocessor, 3KB of shared memory shall be available. Please note that the number of threads per streaming-multiprocessor shall be automatically calculated during the code optimization, a different concept from what we specify with  #plcuda_num_threads  directive.  #plcuda_kernel_maxthreads  directive allows switching optimization policy of the kernel function for the current code block, from maximization of execution efficiency to maximization of number of threads per streaming-multiprocessor (usually 1024). Increase of number of threads per streaming-multiprocessor will improve the performance of workloads which heavily use inter-threads synchronization using shared memory, like reduction operation. On the other hands, it reduces number of registers per thread, needs a right policy in the right place.  #plcuda_num_threads ( value | function name )\n#plcuda_shmem_unitsz  ( value | function name )\n#plcuda_kernel_maxthreads", 
            "title": "PL/CUDA Structure"
        }, 
        {
            "location": "/plcuda/#plcuda_1", 
            "text": "", 
            "title": "PL/CUDA\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9"
        }, 
        {
            "location": "/plcuda/#plcuda-directives", 
            "text": "This section is a reference for PL/CUDA function's directives and related SQL functions.", 
            "title": "PL/CUDA Directives"
        }, 
        {
            "location": "/plcuda/#plcuda_begin", 
            "text": "It marks beginning of the main kernel function code block. This directive is always required. Prior to execution of the code block on GPU, the arguments of PL/CUDA function are initialized for references by variable names like  arg1 ,  arg2 , ... These variables have same representation with what PG-Strom represents SQL data types on GPU, for example, an argument of the  real  data type (that is single precision floating point type) is shown as a  pg_float4_t  type variable as declared below.  typedef struct {\n    cl_float    value;\n    cl_bool     isnull;\n} pg_float4_t;  These variables are kept in private area of each threads, thus, update of these variables are not reflected on execution of the kernel function on the next step. If you want to share the state between kernel functions, value shall be kept in either the working buffer referenced by the  void *workbuf  pointer or the results buffer referenced by the  void *results  pointer.", 
            "title": "#plcuda_begin"
        }, 
        {
            "location": "/plcuda/#plcuda_end", 
            "text": "It marks end of the kernel function code block. By the way, if a directive to start code block was put inside of the different code block, the current code block is implicitly closed by the  #plcuda_end  directive.", 
            "title": "#plcuda_end"
        }, 
        {
            "location": "/plcuda/#plcuda_decl", 
            "text": "Use of this directive is optional. It marks beginning of the declaration code block that contains the raw code to be declared prior to the definition of any kernel functions. Unlike other code blocks, the contents of this code block shall not be applied as a kernel function, thus, you have to put complete definition of functions.", 
            "title": "#plcuda_decl"
        }, 
        {
            "location": "/plcuda/#plcuda_prep", 
            "text": "Use of this directive is optional. It marks beginning of the preparation code block that shall be executed on GPU prior to the main kernel function; begins from  #plcuda_begin  directive. We expect the preparation kernel initializes the results and working buffer. The main kernel shall not be kicked until completion of the preparation kernel. Arguments of PL/CUDA functions can be referenced like as the main kernel function doing.", 
            "title": "#plcuda_prep"
        }, 
        {
            "location": "/plcuda/#plcuda_post", 
            "text": "You can optionally use this directive. It marks beginning of the post-process code block that shall be executed on GPU next to the main kernel function; begins from  #plcuda_begin  directive. We expect the post-process kernel set up the final results to be returned to the CPU side. The post-process kernel shall not be kicked until completion of the preparation kernel. Arguments of PL/CUDA functions can be referenced like as the main kernel function doing.", 
            "title": "#plcuda_post"
        }, 
        {
            "location": "/plcuda/#plcuda_num_threads-valuefunction", 
            "text": "Use of this directive is optional. If not specified, the default is a constant value  1 .\nThis directive allows specifying the number of threads to execute the GPU kernel function if it is used in the code block of  #plcuda_prep ,  #plcuda_begin , or  #plcuda_post .\nIf a constant value is specified, PL/CUDA runtime kicks the specified number of GPU threads to run the GPU kernel function. If a SQL function name is specified, PL/CUDA runtime call the specified SQL function, and then result of the function shall be applied as the number of GPU threads to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.", 
            "title": "#plcuda_num_threads (&lt;value&gt;|&lt;function&gt;)"
        }, 
        {
            "location": "/plcuda/#plcuda_shmem_unitsz-valuefunction", 
            "text": "Use of this directive is optional. If not specified, the default is a constant value  0 .  This directive allows specifying amount of the shared memory per thread to be dinamically allocated on GPU kernel execution, if it is used in the code block of  #plcuda_prep ,  #plcuda_begin , or  #plcuda_post .  If a constant value is specified, PL/CUDA runtime kicks GPU kernel function with the specified amount of the shared memory per thread.  If a SQL function name is specified, PL/CUDA runtime call the specified SQL function, and then result of the function shall be applied as the amount of the shared memory per thread to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.  Please note that amount of the shared memory actually acquired on execution of GPU kernel function depends on the number of threads per streaming-multiprocessor, not only the amount of shared memory per thread specified by this directive. (Also note that the number of threads per streaming-multiprocessor is a different concept what we specified using #plcuda_num_threads.) For example, if amount of shared memory per thread is 8 bytes and the number of streaming-multiprocessor is 384, 3KB of shared memory shall be allocated per streaming-multiprocessor. At that time, if the number of total threads specified by #plcuda_num_threads is 32768, this GPU kernel shall be executed with 86 streaming-multiprocessor. However, it is the role of scheduler to determine the timing to put kernels into, so it does not mean that 86 x 3KB = 256KB of the shared memory is consumed at once.", 
            "title": "#plcuda_shmem_unitsz (&lt;value&gt;|&lt;function&gt;)"
        }, 
        {
            "location": "/plcuda/#plcuda_shmem_blocksz-valuefunction", 
            "text": "Use of this directive is optional. If not specified, the default is a constant value  0 .  This directive allows specifying amount of the shared memory per block to be dinamically allocated on GPU kernel execution, if it is used in the code block of  #plcuda_prep ,  #plcuda_begin , or  #plcuda_post .  If a constant value is specified, PL/CUDA runtime kicks GPU kernel function with the specified amount of the shared memory per block.  If a SQL function name is specified, PL/CUDA runtime call the specified SQL function, and then result of the function shall be applied as the amount of the shared memory per block to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.", 
            "title": "#plcuda_shmem_blocksz (&lt;value&gt;|&lt;function&gt;)"
        }, 
        {
            "location": "/plcuda/#plcuda_kernel_blocksz-valuefunction", 
            "text": "Use of this directive is optional.  This directive allows specifying the number of threads per streaming-multiprocessor, if it is used in the code block of  #plcuda_prep ,  #plcuda_begin , or  #plcuda_post . It is usually a multiple number of the warp value of the device, and equal to or less than  1024 . In the default, an optimal value is applied according to the resource consumption of the GPU kernel function, therefore, this directive shall not be used unless you have no special reason; a larger block size is preferable due to characteristics of the algorithm for example.  If a constant value is specified, PL/CUDA runtime kicks GPU kernel function with the specified amount of the shared memory per block.\nIf a SQL function name is specified, PL/CUDA runtime calls the specified SQL function, and then result of the function shall be applied as the amount of the shared memory per block to run the GPU kernel function. This SQL function takes identical arguments with PL/CUDA function, and returns  bigint  data type.  Increase the number of threads per streaming-multiprocessor allows more threads to synchronize other threads using the shared memory, on the other hands, it leads decrease of the amount of registers a thread can use, thus, it may have performance degradation by private variables allocation on the (slow) global memory for example.", 
            "title": "#plcuda_kernel_blocksz (&lt;value&gt;|&lt;function&gt;)"
        }, 
        {
            "location": "/plcuda/#plcuda_include-library-namefunction-name", 
            "text": "This directive includes the static GPU library of PG-Strom, or a user defined code block, for use in PL/CUDA functions.\nPlease note that it is NOT a feature to include arbitrary header files on the server system.  If any of the static library name below is specified, PL/CUDA runtime injects the library on the head of the generated CUDA C program. Honestlly, it is a legacy manner, so we expect limited use cases.  If a SQL function name is specified, PL/CUDA runtime calls the specified SQL function, and then result of the function shall be injected to the CUDA C code where  #plcuda_include  directive exists. This SQL function takes identical arguments with PL/CUDA function, and returns  text  data type.     Library name  Description      \"cuda_dynpara.h\"  A collection of GPU runtime functions related to dynamic parallelism; that launch kernel functions on GPU. Include of this file also links the device runtime library of CUDA.    \"cuda_matrix.h\"  A collection of GPU runtime functions to process the array type of SQL as if vector/matrix.    \"cuda_timelib.h\"  A collection of GPU runtime functions to process the date and time data type of SQL.    \"cuda_textlib.h\"  A collection of GPU runtime functions to process the text data type and LIKE operator.    \"cuda_numeric.h\"  A a collection of GPU runtime functions to process the  numeric  data type of SQL.    \"cuda_mathlib.h\"  A collection of GPU runtime functions to process the arithmetic operators and mathematic functions of SQL.    \"cuda_money.h\"  A collection of GPU runtime functions to process the currency data type of SQL.    \"cuda_curand.h\"  A collection of GPU runtime functions to use  curand  library which supports random number generation, provided by CUDA.", 
            "title": "#plcuda_include (\"library name\"|&lt;function name&gt;)"
        }, 
        {
            "location": "/plcuda/#plcuda_results_bufsz-valuefunction", 
            "text": "Use of this directive is optional. If not specified, the default is a constant value  0 .  This directive allows specifying amount of the results buffer in bytes, to be acquired on execution of PL/CUDA function. If PL/CUDA function is declared to return variable length datum, allocation of the results buffer is needed.  If a constant value is specified, PL/CUDA language handler acquires the specified amount of GPU RAM as the results buffer, then launch the GPU kernel functions. If a SQL function name is specified, PL/CUDA language handler call the specified SQL function, then result of the function shall be applied as the amount of GPU RAM for the results buffer and launch the GPU kernel functions. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.  GPU kernel functions can access the results buffer as the region pointed by the  void *results  argument. If  0  bytes were specified,  NULL  shall be set on the  void *results .", 
            "title": "#plcuda_results_bufsz (&lt;value&gt;|&lt;function&gt;)"
        }, 
        {
            "location": "/plcuda/#plcuda_working_bufsz-valuefunction", 
            "text": "Use of this directive is optional. If not specified, the default is a constant value  0 .  This directive allows specifying amount of the working buffer in bytes, to be acquired on execution of PL/CUDA function.  If a constant value is specified, PL/CUDA language handler acquires the specified amount of GPU RAM as the working buffer, and then launch the GPU kernel functions. If a SQL function name is specified, PL/CUDA language handler call the specified SQL function, then result of the function shall be applied as the amount of GPU RAM for the working buffer and launch the GPU kernel functions. This SQL function takes identical arguments with PL/CUDA function, and returns bigint data type.  GPU kernel functions can access the working buffer as the region pointed by the void  results argument. If 0 bytes were specified, NULL shall be set on the void  results.", 
            "title": "#plcuda_working_bufsz (&lt;value&gt;|&lt;function&gt;)"
        }, 
        {
            "location": "/plcuda/#plcuda_sanity_checl-function", 
            "text": "It allows to specify the sanity check function that preliminary checks adequacy of the supplied arguments, prior to GPU kernel launch.\nNo sanity check function is configured on the default.\nUsually, launch of GPU kernel function is heavier task than call of another function on CPU, because it also involves initialization of GPU devices. If supplied arguments have unacceptable values from the specification of the PL/CUDA function, a few thousands or millions (or more in some cases) of GPU kernel threads shall be launched just to check the arguments and return an error status. If sanity check can be applied prior to the launch of GPU kernel function with enough small cost, it is a valuable idea to raise an error using sanity check function prior to the GPU kernel function. The sanity check function takes identical arguments with PL/CUDA function, and returns  bool  data type.", 
            "title": "#plcuda_sanity_checl &lt;function&gt;"
        }, 
        {
            "location": "/plcuda/#plcuda_cpu_fallback-function", 
            "text": "It allows to specify the CPU fallback function that performs as like GPU kernel function. No CPU fallback function is configured on the default.  If GPU kernel function returns StromError_CpuReCheck error and the CPU fallback function is configured, the PL/CUDA language handler discards the results of processing on GPU side, then call the CPU fallback function. It is valuable to implement an alternative remedy, in case when GPU kernel function is not always executable for all possible input; for example, data size may be too large to load onto GPU RAM. Also note that we must have a trade-off of the performance because CPU fallback function shall be executed in CPU single thread.", 
            "title": "#plcuda_cpu_fallback &lt;function&gt;"
        }, 
        {
            "location": "/plcuda/#plcuda-related-functions", 
            "text": "Definition  Result  Description      plcuda_function_source(regproc)  text  It returns source code of the GPU kernel generated from the PL/CUDA function, towards the OID input of PL/CUDA function as argument.", 
            "title": "PL/CUDA Related Functions"
        }, 
        {
            "location": "/plcuda/#array-matrix-functions", 
            "text": "This section introduces the SQL functions that supports array-based matrix types provided by PG-Strom.   2-dimensional Array  Element of array begins from 1 for each dimension  No NULL value is contained  Length of the array is less than 1GB, due to the restriction of variable length datum in PostgreSQL  Array with  smallint ,  int ,  bigint ,  real  or  float  data type   If and when the array satisfies the above terms, we can determine the location of (i,j) element of the array by the index uniquely, and it enables GPU thread to fetch the datum to be processed very efficiently. Also, array-based matrix packs only the data to be used for calculation, unlike usual row-based format, so it has advantaged on memory consumption and data transfer.     Definition  Result  Description      array_matrix(variadic arg, ...)  array  It is an aggregate function that combines all the rows supplied. For example, when 3  float  arguments were supplied by 1000 rows, it returns an array-based matrix of 3 columns X 1000 rows, with  float  data type. This function is declared to take variable length arguments. The  arg  takes one or more scalar values of either  smallint ,  int ,  bigint ,  real  or  float . All the arg must have same data types.    matrix_unnest(array)  record  It is a set function that extracts the array-based matrix to set of records.  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. It returns  record  type which consists of more than one columns according to the width of matrix. For example, in case of a matrix of 10 columns X 500 rows, each records contains 10 columns with element type of the matrix, then it generates 500 of the records.  It is similar to the standard  unnest  function, but generates  record  type, thus, it requires to specify the record type to be returned using  AS (colname1 type[, ...])  clause.    rbind(array, array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function combines the supplied two matrices vertically. Both matrices needs to have same element data type. If width of matrices are not equivalent, it fills up the padding area by zero.    rbind(array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function is similar to  rbind(array, array) , but performs as an aggregate function, then combines all the input matrices into one result vertically.    cbind(array, array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function combines the supplied two matrices horizontally. Both matrices needs to have same element data type. If height of matrices are not equivalent, it fills up the padding area by zero.    cbind(array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function is similar to cbind(array, array), but performs as an aggregate function, then combines all the input matrices into one result horizontally.    transpose(array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function makes a transposed matrix that swaps height and width of the supplied matrix.    array_matrix_validation(anyarray)  bool  It validates whether the supplied array ( anyarray ) is adequate for the array-based matrix. It is intended to use for sanity check prior to invocation of PL/CUDA function, or check constraint on domain type definition.    array_matrix_height(array)  int  array  is an array of either  smallint ,  int ,  bigint ,  real  or  float  data. This function returns the height of the supplied matrix.    array_matrix_width(array)  int  array  is an array of either  smallint ,  int ,  bigint ,  real  or  float  data. This function returns the width of the supplied matrix.    array_vector_rawsize(regtype,int)  bigint  It returns required bytesize to store an array-based vector (1-dimensional array) with data type specified by the 1st argument and height by the 2nd argument. It is intended to use for  #plcuda_results_bufsz  and  #plcuda_working_bufsz .    array_matrix_rawsize(regtype,int,int)  bigint  It returns required bytesize to store an array-based matrix with data type specified by the 1st argument, height by the 2nd argument and width by the 3rd argument. It is intended to use for  #plcuda_results_bufsz  and  #plcuda_working_bufsz .    array_cube_rawsize(regtype,int,int,int)  bigint  It returns required bytesize to store an array-based cube (3-dimensional array) with data type specified by the 1st argument, height by the 2nd argument, width by the 3rd argument, and depth by the 4th argument. It is intended to use for  #plcuda_results_bufsz  and  #plcuda_working_bufsz .", 
            "title": "Array-Matrix Functions"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\nSupported Data Types\n\n\nPG-Strom support the following data types for use on GPU device.\n\n\nBuilt-in numeric types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nsmallint\n\n\ncl_short\n\n\n2 bytes\n\n\n\n\n\n\n\n\ninteger\n\n\ncl_int\n\n\n4 bytes\n\n\n\n\n\n\n\n\nbigint\n\n\ncl_long\n\n\n8 bytes\n\n\n\n\n\n\n\n\nreal\n\n\ncl_float\n\n\n4 bytes\n\n\n\n\n\n\n\n\nfloat\n\n\ncl_double\n\n\n8 bytes\n\n\n\n\n\n\n\n\nnumeric\n\n\ncl_ulong\n\n\nvariable length\n\n\nmapped to 64bit internal format\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhen GPU processes values in \nnumeric\n data type, it is converted to an internal 64bit format because of implementation reason.\nIt is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert \nnumaric\n datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if \nnumeric\n data with large number of digits are supplied to GPU device.\nTo avoid the problem, turn off the GUC option \npg_strom.enable_numeric_type\n not to run operational expression including \nnumeric\n data types on GPU devices.\n\n\n\n\nBuilt-in date and time types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\ndate\n\n\nDateADT\n\n\n4 bytes\n\n\n\n\n\n\n\n\ntime\n\n\nTimeADT\n\n\n8 bytes\n\n\n\n\n\n\n\n\ntimetz\n\n\nTimeTzADT\n\n\n12 bytes\n\n\n\n\n\n\n\n\ntimestamp\n\n\nTimestamp\n\n\n8 bytes\n\n\n\n\n\n\n\n\ntimestamptz\n\n\nTimestampTz\n\n\n8 bytes\n\n\n\n\n\n\n\n\ninterval\n\n\nInterval\n\n\n16 bytes\n\n\n\n\n\n\n\n\n\n\nBuilt-in variable length types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nbpchar\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\nvarchar\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\nbytea\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\ntext\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\n\n\nBuilt-in miscellaneous types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nboolean\n\n\ncl_bool\n\n\n1 byte\n\n\n\n\n\n\n\n\nmoney\n\n\ncl_long\n\n\n8 bytes\n\n\n\n\n\n\n\n\nuuid\n\n\npg_uuid\n\n\n16 bytes\n\n\n\n\n\n\n\n\nmacaddr\n\n\nmacaddr\n\n\n6 bytes\n\n\n\n\n\n\n\n\ninet\n\n\ninet_struct\n\n\n7 bytes or 19 bytes\n\n\n\n\n\n\n\n\ncidr\n\n\ninet_struct\n\n\n7 bytes or 19 bytes\n\n\n\n\n\n\n\n\n\n\nBuilt-in range data types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nint4range\n\n\n__int4range\n\n\n14 bytes\n\n\n\n\n\n\n\n\nint8range\n\n\n__int8range\n\n\n22 bytes\n\n\n\n\n\n\n\n\ntsrange\n\n\n__tsrange\n\n\n22 bytes\n\n\n\n\n\n\n\n\ntstzrange\n\n\n__tstzrange\n\n\n22 bytes\n\n\n\n\n\n\n\n\ndaterange\n\n\n__daterange\n\n\n14 bytes\n\n\n\n\n\n\n\n\n\n\nExtra Types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nfloat2\n\n\nhalf_t\n\n\n2 bytes\n\n\nHalf precision data type\n\n\n\n\n\n\n\n\nFunctions and Operators\n\n\nDevice information functions\n\n\nSystem View\n\n\nPG-Strom provides several system view to export its internal state for users or applications.\nThe future version may add extra fields here. So, it is not recommended to reference these information schemas using \nSELECT * FROM ...\n.\n\n\npgstrom.device_info\n\n\npgstrom.device_into\n system view exports device attributes of the GPUs recognized by PG-Strom.\nGPU has different specification for each model, like number of cores, capacity of global memory, maximum number of threads and etc, user's software should be optimized according to the information if you try raw GPU programming with PL/CUDA functions.\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndevice_nr\n\n\nint\n\n\nGPU device number\n\n\n\n\n\n\naindex\n\n\nint\n\n\nAttribute index\n\n\n\n\n\n\nattribute\n\n\ntext\n\n\nAttribute name\n\n\n\n\n\n\nvalue\n\n\ntext\n\n\nValue of the attribute\n\n\n\n\n\n\n\n\npgstrom.device_preserved_meminfo\n\n\npgstrom.device_preserved_meminfo\n system view exports information of the preserved device memory; which can be shared multiple PostgreSQL backend.\nRight now, only gstore_fdw uses this feature.\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndevice_nr\n\n\nint\n\n\nGPU device number\n\n\n\n\n\n\nhandle\n\n\nbytea\n\n\nIPC handle of the preserved device memory\n\n\n\n\n\n\nowner\n\n\nregrole\n\n\nOwner of the preserved device memory\n\n\n\n\n\n\nlength\n\n\nbigint\n\n\nLength of the preserved device memory in bytes\n\n\n\n\n\n\nctime\n\n\ntimestamp with time zone\n\n\nTimestamp when the preserved device memory is created\n\n\n\n\n\n\n\n\npgstrom.ccache_info\n\n\npgstrom.ccache_info\n system view exports attribute of the columnar-cache chunks (128MB unit for each).\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndatabase_id\n\n\noid\n\n\nDatabase Id\n\n\n\n\n\n\ntable_id\n\n\nregclass\n\n\nTable Id\n\n\n\n\n\n\nblock_nr\n\n\nint\n\n\nHead block-number of the chunk\n\n\n\n\n\n\nnitems\n\n\nbigint\n\n\nNumber of rows in the chunk\n\n\n\n\n\n\nlength\n\n\nbigint\n\n\nRaw size of the cached chunk\n\n\n\n\n\n\nctime\n\n\ntimestamp with time zone\n\n\nTimestamp of the chunk creation\n\n\n\n\n\n\natime\n\n\ntimestamp with time zone\n\n\nTimestamp of the least access to the chunk\n\n\n\n\n\n\n\n\npgstrom.ccache_builder_info\n\n\npgstrom.ccache_builder_info\n system view exports information of asynchronous builder process of columnar cache.\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbuilder_id\n\n\nint\n\n\nAsynchronous builder Id of columnar cache\n\n\n\n\n\n\nstate\n\n\ntext\n\n\nState of the builder process (\nshutdown\n, \nstartup\n, \nloading\n or \nsleep\n)\n\n\n\n\n\n\ndatabase_id\n\n\noid\n\n\nDatabase Id where builder process is assigned on\n\n\n\n\n\n\ntable_id\n\n\nregclass\n\n\nTable Id where the builder process is scanning on, if \nstate\n is \nloading\n.\n\n\n\n\n\n\nblock_nr\n\n\nint\n\n\nBlock number where the builder process is scanning on, if \nstate\n is \nloading\n.\n\n\n\n\n\n\n\n\nGUC Parameters\n\n\nThis session introduces PG-Strom's configuration parameters.\n\n\nEnables/disables a particular feature\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.enabled\n\n\nbool\n\n\non\n\n\nEnables/disables entire PG-Strom features at once\n\n\n\n\n\n\npg_strom.enable_gpuscan\n\n\nbool\n\n\non\n\n\nEnables/disables GpuScan\n\n\n\n\n\n\npg_strom.enable_gpuhashjoin\n\n\nbool\n\n\non\n\n\nEnables/disables GpuJoin by HashJoin\n\n\n\n\n\n\npg_strom.enable_gpunestloop\n\n\nbool\n\n\non\n\n\nEnables/disables GpuJoin by NestLoop\n\n\n\n\n\n\npg_strom.enable_gpupreagg\n\n\nbool\n\n\non\n\n\nEnables/disables GpuPreAgg\n\n\n\n\n\n\npg_strom.pullup_outer_scan\n\n\nbool\n\n\non\n\n\nEnables/disables to pull up full-table scan if it is just below GpuPreAgg/GpuJoin, to reduce data transfer between CPU/RAM and GPU.\n\n\n\n\n\n\npg_strom.pullup_outer_join\n\n\nbool\n\n\non\n\n\nEnables/disables to pull up tables-join if GpuJoin is just below GpuPreAgg, to reduce data transfer between CPU/RAM and GPU.\n\n\n\n\n\n\npg_strom.enable_numeric_type\n\n\nbool\n\n\non\n\n\nEnables/disables support of \nnumeric\n data type in arithmetic expression on GPU device\n\n\n\n\n\n\npg_strom.cpu_fallback\n\n\nbool\n\n\noff\n\n\nControls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\"\n\n\n\n\n\n\npg_strom.nvme_strom_enabled\n\n\nbool\n\n\non\n\n\nEnables/disables the feature of SSD-to-GPU Direct SQL Execution\n\n\n\n\n\n\npg_strom.nvme_strom_threshold\n\n\nint\n\n\n\u81ea\u52d5\n\n\nControls the table-size threshold to invoke the feature of SSD-to-GPU Direct SQL Execution\n\n\n\n\n\n\n\n\nOptimizer Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.chunk_size\n\n\nint\n\n\n65534kB\n\n\nSize of the data blocks processed by a single GPU kernel invocation. It was configurable, but makes less sense, so fixed to about 64MB in the current version.\n\n\n\n\n\n\npg_strom.gpu_setup_cost\n\n\nreal\n\n\n4000\n\n\nCost value for initialization of GPU device\n\n\n\n\n\n\npg_strom.gpu_dma_cost\n\n\nreal\n\n\n10\n\n\nCost value for DMA transfer over PCIe bus per data-chunk (64MB)\n\n\n\n\n\n\npg_strom.gpu_operator_cost\n\n\nreal\n\n\n0.00015\n\n\nCost value to process an expression formula on GPU. If larger value than \ncpu_operator_cost\n is configured, no chance to choose PG-Strom towards any size of tables\n\n\n\n\n\n\n\n\nExecutor Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.global_max_async_tasks\n\n\nint\n\n\n160\n\n\nNumber of asynchronous taks PG-Strom can throw into GPU's execution queue in the whole system.\n\n\n\n\n\n\npg_strom.local_max_async_tasks\n\n\nint\n\n\n8\n\n\nNumber of asynchronous taks PG-Strom can throw into GPU's execution queue per process. If CPU parallel is used in combination, this limitation shall be applied for each background worker. So, more than \npg_strom.local_max_async_tasks\n asynchronous tasks are executed in parallel on the entire batch job.\n\n\n\n\n\n\npg_strom.max_number_of_gpucontext\n\n\nint\n\n\nauto\n\n\nSpecifies the number of internal data structure \nGpuContext\n to abstract GPU device. Usually, no need to expand the initial value.\n\n\n\n\n\n\n\n\nColumnar Cache Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.ccache_base_dir\n\n\nstring\n\n\n'/dev/shm'\n\n\nSpecifies the directory path to store columnar cache data files. Usually, no need to change from \n/dev/shm\n where \ntmpfs\n is mounted at.\n\n\n\n\n\n\npg_strom.ccache_databases\n\n\nstring\n\n\n''\n\n\nSpecified the target databases for asynchronous columnar cache build, in comma separated list. It does not affect to the manual cache build by \npgstrom_ccache_prewarm()\n.\n\n\n\n\n\n\npg_strom.ccache_num_builders\n\n\nint\n\n\n2\n\n\nSpecified the number of worker processes for asynchronous columnar cache build. It needs to be larger than or equeal to the number of databases in \npg_strom.ccache_databases\n.\n\n\n\n\n\n\npg_strom.ccache_log_output\n\n\nbool\n\n\nfalse\n\n\nControls whether columnar cache builder prints log messages, or not\n\n\n\n\n\n\npg_strom.ccache_total_size\n\n\nint\n\n\nauto\n\n\nUpper limit of the columnar cache in kB. Default is the smaller in 75% of volume size or 66% of system physical memory.\n\n\n\n\n\n\n\n\ngstore_fdw Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.gstore_max_relations\n\n\nint\n\n\n100\n\n\nUpper limit of the number of foreign tables with gstore_fdw. It needs restart to update the parameter.\n\n\n\n\n\n\n\n\nConfiguration of GPU code generation and build\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.program_cache_size\n\n\nint\n\n\n256MB\n\n\nAmount of the shared memory size to cache GPU programs already built. It needs restart to update the parameter.\n\n\n\n\n\n\npg_strom.debug_jit_compile_options\n\n\nbool\n\n\noff\n\n\nControls to include debug option (line-numbers and symbol information) on JIT compile of GPU programs. It is valuable for complicated bug analysis using GPU core dump, however, should not be enabled on daily use because of performance degradation.\n\n\n\n\n\n\npg_strom.debug_kernel_source\n\n\nbool\n\n\noff\n\n\nIf enables, \nEXPLAIN VERBOSE\n command also prints out file paths of GPU programs written out.\n\n\n\n\n\n\n\n\nGPU Device Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.cuda_visible_devices\n\n\nstring\n\n\n''\n\n\nList of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable \nCUDAVISIBLE_DEVICES\n\n\n\n\n\n\npg_strom.gpu_memory_segment_size\n\n\nint\n\n\n512MB\n\n\nSpecifies the amount of device memory to be allocated per CUDA API call. Larger configuration will reduce the overhead of API calls, but not efficient usage of device memory.\n\n\n\n\n\n\npg_strom.max_num_preserved_gpu_memory\n\n\nint\n\n\n2048\n\n\nUpper limit of the number of preserved GPU device memory segment. Usually, don't need to change from the default value.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/references/#supported-data-types", 
            "text": "PG-Strom support the following data types for use on GPU device.  Built-in numeric types     SQL data types  Internal format  Length  Memo      smallint  cl_short  2 bytes     integer  cl_int  4 bytes     bigint  cl_long  8 bytes     real  cl_float  4 bytes     float  cl_double  8 bytes     numeric  cl_ulong  variable length  mapped to 64bit internal format      Note  When GPU processes values in  numeric  data type, it is converted to an internal 64bit format because of implementation reason.\nIt is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert  numaric  datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if  numeric  data with large number of digits are supplied to GPU device.\nTo avoid the problem, turn off the GUC option  pg_strom.enable_numeric_type  not to run operational expression including  numeric  data types on GPU devices.   Built-in date and time types     SQL data types  Internal format  Length  Memo      date  DateADT  4 bytes     time  TimeADT  8 bytes     timetz  TimeTzADT  12 bytes     timestamp  Timestamp  8 bytes     timestamptz  TimestampTz  8 bytes     interval  Interval  16 bytes      Built-in variable length types     SQL data types  Internal format  Length  Memo      bpchar  varlena *  variable length     varchar  varlena *  variable length     bytea  varlena *  variable length     text  varlena *  variable length      Built-in miscellaneous types     SQL data types  Internal format  Length  Memo      boolean  cl_bool  1 byte     money  cl_long  8 bytes     uuid  pg_uuid  16 bytes     macaddr  macaddr  6 bytes     inet  inet_struct  7 bytes or 19 bytes     cidr  inet_struct  7 bytes or 19 bytes      Built-in range data types     SQL data types  Internal format  Length  Memo      int4range  __int4range  14 bytes     int8range  __int8range  22 bytes     tsrange  __tsrange  22 bytes     tstzrange  __tstzrange  22 bytes     daterange  __daterange  14 bytes      Extra Types     SQL data types  Internal format  Length  Memo      float2  half_t  2 bytes  Half precision data type", 
            "title": "Supported Data Types"
        }, 
        {
            "location": "/references/#functions-and-operators", 
            "text": "", 
            "title": "Functions and Operators"
        }, 
        {
            "location": "/references/#device-information-functions", 
            "text": "", 
            "title": "Device information functions"
        }, 
        {
            "location": "/references/#system-view", 
            "text": "PG-Strom provides several system view to export its internal state for users or applications.\nThe future version may add extra fields here. So, it is not recommended to reference these information schemas using  SELECT * FROM ... .", 
            "title": "System View"
        }, 
        {
            "location": "/references/#pgstromdevice_info", 
            "text": "pgstrom.device_into  system view exports device attributes of the GPUs recognized by PG-Strom.\nGPU has different specification for each model, like number of cores, capacity of global memory, maximum number of threads and etc, user's software should be optimized according to the information if you try raw GPU programming with PL/CUDA functions.     Name  Data Type  Description      device_nr  int  GPU device number    aindex  int  Attribute index    attribute  text  Attribute name    value  text  Value of the attribute", 
            "title": "pgstrom.device_info"
        }, 
        {
            "location": "/references/#pgstromdevice_preserved_meminfo", 
            "text": "pgstrom.device_preserved_meminfo  system view exports information of the preserved device memory; which can be shared multiple PostgreSQL backend.\nRight now, only gstore_fdw uses this feature.     Name  Data Type  Description      device_nr  int  GPU device number    handle  bytea  IPC handle of the preserved device memory    owner  regrole  Owner of the preserved device memory    length  bigint  Length of the preserved device memory in bytes    ctime  timestamp with time zone  Timestamp when the preserved device memory is created", 
            "title": "pgstrom.device_preserved_meminfo"
        }, 
        {
            "location": "/references/#pgstromccache_info", 
            "text": "pgstrom.ccache_info  system view exports attribute of the columnar-cache chunks (128MB unit for each).     Name  Data Type  Description      database_id  oid  Database Id    table_id  regclass  Table Id    block_nr  int  Head block-number of the chunk    nitems  bigint  Number of rows in the chunk    length  bigint  Raw size of the cached chunk    ctime  timestamp with time zone  Timestamp of the chunk creation    atime  timestamp with time zone  Timestamp of the least access to the chunk", 
            "title": "pgstrom.ccache_info"
        }, 
        {
            "location": "/references/#pgstromccache_builder_info", 
            "text": "pgstrom.ccache_builder_info  system view exports information of asynchronous builder process of columnar cache.     Name  Data Type  Description      builder_id  int  Asynchronous builder Id of columnar cache    state  text  State of the builder process ( shutdown ,  startup ,  loading  or  sleep )    database_id  oid  Database Id where builder process is assigned on    table_id  regclass  Table Id where the builder process is scanning on, if  state  is  loading .    block_nr  int  Block number where the builder process is scanning on, if  state  is  loading .", 
            "title": "pgstrom.ccache_builder_info"
        }, 
        {
            "location": "/references/#guc-parameters", 
            "text": "This session introduces PG-Strom's configuration parameters.", 
            "title": "GUC Parameters"
        }, 
        {
            "location": "/references/#enablesdisables-a-particular-feature", 
            "text": "Parameter  Type  Default  Description      pg_strom.enabled  bool  on  Enables/disables entire PG-Strom features at once    pg_strom.enable_gpuscan  bool  on  Enables/disables GpuScan    pg_strom.enable_gpuhashjoin  bool  on  Enables/disables GpuJoin by HashJoin    pg_strom.enable_gpunestloop  bool  on  Enables/disables GpuJoin by NestLoop    pg_strom.enable_gpupreagg  bool  on  Enables/disables GpuPreAgg    pg_strom.pullup_outer_scan  bool  on  Enables/disables to pull up full-table scan if it is just below GpuPreAgg/GpuJoin, to reduce data transfer between CPU/RAM and GPU.    pg_strom.pullup_outer_join  bool  on  Enables/disables to pull up tables-join if GpuJoin is just below GpuPreAgg, to reduce data transfer between CPU/RAM and GPU.    pg_strom.enable_numeric_type  bool  on  Enables/disables support of  numeric  data type in arithmetic expression on GPU device    pg_strom.cpu_fallback  bool  off  Controls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\"    pg_strom.nvme_strom_enabled  bool  on  Enables/disables the feature of SSD-to-GPU Direct SQL Execution    pg_strom.nvme_strom_threshold  int  \u81ea\u52d5  Controls the table-size threshold to invoke the feature of SSD-to-GPU Direct SQL Execution", 
            "title": "Enables/disables a particular feature"
        }, 
        {
            "location": "/references/#optimizer-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.chunk_size  int  65534kB  Size of the data blocks processed by a single GPU kernel invocation. It was configurable, but makes less sense, so fixed to about 64MB in the current version.    pg_strom.gpu_setup_cost  real  4000  Cost value for initialization of GPU device    pg_strom.gpu_dma_cost  real  10  Cost value for DMA transfer over PCIe bus per data-chunk (64MB)    pg_strom.gpu_operator_cost  real  0.00015  Cost value to process an expression formula on GPU. If larger value than  cpu_operator_cost  is configured, no chance to choose PG-Strom towards any size of tables", 
            "title": "Optimizer Configuration"
        }, 
        {
            "location": "/references/#executor-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.global_max_async_tasks  int  160  Number of asynchronous taks PG-Strom can throw into GPU's execution queue in the whole system.    pg_strom.local_max_async_tasks  int  8  Number of asynchronous taks PG-Strom can throw into GPU's execution queue per process. If CPU parallel is used in combination, this limitation shall be applied for each background worker. So, more than  pg_strom.local_max_async_tasks  asynchronous tasks are executed in parallel on the entire batch job.    pg_strom.max_number_of_gpucontext  int  auto  Specifies the number of internal data structure  GpuContext  to abstract GPU device. Usually, no need to expand the initial value.", 
            "title": "Executor Configuration"
        }, 
        {
            "location": "/references/#columnar-cache-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.ccache_base_dir  string  '/dev/shm'  Specifies the directory path to store columnar cache data files. Usually, no need to change from  /dev/shm  where  tmpfs  is mounted at.    pg_strom.ccache_databases  string  ''  Specified the target databases for asynchronous columnar cache build, in comma separated list. It does not affect to the manual cache build by  pgstrom_ccache_prewarm() .    pg_strom.ccache_num_builders  int  2  Specified the number of worker processes for asynchronous columnar cache build. It needs to be larger than or equeal to the number of databases in  pg_strom.ccache_databases .    pg_strom.ccache_log_output  bool  false  Controls whether columnar cache builder prints log messages, or not    pg_strom.ccache_total_size  int  auto  Upper limit of the columnar cache in kB. Default is the smaller in 75% of volume size or 66% of system physical memory.", 
            "title": "Columnar Cache Configuration"
        }, 
        {
            "location": "/references/#gstore_fdw-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.gstore_max_relations  int  100  Upper limit of the number of foreign tables with gstore_fdw. It needs restart to update the parameter.", 
            "title": "gstore_fdw Configuration"
        }, 
        {
            "location": "/references/#configuration-of-gpu-code-generation-and-build", 
            "text": "Parameter  Type  Default  Description      pg_strom.program_cache_size  int  256MB  Amount of the shared memory size to cache GPU programs already built. It needs restart to update the parameter.    pg_strom.debug_jit_compile_options  bool  off  Controls to include debug option (line-numbers and symbol information) on JIT compile of GPU programs. It is valuable for complicated bug analysis using GPU core dump, however, should not be enabled on daily use because of performance degradation.    pg_strom.debug_kernel_source  bool  off  If enables,  EXPLAIN VERBOSE  command also prints out file paths of GPU programs written out.", 
            "title": "Configuration of GPU code generation and build"
        }, 
        {
            "location": "/references/#gpu-device-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.cuda_visible_devices  string  ''  List of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable  CUDAVISIBLE_DEVICES    pg_strom.gpu_memory_segment_size  int  512MB  Specifies the amount of device memory to be allocated per CUDA API call. Larger configuration will reduce the overhead of API calls, but not efficient usage of device memory.    pg_strom.max_num_preserved_gpu_memory  int  2048  Upper limit of the number of preserved GPU device memory segment. Usually, don't need to change from the default value.", 
            "title": "GPU Device Configuration"
        }, 
        {
            "location": "/release_note/", 
            "text": "PG-Strom v2.0 Release\n\n\nPG-Strom Development Team (xx-Apr-2018)\n\n\n\n\u672c\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u4f4d\u7f6e\u3065\u3051\n\n\n\u7279\u9577\n\n\n\u65b0\u6a5f\u80fd\n\n\nSSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c\n\n\nGpuPreAgg+GpuJoin+GpuScan \n\n\n\u5217\u6307\u5411\u30ad\u30e3\u30c3\u30b7\u30e5\n\n\ngstore_fdw (GPU\u30c7\u30fc\u30bf\u30b9\u30c8\u30a2)\n\n\n\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30c7\u30fc\u30bf\u578b\u3078\u306e\u5bfe\u5fdc\n    inet\u578b/macaddr\u578b\u306b\u5bfe\u5fdc\u3057\u307e\u3057\u305f\u3002\u30ed\u30b0\u30c7\u30fc\u30bf\u306e\u89e3\u6790\u306b\u4fbf\u5229\u3067\u3059\u3002\n\n\n\u7bc4\u56f2\u578b\u30c7\u30fc\u30bf\u3078\u306e\u5bfe\u5fdc\n\n\n\u305d\u306e\u4ed6\u306e\u4fee\u6b63\n\n\n\u5ec3\u6b62\u3055\u308c\u305f\u6a5f\u80fd\n\n\n\n\nPostgreSQL v9.5\u306e\u30b5\u30dd\u30fc\u30c8\u306f\u5ec3\u6b62\u3055\u308c\u307e\u3057\u305f\u3002\n\n\n\u6027\u80fd\u4e0a\u306e\u30e1\u30ea\u30c3\u30c8\u304c\u5f97\u3089\u308c\u306a\u3044\u305f\u3081\u3001GpuSort\u6a5f\u80fd\u306f\u5ec3\u6b62\u3055\u308c\u307e\u3057\u305f\u3002\n\n\n\n\nPG-Strom v1.0 Release\n\n\nPG-Strom Development Team (26-Oct-2016)\n\n\n\nRelease High Light\n\n\n\u7279\u9577\n\n\n\u534a\u5c0e\u4f53\u6280\u8853\u306e\u9032\u5316\u306b\u4f34\u3044\u3001GPU\u306f\u30ef\u30f3\u30c1\u30c3\u30d7\u306b\u6570\u767e\uff5e\u6570\u5343\u4e26\u5217\u30b3\u30a2\u3068\u6570\u767eGB/s\u306e\u5e83\u5e2f\u57dfRAM\u306e\u642d\u8f09\u304c\u4e00\u822c\u7684\u306b\u306a\u308a\u3001\u304b\u3064\u3066\u306f\u30b9\u30fc\u30d1\u30fc\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u6c34\u6e96\u306e\u6027\u80fd\u3068\u3055\u308c\u3066\u3044\u305f\u51e6\u7406\u80fd\u529b\u304c\u3001\u500b\u4eba\u306e\u624b\u306b\u3082\u5c4a\u304f\u6c34\u6e96\u3068\u306a\u3063\u3066\u304d\u307e\u3057\u305f\u3002 PG-Strom\u306f\u3053\u306e\u3088\u3046\u306aGPU\u306e\u8a08\u7b97\u80fd\u529b\u3092\u30af\u30a8\u30ea\u51e6\u7406\u306e\u5206\u91ce\u306b\u9069\u7528\u3059\u308b\u4e8b\u3067\u3001\u65e2\u5b58\u306ePostgreSQL\u904b\u7528\u74b0\u5883\u306b\u624b\u3092\u52a0\u3048\u308b\u3053\u3068\u306a\u304f\uff08\u3064\u307e\u308a\u3001\u65e2\u5b58\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3084\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u4fee\u6b63\u306a\u304f\u4f7f\u3044\u7d9a\u3051\u308b\u3068\u3044\u3046\u6761\u4ef6\u4e0b\u3067\uff09\u3001\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u30ec\u30b9\u304b\u3064\u5b89\u4fa1\u306b\u6027\u80fd\u4e0a\u306e\u9650\u754c\u3092\u7a81\u7834\u3059\u308b\u4e8b\u3092\u610f\u56f3\u3057\u3066\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002\nPG-Strom\u306e\u4e2d\u6838\u3068\u306a\u308b\u6280\u8853\u306f\u3001\u2460\u5165\u529b\u3055\u308c\u305fSQL\u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u3092\u51e6\u7406\u3059\u308bGPU\u7528\u30d0\u30a4\u30ca\u30ea\u3092\u751f\u6210\u3059\u308b\u30b3\u30fc\u30c9\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3001\u2461\u6570\u4e07\uff5e\u6570\u5341\u4e07\u884c\u7a0b\u5ea6\u306e\u30c7\u30fc\u30bf\u30c1\u30e3\u30f3\u30af\u5358\u4f4d\u3067\u975e\u540c\u671f\u51e6\u7406\u3092\u884c\u3046\u5b9f\u884c\u30a8\u30f3\u30b8\u30f3\u306e\uff12\u3064\u3067\u3059\u3002 PostgreSQL v9.5\u3067\u65b0\u305f\u306b\u5bfe\u5fdc\u3057\u305f CustomScan Interface \u306b\u5bfe\u5fdc\u3059\u308b\u4e8b\u3067\u3001PostgreSQL\u5074\u306b\u306f\u30d1\u30c3\u30c1\u7b49\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u306a\u304f\u3001\u7d14\u7c8b\u306a\u62e1\u5f35\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u3053\u308c\u3089\u306e\u6a5f\u80fd\u3092\u5b9f\u88c5\u3059\u308b\u4e8b\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\n\n\u60f3\u5b9a\u7528\u9014\n\n\nPG-Strom v1.0\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7528\u9014\u3067\u306e\u5229\u7528\u3092\u60f3\u5b9a\u3057\u3066\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n- BI\u306a\u3069OLAP\u7cfb\u51e6\u7406\u3092\u884c\u3046\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\n- ETL\u3084\u30ec\u30dd\u30fc\u30c6\u30a3\u30f3\u30b0\u306a\u3069\u30d0\u30c3\u30c1\u51e6\u7406\u7cfb\n- \u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u7cfbDB\u3068\u60c5\u5831\u7cfbDB\u306e\u7d71\u5408\n- \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u4e0a\u3067\u306e\u7d71\u8a08\u89e3\u6790\u51e6\u7406\n\n\n\u65b0\u6a5f\u80fd\n\n\n\n\nGpuScan\n\n\n\u6761\u4ef6\u53e5\u3092\u4f34\u3046\u30c6\u30fc\u30d6\u30eb\u306e\u5168\u4ef6\u30b9\u30ad\u30e3\u30f3\u3092GPU\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\u30c6\u30fc\u30d6\u30eb\u306e\u5927\u90e8\u5206\u304c\u30aa\u30f3\u30e1\u30e2\u30ea\u306e\u72b6\u6cc1\u3067\u306f\u6761\u4ef6\u53e5\u306e\u8a55\u4fa1\u304c\u8ca0\u8377\u306e\u5927\u90e8\u5206\u3092\u5360\u3081\u308b\u305f\u3081\u3001\u4e26\u5217\u51e6\u7406\u306f\u6709\u52b9\u306a\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u7b56\u3067\u3059\u3002\n\n\nGpuJoin\n\n\n\u30c6\u30fc\u30d6\u30ebJOIN\u3092GPU\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\u4e26\u5217\u7248HashJoin\u304a\u3088\u3073NestLoop\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u5b9f\u88c5\u3055\u308c\u3066\u304a\u308a\u3001\u307e\u305f\u30011\u56de\u306eGPU\u547c\u3073\u51fa\u3057\u30673\u500b\u4ee5\u4e0a\u306e\u30c6\u30fc\u30d6\u30eb\u3092\u7d50\u5408\u3059\u308b\u4e8b\u3082\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u901a\u5e38\u3001RDBMS\u306e\u51e6\u7406\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u3068\u306a\u308a\u304c\u3061\u306a\u30c6\u30fc\u30d6\u30eb\u540c\u58eb\u306e\u7d50\u5408\u3092\u52b9\u7387\u7684\u306b\u5b9f\u884c\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u3059\u3002\n\n\nGpuPreAgg\n\n\n\u96c6\u7d04\u6f14\u7b97\u3092GPU\u3067\u5b9f\u884c\u3057CPU\u306e\u8ca0\u8377\u3092\u8efd\u6e1b\u3057\u307e\u3059\u3002\u975e\u5e38\u306b\u591a\u304f\u306e\u6f14\u7b97\u30b3\u30a2\u304c\u5354\u8abf\u3057\u3066\u5927\u91cf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u3054\u304f\u5c11\u6570\u306e\u30b5\u30de\u30ea\u3078\u3068\u7e2e\u7d04\u3059\u308b\u6f14\u7b97\u306fGPU\u304c\u6700\u3082\u5f97\u610f\u3068\u3059\u308b\u3082\u306e\u3067\u3001\u7279\u306b\u3001GROUP BY\u306e\u7d50\u679c\u96c6\u7d04\u7387\u304c\u975e\u5e38\u306b\u9ad8\u304f\u306a\u308b\u30b1\u30fc\u30b9\u3067\u52b9\u679c\u3092\u767a\u63ee\u3057\u307e\u3059\u3002\n\n\nGpuSort\n\n\nGPU\u3067\u306eBitonic Sorting\u3068\u3001CPU\u5074\u3067\u306eMerge Sorting\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u4e8b\u3067\u3001\u672c\u4f53\u306e\u30bd\u30fc\u30c8\u51e6\u7406\u3092\u4ee3\u66ff\u3057\u307e\u3059\u3002\u3067\u304d\u308b\u3060\u3051\u591a\u304f\u306e\u30c7\u30fc\u30bf\u3092\u4e00\u5ea6\u306bGPU\u3067\u51e6\u7406\u3057\u305f\u65b9\u304c\u52b9\u7387\u7684\u3067\u3042\u308b\u4e00\u65b9\u3001\u30c7\u30fc\u30bf\u30c1\u30e3\u30f3\u30af\u306e\u62e1\u5927\u306b\u4f34\u3063\u3066\u975e\u540c\u671f\u51e6\u7406\u306e\u52b9\u7387\u306f\u4f4e\u4e0b\u3057\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u73fe\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f\u5341\u5206\u306a\u6027\u80fd\u512a\u4f4d\u6027\u304c\u5f97\u3089\u308c\u3066\u304a\u3089\u305a\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u7121\u52b9\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\nGpuProjection\n\n\n\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u53d6\u308a\u51fa\u3057\u305f\u30c7\u30fc\u30bf\u3092\u8907\u96d1\u306a\u8a08\u7b97\u5f0f\u3067\u51e6\u7406\u3057\u3001\u305d\u306e\u7d50\u679c\u3092\u5229\u7528\u8005\u3078\u3068\u8fd4\u5374\u3059\u308b\u4e8b\u304c\u3042\u308a\u307e\u3059\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u3068\u547c\u3070\u308c\u308b\u3053\u306e\u51e6\u7406\u306f\u3001\u7d50\u679c\u306e\u4ef6\u6570\u3084\u8a08\u7b97\u5f0f\u306e\u8907\u96d1\u3055\u6b21\u7b2c\u3067\u306f\u3001\u30b9\u30ad\u30e3\u30f3\u3084\u30b8\u30e7\u30a4\u30f3\u3068\u3044\u3063\u305f\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u4e2d\u6838\u7684\u306a\u51e6\u7406\u3088\u308a\u3082\u51e6\u7406\u6642\u9593\u3092\u8981\u3057\u307e\u3059\u3002GpuProjection\u6a5f\u80fd\u306f\u3001\u3053\u308c\u3089\u306e\u8907\u96d1\u306a\u8a08\u7b97\u3092GPU\u5074\u3067\u4e88\u3081\u884c\u3063\u3066\u304a\u304d\u3001CPU\u5074\u3067\u306f\u300eGPU\u3067\u306e\u8a08\u7b97\u7d50\u679c\u3092\u53c2\u7167\u3059\u308b\u300f\u3088\u3046\u30af\u30a8\u30ea\u5b9f\u884c\u8a08\u753b\u3092\u66f8\u304d\u63db\u3048\u308b\u4e8b\u3067\u30af\u30a8\u30ea\u51e6\u7406\u3092\u9ad8\u901f\u5316\u3057\u307e\u3059\u3002\n\n\nPL/CUDA\n\n\n\u4efb\u610f\u306eCUDA\u30d7\u30ed\u30b0\u30e9\u30e0\u3092SQL\u95a2\u6570\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u6a5f\u80fd\u3067\u3059\u3002PostgreSQL\u306eProcedural Language\u3068\u3057\u3066\u5b9f\u88c5\u3055\u308c\u3066\u304a\u308a\u3001SQL\u95a2\u6570\u306e\u5f15\u6570\u3084\u5b9f\u884c\u7d50\u679c\u306f\u81ea\u52d5\u7684\u306bCPU\uff5eGPU\u9593\u3067\u53d7\u3051\u6e21\u3055\u308c\u308b\u305f\u3081\u3001\u30e6\u30fc\u30b6\u306f\u9ad8\u5ea6\u306a\u30c7\u30fc\u30bf\u89e3\u6790\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5b9f\u88c5\u306b\u96c6\u4e2d\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u3059\u3002PL/CUDA\u95a2\u6570\u306fGPU\u306e\u672c\u6765\u6301\u3063\u3066\u3044\u308b\u901f\u5ea6\u3067\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u52d5\u4f5c\u3055\u305b\u308b\u4e8b\u304c\u3067\u304d\u308b\u305f\u3081\u3001\u52b9\u7387\u7684\u306ain-database\u30c7\u30fc\u30bf\u89e3\u6790\u3092\u5b9f\u73fe\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u3059\u3002\n\n\nLIKE\u53e5\n\n\nLIKE\u53e5\u306b\u3088\u308b\u30d1\u30bf\u30fc\u30f3\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002\n\n\n\n\n\u5236\u9650\u4e8b\u9805\n\n\n\n\n\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316\u306b\u8981\u3059\u308b\u6642\u9593\n\n\nGPU\u30c7\u30d0\u30a4\u30b9\u306e\u521d\u671f\u5316\u304a\u3088\u3073CUDA\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u4f5c\u6210\u306b\u306f\u3001\u6982\u306d0.2\uff5e0.3\u79d2\u7a0b\u5ea6\u306e\u6642\u9593\u3092\u8981\u3057\u307e\u3059\u3002\u77ed\u6642\u9593\u3067\u5b8c\u4e86\u3059\u308b\u30af\u30a8\u30ea\u306b\u3068\u3063\u3066\u306f\u3053\u306e\u8ffd\u52a0\u30b3\u30b9\u30c8\u306f\u7121\u8996\u3067\u304d\u306a\u3044\u3082\u306e\u306b\u306a\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3044\u3063\u305f\u3093\u4f5c\u6210\u3057\u305fCUDA\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306f\u518d\u5229\u7528\u3055\u308c\u308b\u305f\u3081\u3001\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u30d7\u30fc\u30ea\u30f3\u30b0\u306e\u5229\u7528\u306b\u3088\u308a\u3053\u306e\u5236\u9650\u306f\u3042\u308b\u7a0b\u5ea6\u7de9\u548c\u3055\u308c\u307e\u3059\u304c\u3001\u672c\u8cea\u7684\u306b\u306fv2.0\u3067\u306e\u6539\u5584\u3092\u5f85\u3064\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n\n\n\n\n\n\u540c\u6642\u4e26\u884c\u30bb\u30c3\u30b7\u30e7\u30f3\u6570\n\n\nCUDA\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u518d\u5229\u7528\u306b\u4f34\u3044\u3001\u65e2\u306bGPU\u3092\u4f7f\u3044\u7d42\u3048\u305f\u30bb\u30c3\u30b7\u30e7\u30f3\u304c\u6b21\u56de\u306e\u5229\u7528\u306b\u5099\u3048\u3066GPU\u306eRAM\u3092\u30ad\u30e3\u30c3\u30b7\u30e5\u3059\u308b\u4e8b\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u6b21\u56de\u306e\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316\u6642\u9593\u3092\u77ed\u7e2e\u3059\u308b\u305f\u3081\u3067\u3059\u304c\u3001\u7121\u95a2\u4fc2\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u304cGPU\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\u6642\u306e\u969c\u5bb3\u3068\u3082\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3092\u907f\u3051\u308b\u306b\u306f\u3001\u540c\u6642\u306bPG-Strom\u3092\u4f7f\u7528\u3059\u308b\u30bb\u30c3\u30b7\u30e7\u30f3\u6570\u306f\u6700\u59273\uff5e5\u7a0b\u5ea6\u306b\u6291\u3048\u3066\u304f\u3060\u3055\u3044\u3002v2.0\u3067\u306e\u6539\u5584\u304c\u4e88\u5b9a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\n\n\n\n\n\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u30b5\u30a4\u30ba\n\n\n\u4e00\u822c\u306b\u3001\u30b9\u30c8\u30ec\u30fc\u30b8\u3078\u306eI/O\u3092\u4f34\u3046\u30c7\u30fc\u30bf\u30a2\u30af\u30bb\u30b9\u306fRAM\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306b\u6bd4\u3079\u3066\u975e\u5e38\u306b\u4f4e\u901f\u3067\u3059\u3002PG-Strom\u306fSQL\u51e6\u7406\u306b\u4f34\u3046CPU\u8ca0\u8377\u3092\u30aa\u30d5\u30ed\u30fc\u30c9\u3057\u307e\u3059\u304c\u3001I/O\u4e3b\u4f53\u306e\u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u306b\u5bfe\u3057\u3066\u306f\u52b9\u679c\u3092\u767a\u63ee\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u305b\u3093\u3002\u51e6\u7406\u5bfe\u8c61\u306e\u30c7\u30fc\u30bf\u304cPostgreSQL\u306e\u5171\u6709\u30d0\u30c3\u30d5\u30a1\u304b\u3001\u5c11\u306a\u304f\u3068\u3082OS\u306e\u30c7\u30a3\u30b9\u30af\u30ad\u30e3\u30c3\u30b7\u30e5\u306b\u8f09\u308a\u5207\u308b\u7a0b\u5ea6\u306e\u5927\u304d\u3055\u306e\u3082\u306e\u306b\u5bfe\u3057\u3066\u9069\u7528\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002 \u3053\u306e\u5236\u9650\u306f\u3001v2.0\u3067\u5c0e\u5165\u3055\u308c\u308b\u4e88\u5b9a\u306eSSD\u9023\u643a\u6a5f\u80fd\u306b\u3088\u308a\u3042\u308b\u7a0b\u5ea6\u7de9\u548c\u3055\u308c\u308b\u898b\u901a\u3057\u3067\u3059\u3002", 
            "title": "Release Note"
        }, 
        {
            "location": "/release_note/#pg-strom-v20-release", 
            "text": "PG-Strom Development Team (xx-Apr-2018)", 
            "title": "PG-Strom v2.0 Release"
        }, 
        {
            "location": "/release_note/#_1", 
            "text": "", 
            "title": "\u672c\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u4f4d\u7f6e\u3065\u3051"
        }, 
        {
            "location": "/release_note/#_2", 
            "text": "", 
            "title": "\u7279\u9577"
        }, 
        {
            "location": "/release_note/#_3", 
            "text": "SSD-to-GPU\u30c0\u30a4\u30ec\u30af\u30c8SQL\u5b9f\u884c  GpuPreAgg+GpuJoin+GpuScan   \u5217\u6307\u5411\u30ad\u30e3\u30c3\u30b7\u30e5  gstore_fdw (GPU\u30c7\u30fc\u30bf\u30b9\u30c8\u30a2)  \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30c7\u30fc\u30bf\u578b\u3078\u306e\u5bfe\u5fdc\n    inet\u578b/macaddr\u578b\u306b\u5bfe\u5fdc\u3057\u307e\u3057\u305f\u3002\u30ed\u30b0\u30c7\u30fc\u30bf\u306e\u89e3\u6790\u306b\u4fbf\u5229\u3067\u3059\u3002  \u7bc4\u56f2\u578b\u30c7\u30fc\u30bf\u3078\u306e\u5bfe\u5fdc", 
            "title": "\u65b0\u6a5f\u80fd"
        }, 
        {
            "location": "/release_note/#_4", 
            "text": "", 
            "title": "\u305d\u306e\u4ed6\u306e\u4fee\u6b63"
        }, 
        {
            "location": "/release_note/#_5", 
            "text": "PostgreSQL v9.5\u306e\u30b5\u30dd\u30fc\u30c8\u306f\u5ec3\u6b62\u3055\u308c\u307e\u3057\u305f\u3002  \u6027\u80fd\u4e0a\u306e\u30e1\u30ea\u30c3\u30c8\u304c\u5f97\u3089\u308c\u306a\u3044\u305f\u3081\u3001GpuSort\u6a5f\u80fd\u306f\u5ec3\u6b62\u3055\u308c\u307e\u3057\u305f\u3002", 
            "title": "\u5ec3\u6b62\u3055\u308c\u305f\u6a5f\u80fd"
        }, 
        {
            "location": "/release_note/#pg-strom-v10-release", 
            "text": "PG-Strom Development Team (26-Oct-2016)", 
            "title": "PG-Strom v1.0 Release"
        }, 
        {
            "location": "/release_note/#release-high-light", 
            "text": "", 
            "title": "Release High Light"
        }, 
        {
            "location": "/release_note/#_6", 
            "text": "\u534a\u5c0e\u4f53\u6280\u8853\u306e\u9032\u5316\u306b\u4f34\u3044\u3001GPU\u306f\u30ef\u30f3\u30c1\u30c3\u30d7\u306b\u6570\u767e\uff5e\u6570\u5343\u4e26\u5217\u30b3\u30a2\u3068\u6570\u767eGB/s\u306e\u5e83\u5e2f\u57dfRAM\u306e\u642d\u8f09\u304c\u4e00\u822c\u7684\u306b\u306a\u308a\u3001\u304b\u3064\u3066\u306f\u30b9\u30fc\u30d1\u30fc\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u6c34\u6e96\u306e\u6027\u80fd\u3068\u3055\u308c\u3066\u3044\u305f\u51e6\u7406\u80fd\u529b\u304c\u3001\u500b\u4eba\u306e\u624b\u306b\u3082\u5c4a\u304f\u6c34\u6e96\u3068\u306a\u3063\u3066\u304d\u307e\u3057\u305f\u3002 PG-Strom\u306f\u3053\u306e\u3088\u3046\u306aGPU\u306e\u8a08\u7b97\u80fd\u529b\u3092\u30af\u30a8\u30ea\u51e6\u7406\u306e\u5206\u91ce\u306b\u9069\u7528\u3059\u308b\u4e8b\u3067\u3001\u65e2\u5b58\u306ePostgreSQL\u904b\u7528\u74b0\u5883\u306b\u624b\u3092\u52a0\u3048\u308b\u3053\u3068\u306a\u304f\uff08\u3064\u307e\u308a\u3001\u65e2\u5b58\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3084\u30c7\u30fc\u30bf\u69cb\u9020\u3092\u4fee\u6b63\u306a\u304f\u4f7f\u3044\u7d9a\u3051\u308b\u3068\u3044\u3046\u6761\u4ef6\u4e0b\u3067\uff09\u3001\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u30ec\u30b9\u304b\u3064\u5b89\u4fa1\u306b\u6027\u80fd\u4e0a\u306e\u9650\u754c\u3092\u7a81\u7834\u3059\u308b\u4e8b\u3092\u610f\u56f3\u3057\u3066\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002\nPG-Strom\u306e\u4e2d\u6838\u3068\u306a\u308b\u6280\u8853\u306f\u3001\u2460\u5165\u529b\u3055\u308c\u305fSQL\u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u3092\u51e6\u7406\u3059\u308bGPU\u7528\u30d0\u30a4\u30ca\u30ea\u3092\u751f\u6210\u3059\u308b\u30b3\u30fc\u30c9\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3001\u2461\u6570\u4e07\uff5e\u6570\u5341\u4e07\u884c\u7a0b\u5ea6\u306e\u30c7\u30fc\u30bf\u30c1\u30e3\u30f3\u30af\u5358\u4f4d\u3067\u975e\u540c\u671f\u51e6\u7406\u3092\u884c\u3046\u5b9f\u884c\u30a8\u30f3\u30b8\u30f3\u306e\uff12\u3064\u3067\u3059\u3002 PostgreSQL v9.5\u3067\u65b0\u305f\u306b\u5bfe\u5fdc\u3057\u305f CustomScan Interface \u306b\u5bfe\u5fdc\u3059\u308b\u4e8b\u3067\u3001PostgreSQL\u5074\u306b\u306f\u30d1\u30c3\u30c1\u7b49\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u306a\u304f\u3001\u7d14\u7c8b\u306a\u62e1\u5f35\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u3053\u308c\u3089\u306e\u6a5f\u80fd\u3092\u5b9f\u88c5\u3059\u308b\u4e8b\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3057\u305f\u3002", 
            "title": "\u7279\u9577"
        }, 
        {
            "location": "/release_note/#_7", 
            "text": "PG-Strom v1.0\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u7528\u9014\u3067\u306e\u5229\u7528\u3092\u60f3\u5b9a\u3057\u3066\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n- BI\u306a\u3069OLAP\u7cfb\u51e6\u7406\u3092\u884c\u3046\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306e\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\n- ETL\u3084\u30ec\u30dd\u30fc\u30c6\u30a3\u30f3\u30b0\u306a\u3069\u30d0\u30c3\u30c1\u51e6\u7406\u7cfb\n- \u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u7cfbDB\u3068\u60c5\u5831\u7cfbDB\u306e\u7d71\u5408\n- \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u4e0a\u3067\u306e\u7d71\u8a08\u89e3\u6790\u51e6\u7406", 
            "title": "\u60f3\u5b9a\u7528\u9014"
        }, 
        {
            "location": "/release_note/#_8", 
            "text": "GpuScan  \u6761\u4ef6\u53e5\u3092\u4f34\u3046\u30c6\u30fc\u30d6\u30eb\u306e\u5168\u4ef6\u30b9\u30ad\u30e3\u30f3\u3092GPU\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\u30c6\u30fc\u30d6\u30eb\u306e\u5927\u90e8\u5206\u304c\u30aa\u30f3\u30e1\u30e2\u30ea\u306e\u72b6\u6cc1\u3067\u306f\u6761\u4ef6\u53e5\u306e\u8a55\u4fa1\u304c\u8ca0\u8377\u306e\u5927\u90e8\u5206\u3092\u5360\u3081\u308b\u305f\u3081\u3001\u4e26\u5217\u51e6\u7406\u306f\u6709\u52b9\u306a\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u7b56\u3067\u3059\u3002  GpuJoin  \u30c6\u30fc\u30d6\u30ebJOIN\u3092GPU\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002\u4e26\u5217\u7248HashJoin\u304a\u3088\u3073NestLoop\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u5b9f\u88c5\u3055\u308c\u3066\u304a\u308a\u3001\u307e\u305f\u30011\u56de\u306eGPU\u547c\u3073\u51fa\u3057\u30673\u500b\u4ee5\u4e0a\u306e\u30c6\u30fc\u30d6\u30eb\u3092\u7d50\u5408\u3059\u308b\u4e8b\u3082\u3042\u308a\u307e\u3059\u306e\u3067\u3001\u901a\u5e38\u3001RDBMS\u306e\u51e6\u7406\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u3068\u306a\u308a\u304c\u3061\u306a\u30c6\u30fc\u30d6\u30eb\u540c\u58eb\u306e\u7d50\u5408\u3092\u52b9\u7387\u7684\u306b\u5b9f\u884c\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u3059\u3002  GpuPreAgg  \u96c6\u7d04\u6f14\u7b97\u3092GPU\u3067\u5b9f\u884c\u3057CPU\u306e\u8ca0\u8377\u3092\u8efd\u6e1b\u3057\u307e\u3059\u3002\u975e\u5e38\u306b\u591a\u304f\u306e\u6f14\u7b97\u30b3\u30a2\u304c\u5354\u8abf\u3057\u3066\u5927\u91cf\u306e\u5165\u529b\u30c7\u30fc\u30bf\u3092\u3054\u304f\u5c11\u6570\u306e\u30b5\u30de\u30ea\u3078\u3068\u7e2e\u7d04\u3059\u308b\u6f14\u7b97\u306fGPU\u304c\u6700\u3082\u5f97\u610f\u3068\u3059\u308b\u3082\u306e\u3067\u3001\u7279\u306b\u3001GROUP BY\u306e\u7d50\u679c\u96c6\u7d04\u7387\u304c\u975e\u5e38\u306b\u9ad8\u304f\u306a\u308b\u30b1\u30fc\u30b9\u3067\u52b9\u679c\u3092\u767a\u63ee\u3057\u307e\u3059\u3002  GpuSort  GPU\u3067\u306eBitonic Sorting\u3068\u3001CPU\u5074\u3067\u306eMerge Sorting\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u4e8b\u3067\u3001\u672c\u4f53\u306e\u30bd\u30fc\u30c8\u51e6\u7406\u3092\u4ee3\u66ff\u3057\u307e\u3059\u3002\u3067\u304d\u308b\u3060\u3051\u591a\u304f\u306e\u30c7\u30fc\u30bf\u3092\u4e00\u5ea6\u306bGPU\u3067\u51e6\u7406\u3057\u305f\u65b9\u304c\u52b9\u7387\u7684\u3067\u3042\u308b\u4e00\u65b9\u3001\u30c7\u30fc\u30bf\u30c1\u30e3\u30f3\u30af\u306e\u62e1\u5927\u306b\u4f34\u3063\u3066\u975e\u540c\u671f\u51e6\u7406\u306e\u52b9\u7387\u306f\u4f4e\u4e0b\u3057\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u73fe\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f\u5341\u5206\u306a\u6027\u80fd\u512a\u4f4d\u6027\u304c\u5f97\u3089\u308c\u3066\u304a\u3089\u305a\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u7121\u52b9\u5316\u3055\u308c\u3066\u3044\u307e\u3059\u3002  GpuProjection  \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u53d6\u308a\u51fa\u3057\u305f\u30c7\u30fc\u30bf\u3092\u8907\u96d1\u306a\u8a08\u7b97\u5f0f\u3067\u51e6\u7406\u3057\u3001\u305d\u306e\u7d50\u679c\u3092\u5229\u7528\u8005\u3078\u3068\u8fd4\u5374\u3059\u308b\u4e8b\u304c\u3042\u308a\u307e\u3059\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u3068\u547c\u3070\u308c\u308b\u3053\u306e\u51e6\u7406\u306f\u3001\u7d50\u679c\u306e\u4ef6\u6570\u3084\u8a08\u7b97\u5f0f\u306e\u8907\u96d1\u3055\u6b21\u7b2c\u3067\u306f\u3001\u30b9\u30ad\u30e3\u30f3\u3084\u30b8\u30e7\u30a4\u30f3\u3068\u3044\u3063\u305f\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u4e2d\u6838\u7684\u306a\u51e6\u7406\u3088\u308a\u3082\u51e6\u7406\u6642\u9593\u3092\u8981\u3057\u307e\u3059\u3002GpuProjection\u6a5f\u80fd\u306f\u3001\u3053\u308c\u3089\u306e\u8907\u96d1\u306a\u8a08\u7b97\u3092GPU\u5074\u3067\u4e88\u3081\u884c\u3063\u3066\u304a\u304d\u3001CPU\u5074\u3067\u306f\u300eGPU\u3067\u306e\u8a08\u7b97\u7d50\u679c\u3092\u53c2\u7167\u3059\u308b\u300f\u3088\u3046\u30af\u30a8\u30ea\u5b9f\u884c\u8a08\u753b\u3092\u66f8\u304d\u63db\u3048\u308b\u4e8b\u3067\u30af\u30a8\u30ea\u51e6\u7406\u3092\u9ad8\u901f\u5316\u3057\u307e\u3059\u3002  PL/CUDA  \u4efb\u610f\u306eCUDA\u30d7\u30ed\u30b0\u30e9\u30e0\u3092SQL\u95a2\u6570\u3068\u3057\u3066\u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u6a5f\u80fd\u3067\u3059\u3002PostgreSQL\u306eProcedural Language\u3068\u3057\u3066\u5b9f\u88c5\u3055\u308c\u3066\u304a\u308a\u3001SQL\u95a2\u6570\u306e\u5f15\u6570\u3084\u5b9f\u884c\u7d50\u679c\u306f\u81ea\u52d5\u7684\u306bCPU\uff5eGPU\u9593\u3067\u53d7\u3051\u6e21\u3055\u308c\u308b\u305f\u3081\u3001\u30e6\u30fc\u30b6\u306f\u9ad8\u5ea6\u306a\u30c7\u30fc\u30bf\u89e3\u6790\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5b9f\u88c5\u306b\u96c6\u4e2d\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u3059\u3002PL/CUDA\u95a2\u6570\u306fGPU\u306e\u672c\u6765\u6301\u3063\u3066\u3044\u308b\u901f\u5ea6\u3067\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u52d5\u4f5c\u3055\u305b\u308b\u4e8b\u304c\u3067\u304d\u308b\u305f\u3081\u3001\u52b9\u7387\u7684\u306ain-database\u30c7\u30fc\u30bf\u89e3\u6790\u3092\u5b9f\u73fe\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u3059\u3002  LIKE\u53e5  LIKE\u53e5\u306b\u3088\u308b\u30d1\u30bf\u30fc\u30f3\u30de\u30c3\u30c1\u30f3\u30b0\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002", 
            "title": "\u65b0\u6a5f\u80fd"
        }, 
        {
            "location": "/release_note/#_9", 
            "text": "\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316\u306b\u8981\u3059\u308b\u6642\u9593  GPU\u30c7\u30d0\u30a4\u30b9\u306e\u521d\u671f\u5316\u304a\u3088\u3073CUDA\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u4f5c\u6210\u306b\u306f\u3001\u6982\u306d0.2\uff5e0.3\u79d2\u7a0b\u5ea6\u306e\u6642\u9593\u3092\u8981\u3057\u307e\u3059\u3002\u77ed\u6642\u9593\u3067\u5b8c\u4e86\u3059\u308b\u30af\u30a8\u30ea\u306b\u3068\u3063\u3066\u306f\u3053\u306e\u8ffd\u52a0\u30b3\u30b9\u30c8\u306f\u7121\u8996\u3067\u304d\u306a\u3044\u3082\u306e\u306b\u306a\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3044\u3063\u305f\u3093\u4f5c\u6210\u3057\u305fCUDA\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306f\u518d\u5229\u7528\u3055\u308c\u308b\u305f\u3081\u3001\u30b3\u30cd\u30af\u30b7\u30e7\u30f3\u30d7\u30fc\u30ea\u30f3\u30b0\u306e\u5229\u7528\u306b\u3088\u308a\u3053\u306e\u5236\u9650\u306f\u3042\u308b\u7a0b\u5ea6\u7de9\u548c\u3055\u308c\u307e\u3059\u304c\u3001\u672c\u8cea\u7684\u306b\u306fv2.0\u3067\u306e\u6539\u5584\u3092\u5f85\u3064\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002    \u540c\u6642\u4e26\u884c\u30bb\u30c3\u30b7\u30e7\u30f3\u6570  CUDA\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u518d\u5229\u7528\u306b\u4f34\u3044\u3001\u65e2\u306bGPU\u3092\u4f7f\u3044\u7d42\u3048\u305f\u30bb\u30c3\u30b7\u30e7\u30f3\u304c\u6b21\u56de\u306e\u5229\u7528\u306b\u5099\u3048\u3066GPU\u306eRAM\u3092\u30ad\u30e3\u30c3\u30b7\u30e5\u3059\u308b\u4e8b\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u6b21\u56de\u306e\u30c7\u30d0\u30a4\u30b9\u521d\u671f\u5316\u6642\u9593\u3092\u77ed\u7e2e\u3059\u308b\u305f\u3081\u3067\u3059\u304c\u3001\u7121\u95a2\u4fc2\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u304cGPU\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b\u6642\u306e\u969c\u5bb3\u3068\u3082\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3092\u907f\u3051\u308b\u306b\u306f\u3001\u540c\u6642\u306bPG-Strom\u3092\u4f7f\u7528\u3059\u308b\u30bb\u30c3\u30b7\u30e7\u30f3\u6570\u306f\u6700\u59273\uff5e5\u7a0b\u5ea6\u306b\u6291\u3048\u3066\u304f\u3060\u3055\u3044\u3002v2.0\u3067\u306e\u6539\u5584\u304c\u4e88\u5b9a\u3055\u308c\u3066\u3044\u307e\u3059\u3002    \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u30b5\u30a4\u30ba  \u4e00\u822c\u306b\u3001\u30b9\u30c8\u30ec\u30fc\u30b8\u3078\u306eI/O\u3092\u4f34\u3046\u30c7\u30fc\u30bf\u30a2\u30af\u30bb\u30b9\u306fRAM\u3078\u306e\u30a2\u30af\u30bb\u30b9\u306b\u6bd4\u3079\u3066\u975e\u5e38\u306b\u4f4e\u901f\u3067\u3059\u3002PG-Strom\u306fSQL\u51e6\u7406\u306b\u4f34\u3046CPU\u8ca0\u8377\u3092\u30aa\u30d5\u30ed\u30fc\u30c9\u3057\u307e\u3059\u304c\u3001I/O\u4e3b\u4f53\u306e\u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u306b\u5bfe\u3057\u3066\u306f\u52b9\u679c\u3092\u767a\u63ee\u3059\u308b\u4e8b\u304c\u3067\u304d\u307e\u305b\u3093\u3002\u51e6\u7406\u5bfe\u8c61\u306e\u30c7\u30fc\u30bf\u304cPostgreSQL\u306e\u5171\u6709\u30d0\u30c3\u30d5\u30a1\u304b\u3001\u5c11\u306a\u304f\u3068\u3082OS\u306e\u30c7\u30a3\u30b9\u30af\u30ad\u30e3\u30c3\u30b7\u30e5\u306b\u8f09\u308a\u5207\u308b\u7a0b\u5ea6\u306e\u5927\u304d\u3055\u306e\u3082\u306e\u306b\u5bfe\u3057\u3066\u9069\u7528\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002 \u3053\u306e\u5236\u9650\u306f\u3001v2.0\u3067\u5c0e\u5165\u3055\u308c\u308b\u4e88\u5b9a\u306eSSD\u9023\u643a\u6a5f\u80fd\u306b\u3088\u308a\u3042\u308b\u7a0b\u5ea6\u7de9\u548c\u3055\u308c\u308b\u898b\u901a\u3057\u3067\u3059\u3002", 
            "title": "\u5236\u9650\u4e8b\u9805"
        }
    ]
}