{
    "docs": [
        {
            "location": "/", 
            "text": "This chapter introduces the overview of PG-Strom, and developer's community.\n\n\nWhat is PG-Strom?\n\n\nPG-Strom is an extension module of PostgreSQL designed for version 9.6 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set.\n\n\nIts core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications.\n\n\nUnlike some DWH systems, PG-Strom shares the storage system of PostgreSQL which saves data in row-format. It is not always best choice for summary or analytics workloads, however, it is also an advantage as well. Users don't need to export and transform the data from transactional database for processing.\n\n\nPG-Strom v2.0 enhanced the capability to read from the storage. SSD-to-GPU Direct SQL Execution and in-memory columnar cache make up for the slowness of storage devices, and enable to provide massive data blocks to GPU fast which runs SQL workloads.\n\n\nOn the other hands, the feature of PL/CUDA and gstore_fdw allows to run highly computing density problems, like advanced statistical analytics or machine learning, on the database management system, and to return only results to users.\n\n\nLicense and Copyright\n\n\nPG-Strom is an open source software distributed under the GPL(GNU Public License) v2.\nSee \nLICENSE\n for the license details.\n\n\nPG-Strom Development Team reserves the copyright of the software.\nPG-Strom Development Team is an international, unincorporated association of individuals and companies who have contributed to the PG-Strom project, but not a legal entity.\n\n\nCommunity\n\n\nWe have a community mailing-list at: \nPG-Strom community ML\n It is a right place to post questions, requests, troubles and etc, related to PG-Strom project.\n\n\nPlease pay attention it is a public list for world wide. So, it is your own responsibility not to disclose confidential information.\n\n\nThe primary language of the mailing-list is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach \n(JP)\n prefix on the subject like, for non-Japanese speakers to skip messages.\n\n\nBug or troubles report\n\n\nIf you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue with \nbug\n tag at the \nPG-Strom Issue Tracker\n.\n\n\nPlease ensure the items below on bug reports.\n\n\n\n\nWhether you can reproduce the same problem on the latest revision?\n\n\nHopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software.\n\n\n\n\n\n\nWhether you can reproduce the same problem if PG-Strom is disabled?\n\n\nGUC option pg_strom.enabled can turn on/off PG-Strom.\n\n\n\n\n\n\nIs there any known issues on the issue tracker of GitHub?\n\n\nPlease don't forget to search \nclosed\n issues\n\n\n\n\n\n\n\n\nThe information below are helpful for bug-reports.\n\n\n\n\nOutput of \nEXPLAIN VERBOSE\n for the queries in trouble.\n\n\nData structure of the tables involved with \n\\d+ \ntable name\n on psql command.\n\n\nLog messages (verbose messages are more helpful)\n\n\nStatus of GUC options you modified from the default configurations.\n\n\nHardware configuration - GPU model and host RAM size especially.\n\n\n\n\nIf you are not certain whether the strange behavior on your site is bug or not, please report it to the mailing-list prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information.\n\n\nNew features proposition\n\n\nIf you have any ideas of new features, please open a new issue with \nfeature\n tag at the \nPG-Strom Issue Tracker\n, then have a discussion with other developers.\n\n\nA preferable design proposal will contain the items below.\n\n\n\n\nWhat is your problem to solve / improve?\n\n\nHow much serious is it on your workloads / user case?\n\n\nWay to implement your idea?\n\n\nExpected downside, if any.\n\n\n\n\nOnce we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed.\n\n\nOnce a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again.\n\n\nOn the development stage, please attach patch file on the issue ticket. We don't use pull request.\n\n\nSupport Policy\n\n\nThe PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release.\n\n\nPlease note that it is volunteer based community support policy, so our support is best effort and no SLA definition.\n\n\nIf you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom).\n\n\nVersioning Policy\n\n\nPG-Strom's version number is consists of two portion; major and minor version. \nmajor\n.\nminor\n\n\nIts minor version shall be incremented for each release; including bug fixes and new features.\nIts major version shall be incremented in the following situation.\n\n\n\n\nSome of supported PostgreSQL version gets deprecated.\n\n\nSome of supported GPU devices gets deprecated.\n\n\nNew version adds epoch making features.", 
            "title": "Home"
        }, 
        {
            "location": "/#what-is-pg-strom", 
            "text": "PG-Strom is an extension module of PostgreSQL designed for version 9.6 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set.  Its core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications.  Unlike some DWH systems, PG-Strom shares the storage system of PostgreSQL which saves data in row-format. It is not always best choice for summary or analytics workloads, however, it is also an advantage as well. Users don't need to export and transform the data from transactional database for processing.  PG-Strom v2.0 enhanced the capability to read from the storage. SSD-to-GPU Direct SQL Execution and in-memory columnar cache make up for the slowness of storage devices, and enable to provide massive data blocks to GPU fast which runs SQL workloads.  On the other hands, the feature of PL/CUDA and gstore_fdw allows to run highly computing density problems, like advanced statistical analytics or machine learning, on the database management system, and to return only results to users.", 
            "title": "What is PG-Strom?"
        }, 
        {
            "location": "/#license-and-copyright", 
            "text": "PG-Strom is an open source software distributed under the GPL(GNU Public License) v2.\nSee  LICENSE  for the license details.  PG-Strom Development Team reserves the copyright of the software.\nPG-Strom Development Team is an international, unincorporated association of individuals and companies who have contributed to the PG-Strom project, but not a legal entity.", 
            "title": "License and Copyright"
        }, 
        {
            "location": "/#community", 
            "text": "We have a community mailing-list at:  PG-Strom community ML  It is a right place to post questions, requests, troubles and etc, related to PG-Strom project.  Please pay attention it is a public list for world wide. So, it is your own responsibility not to disclose confidential information.  The primary language of the mailing-list is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach  (JP)  prefix on the subject like, for non-Japanese speakers to skip messages.", 
            "title": "Community"
        }, 
        {
            "location": "/#bug-or-troubles-report", 
            "text": "If you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue with  bug  tag at the  PG-Strom Issue Tracker .  Please ensure the items below on bug reports.   Whether you can reproduce the same problem on the latest revision?  Hopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software.    Whether you can reproduce the same problem if PG-Strom is disabled?  GUC option pg_strom.enabled can turn on/off PG-Strom.    Is there any known issues on the issue tracker of GitHub?  Please don't forget to search  closed  issues     The information below are helpful for bug-reports.   Output of  EXPLAIN VERBOSE  for the queries in trouble.  Data structure of the tables involved with  \\d+  table name  on psql command.  Log messages (verbose messages are more helpful)  Status of GUC options you modified from the default configurations.  Hardware configuration - GPU model and host RAM size especially.   If you are not certain whether the strange behavior on your site is bug or not, please report it to the mailing-list prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information.", 
            "title": "Bug or troubles report"
        }, 
        {
            "location": "/#new-features-proposition", 
            "text": "If you have any ideas of new features, please open a new issue with  feature  tag at the  PG-Strom Issue Tracker , then have a discussion with other developers.  A preferable design proposal will contain the items below.   What is your problem to solve / improve?  How much serious is it on your workloads / user case?  Way to implement your idea?  Expected downside, if any.   Once we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed.  Once a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again.  On the development stage, please attach patch file on the issue ticket. We don't use pull request.", 
            "title": "New features proposition"
        }, 
        {
            "location": "/#support-policy", 
            "text": "The PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release.  Please note that it is volunteer based community support policy, so our support is best effort and no SLA definition.  If you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom).", 
            "title": "Support Policy"
        }, 
        {
            "location": "/#versioning-policy", 
            "text": "PG-Strom's version number is consists of two portion; major and minor version.  major . minor  Its minor version shall be incremented for each release; including bug fixes and new features.\nIts major version shall be incremented in the following situation.   Some of supported PostgreSQL version gets deprecated.  Some of supported GPU devices gets deprecated.  New version adds epoch making features.", 
            "title": "Versioning Policy"
        }, 
        {
            "location": "/install/", 
            "text": "This chapter introduces the steps to install PG-Strom.\n\n\nChecklist\n\n\n\n\nServer Hardware\n\n\nIt requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices.\n\n\nnote002:HW Validation List\n may help you to choose the hardware.\n\n\nSSD-to-GPU Direct SQL Execution needs SSD devices which support NVMe specification, and to be installed under the same PCIe Root Complex where GPU is located on.\n\n\n\n\n\n\nGPU Device\n\n\nPG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later;\n\n\nnote001:GPU Availability Matrix\n shows more detailed information. Check this list for the support status of SSD-to-GPU Direct SQL Execution.\n\n\n\n\n\n\nOperating System\n\n\nPG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or CentOS version 7.x series.    - SSD-to-GPU Direct SQL Execution needs Red Hat Enterprise Linux or CentOS version 7.3 or later.\n\n\n\n\n\n\nPostgreSQL\n\n\nPG-Strom requires PostgreSQL version 9.6 or later. PostgreSQL v9.6 renew the custom-scan interface for CPU-parallel execution or \nGROUP BY\n planning, thus, it allows cooperation of custom-plans provides by extension modules.\n\n\n\n\n\n\nCUDA Toolkit\n\n\nPG-Strom requires CUDA Toolkit version 9.1 or later.\n\n\nPG-Strom provides half-precision floating point type (\nfloat2\n), and it internally use \nhalf_t\n type of CUDA C, so we cannot build it with older CUDA Toolkit.\n\n\n\n\n\n\n\n\nOS Installation\n\n\nChoose a Linux distribution which is supported by CUDA Toolkit, then install the system according to the installation process of the distribution. \nNVIDIA DEVELOPER ZONE\n introduces the list of Linux distributions which are supported by CUDA Toolkit.\n\n\nIn case of Red Hat Enterprise Linux 7.x or CentOS 7.x series, choose \"Minimal installation\" as base environment, and also check the following add-ons.\n\n\n\n\nDebugging Tools\n\n\nDevelopment Tools\n\n\n\n\nPost OS Installation Configuration\n\n\nNext to the OS installation, a few additionsl configurations are required to install GPU-drivers and NVMe-Strom driver on the later steps.\n\n\nSetup EPEL Repository\n\n\nSeveral software modules required by PG-Strom are distributed as a part of EPEL (Extra Packages for Enterprise Linux).\nYou need to add a repository definition of EPEL packages for yum system to obtain these software.\n\n\nOne of the package we will get from EPEL repository is DKMS (Dynamic Kernel Module Support). It is a framework to build Linux kernel module for the running Linux kernel on demand; used for NVIDIA's GPU driver or NVMe-Strom which is a kernel module to support SSD-to-GPU Direct SQL Execution.\n\n\nepel-release\n package provides the repository definition of EPEL.\nYou can obtain this package from the public FTP site of Fedora Project. Downloads the \nepel-release-\ndistribution version\n.noarch.rpm\n, and install the package.\nOnce \nepel-release\n package gets installed, yum system configuration is updated to get software from the EPEL repository.\n\n\n\n\nFedora Project Public FTP Site\n\n\nhttps://dl.fedoraproject.org/pub/epel/7/x86_64/\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nWalk down the directory: \nPackages\n --\n \ne\n, from the above URL.\n\n\n\n\nInstall the \nepel-release\n package as follows.\n\n\n$ sudo yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm\n          :\n================================================================================\n Package           Arch        Version     Repository                      Size\n================================================================================\nInstalling:\n epel-release      noarch      7-11        /epel-release-7-11.noarch       24 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n          :\nInstalled:\n  epel-release.noarch 0:7-11\n\nComplete!\n\n\n\n\nHeteroDB-SWDC Installation\n\n\nPG-Strom and related packages are distributed from \nHeteroDB Software Distribution Center\n.\nYou need to add a repository definition of HeteroDB-SWDC for you system to obtain these software.\n\n\nheterodb-swdc\n package provides the repository definition of HeteroDB-SWDC.\nAccess to the \nHeteroDB Software Distribution Center\n using Web browser, download the \nheterodb-swdc-1.0-1.el7.noarch.rpm\n on top of the file list, then install this package.\nOnce heterodb-swdc package gets installed, yum system configuration is updated to get software from the HeteroDB-SWDC repository.\n\n\nInstall the \nheterodb-swdc\n package as follows.\n\n\n$ sudo yum install https://heterodb.github.io/swdc/yum/rhel7-x86_64/heterodb-swdc-1.0-1.el7.noarch.rpm\n          :\n================================================================================\n Package         Arch     Version       Repository                         Size\n================================================================================\nInstalling:\n heterodb-swdc   noarch   1.0-1.el7     /heterodb-swdc-1.0-1.el7.noarch   2.4 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n          :\nInstalled:\n  heterodb-swdc.noarch 0:1.0-1.el7\n\nComplete!\n\n\n\n\nCUDA Toolkit Installation\n\n\nThis section introduces the installation of CUDA Toolkit. If you already installed the latest CUDA Toolkit, you can skip this section.\n\n\nNVIDIA offers two approach to install CUDA Toolkit; one is by self-extracting archive (called runfile), and the other is by RPM packages.\nWe recommend RPM installation because it allows simple software updates.\n\n\nYou can download the installation package for CUDA Toolkit from NVIDIA DEVELOPER ZONE. Choose your OS, architecture, distribution and version, then choose \"rpm(network)\" edition.\n\n\n\n\nThe \"rpm(network)\" edition contains only yum repositoty definition to distribute CUDA Toolkit. It is similar to the EPEL repository definition at the OS installation.\nSo, you needs to installa the related RPM packages over network after the resistoration of CUDA repository. Run the following command.\n\n\n$ sudo rpm -i cuda-repo-\ndistribution\n-\nversion\n.x86_64.rpm\n$ sudo yum clean all\n$ sudo yum install cuda\n\n\n\n\nOnce installation completed successfully, CUDA Toolkit is deployed at \n/usr/local/cuda\n.\n\n\n$ ls /usr/local/cuda\nbin     include  libnsight         nvml       samples  tools\ndoc     jre      libnvvp           nvvm       share    version.txt\nextras  lib64    nsightee_plugins  pkgconfig  src\n\n\n\n\nOnce installation gets completed, ensure the system recognizes the GPU devices correctly.\n\nnvidia-smi\n command shows GPU information installed on your system, as follows.\n\n\n$ nvidia-smi\nWed Feb 14 09:43:48 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   41C    P0    37W / 250W |      0MiB / 16152MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\n\n\nTip\n\n\nIf nouveau driver which conflicts to nvidia driver is loaded, system cannot load the nvidia driver immediately.\nIn this case, reboot the operating system once, then confirm whether you can run nvidia-smi command successfully, or not. CUDA installer also disables nouveau driver, nouveau driver will not be loaded on the next boot.\n\n\n\n\nPostgreSQL Installation\n\n\nThis section introduces PostgreSQL installation with RPM.\nWe don't introduce the installation steps from the source because there are many documents for this approach, and there are also various options for the \n./configure\n script.\n\n\nPostgreSQL is also distributed in the packages of Linux distributions, however, it is not the latest one, and often older than the version which supports PG-Strom. For example, Red Hat Enterprise Linux 7.x or CentOS 7.x distributes PostgreSQL v9.2.x series. This version had been EOL by the PostgreSQL community.\n\n\nPostgreSQL Global Development Group provides yum repository to distribute the latest PostgreSQL and related packages.\nLike the configuration of EPEL, you can install a small package to set up yum repository, then install PostgreSQL and related software.\n\n\nHere is the list of yum repository definition: \nhttp://yum.postgresql.org/repopackages.php\n.\n\n\nRepository definitions are per PostgreSQL major version and Linux distribution. You need to choose the one for your Linux distribution, and for PostgreSQL v9.6 or later.\n\n\nAll you need to install are yum repository definition, and PostgreSQL packages. If you choose PostgreSQL v10, the pakages below are required to install PG-Strom.\n\n\n\n\npostgresql10-devel\n\n\npostgresql10-server\n\n\n\n\n$ sudo yum install -y https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm\n$ sudo yum install -y postgresql10-server postgresql10-devel\n          :\n================================================================================\n Package                  Arch        Version                 Repository   Size\n================================================================================\nInstalling:\n postgresql10-devel       x86_64      10.2-1PGDG.rhel7        pgdg10      2.0 M\n postgresql10-server      x86_64      10.2-1PGDG.rhel7        pgdg10      4.4 M\nInstalling for dependencies:\n postgresql10             x86_64      10.2-1PGDG.rhel7        pgdg10      1.5 M\n postgresql10-libs        x86_64      10.2-1PGDG.rhel7        pgdg10      354 k\n\nTransaction Summary\n================================================================================\nInstall  2 Packages (+2 Dependent packages)\n          :\nInstalled:\n  postgresql10-devel.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-server.x86_64 0:10.2-1PGDG.rhel7\n\nDependency Installed:\n  postgresql10.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-libs.x86_64 0:10.2-1PGDG.rhel7\n\nComplete!\n\n\n\n\nThe RPM packages provided by PostgreSQL Global Development Group installs software under the \n/usr/pgsql-\nversion\n directory, so you may pay attention whether the PATH environment variable is configured appropriately.\n\n\npostgresql-alternative\n package set up symbolic links to the related commands under \n/usr/local/bin\n, so allows to simplify the operations. Also, it enables to switch target version using \nalternatives\n command even if multiple version of PostgreSQL.\n\n\n$ sudo yum install postgresql-alternatives\n          :\nResolving Dependencies\n--\n Running transaction check\n---\n Package postgresql-alternatives.noarch 0:1.0-1.el7 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n          :\n================================================================================\n Package                      Arch        Version           Repository     Size\n================================================================================\nInstalling:\n postgresql-alternatives      noarch      1.0-1.el7         heterodb      9.2 k\n\nTransaction Summary\n================================================================================\n          :\nInstalled:\n  postgresql-alternatives.noarch 0:1.0-1.el7\n\nComplete!\n\n\n\n\nPG-Strom Installation\n\n\nRPM Installation\n\n\nPG-Strom and related packages are distributed from \nHeteroDB Software Distribution Center\n.\nIf you repository definition has been added, not many tasks are needed.\n\n\nWe provide individual RPM packages of PG-Strom for each base PostgreSQL version. \npg_strom-PG96\n package is built for PostgreSQL 9.6, and \npg_strom-PG10\n is also built for PostgreSQL v10.\n\n\n$ sudo yum install pg_strom-PG10\n          :\n================================================================================\n Package              Arch          Version               Repository       Size\n================================================================================\nInstalling:\n pg_strom-PG10        x86_64        1.9-180301.el7        heterodb        320 k\n\nTransaction Summary\n================================================================================\n          :\nInstalled:\n  pg_strom-PG10.x86_64 0:1.9-180301.el7\n\nComplete!\n\n\n\n\nThat's all for package installation.\n\n\nInstallation from the source\n\n\nFor developers, we also introduces the steps to build and install PG-Strom from the source code.\n\n\nGetting the source code\n\n\nLike RPM packages, you can download tarball of the source code from \nHeteroDB Software Distribution Center\n.\nOn the other hands, here is a certain time-lags to release the tarball, it may be preferable to checkout the master branch of \nPG-Strom on GitHub\n to use the latest development branch.\n\n\n$ git clone https://github.com/heterodb/pg-strom.git\nCloning into 'pg-strom'...\nremote: Counting objects: 13797, done.\nremote: Compressing objects: 100% (215/215), done.\nremote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400\nReceiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done.\nResolving deltas: 100% (10504/10504), done.\n\n\n\n\nBuilding the PG-Strom\n\n\nConfiguration to build PG-Strom must match to the target PostgreSQL strictly. For example, if a particular \nstrcut\n has inconsistent layout by the configuration at build, it may lead problematic bugs; not easy to find out.\nThus, not to have inconsistency, PG-Strom does not have own configure script, but references the build configuration of PostgreSQL using \npg_config\n command.\n\n\nIf PATH environment variable is set to the \npg_config\n command of the target PostgreSQL, run \nmake\n and \nmake install\n.\nElsewhere, give \nPG_CONFIG=...\n parameter on \nmake\n command to tell the full path of the \npg_config\n command.\n\n\n$ cd pg-strom\n$ make PG_CONFIG=/usr/pgsql-10/bin/pg_config\n$ sudo make install PG_CONFIG=/usr/pgsql-10/bin/pg_config\n\n\n\n\nPost Installation Setup\n\n\nCreation of database cluster\n\n\nDatabase cluster is not constructed yet, run \ninitdb\n command to set up initial database of PostgreSQL.\n\n\nThe default path of the database cluster on RPM installation is \n/var/lib/pgsql/\nversion number\n/data\n.\nIf you install \npostgresql-alternatives\n package, this default path can be referenced by \n/var/lib/pgdata\n regardless of the PostgreSQL version.\n\n\n$ sudo su - postgres\n$ initdb -D /var/lib/pgdata/\nThe files belonging to this database system will be owned by user \npostgres\n.\nThis user must also own the server process.\n\nThe database cluster will be initialized with locale \nen_US.UTF-8\n.\nThe default database encoding has accordingly been set to \nUTF8\n.\nThe default text search configuration will be set to \nenglish\n.\n\nData page checksums are disabled.\n\nfixing permissions on existing directory /var/lib/pgdata ... ok\ncreating subdirectories ... ok\nselecting default max_connections ... 100\nselecting default shared_buffers ... 128MB\nselecting dynamic shared memory implementation ... posix\ncreating configuration files ... ok\nrunning bootstrap script ... ok\nperforming post-bootstrap initialization ... ok\nsyncing data to disk ... ok\n\nWARNING: enabling \ntrust\n authentication for local connections\nYou can change this by editing pg_hba.conf or using the option -A, or\n--auth-local and --auth-host, the next time you run initdb.\n\nSuccess. You can now start the database server using:\n\n    pg_ctl -D /var/lib/pgdata/ -l logfile start\n\n\n\n\nSetup postgresql.conf\n\n\nNext, edit \npostgresql.conf\n which is a configuration file of PostgreSQL.\nThe parameters below should be edited at least to work PG-Strom.\nInvestigate other parameters according to usage of the system and expected workloads.\n\n\n\n\nshared_preload_libraries\n\n\nPG-Strom module must be loaded on startup of the postmaster process by the \nshared_preload_libraries\n. Unable to load it on demand. Therefore, you must add the configuration below.\n\n\nshared_preload_libraries = '$libdir/pg_strom'\n\n\n\n\n\n\nmax_worker_processes\n\n\nPG-Strom internally uses several background workers, so the default configuration (= 8) is too small for other usage. So, we recommand to expand the variable for a certain margin.\n\n\nmax_worker_processes = 100\n\n\n\n\n\n\nshared_buffers\n\n\nAlthough it depends on the workloads, the initial configuration of \nshared_buffers\n is too small for the data size where PG-Strom tries to work, thus storage workloads restricts the entire performance, and may be unable to work GPU efficiently.\n\n\nSo, we recommend to expand the variable for a certain margin.\n\n\nshared_buffers = 10GB\n\n\nPlease consider to apply \nSSD-to-GPU Direct SQL Execution\n to process larger than system's physical RAM size.\n\n\nPlease consider to apply \nColumnar Cache\n if you want to cache particular tables.\n\n\n\n\n\n\nwork_mem\n\n\nAlthough it depends on the workloads, the initial configuration of \nwork_mem\n is too small to choose the optimal query execution plan on analytic queries.\n\n\nAn typical example is, disk-based merge sort may be chosen instead of the in-memory quick-sorting.\n\n\nSo, we recommend to expand the variable for a certain margin.\n\n\nwork_mem = 1GB\n\n\n\n\n\n\n\n\nStart PostgreSQL\n\n\nStart PostgreSQL service.\n\n\nIf PG-Strom is set up appropriately, it writes out log message which shows PG-Strom recognized GPU devices.\nThe example below recognized the Tesla V100(PCIe; 16GB edition) device.\n\n\n# systemctl start postgresql-10\n# systemctl status -l postgresql-10\n* postgresql-10.service - PostgreSQL 10 database server\n   Loaded: loaded (/usr/lib/systemd/system/postgresql-10.service; disabled; vendor preset: disabled)\n   Active: active (running) since Sat 2018-03-03 15:45:23 JST; 2min 21s ago\n     Docs: https://www.postgresql.org/docs/10/static/\n  Process: 24851 ExecStartPre=/usr/pgsql-10/bin/postgresql-10-check-db-dir ${PGDATA} (code=exited, status=0/SUCCESS)\n Main PID: 24858 (postmaster)\n   CGroup: /system.slice/postgresql-10.service\n           |-24858 /usr/pgsql-10/bin/postmaster -D /var/lib/pgsql/10/data/\n           |-24890 postgres: logger process\n           |-24892 postgres: bgworker: PG-Strom GPU memory keeper\n           |-24896 postgres: checkpointer process\n           |-24897 postgres: writer process\n           |-24898 postgres: wal writer process\n           |-24899 postgres: autovacuum launcher process\n           |-24900 postgres: stats collector process\n           |-24901 postgres: bgworker: PG-Strom ccache-builder2\n           |-24902 postgres: bgworker: PG-Strom ccache-builder1\n           `-24903 postgres: bgworker: logical replication launcher\n\nMar 03 15:45:19 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:19.195 JST [24858] HINT:  Run 'nvidia-cuda-mps-control -d', then start server process. Check 'man nvidia-cuda-mps-control' for more details.\nMar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.509 JST [24858] LOG:  PG-Strom: GPU0 Tesla V100-PCIE-16GB (5120 CUDA cores; 1380MHz, L2 6144kB), RAM 15.78GB (4096bits, 856MHz), CC 7.0\nMar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.510 JST [24858] LOG:  NVRTC - CUDA Runtime Compilation vertion 9.1\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG:  listening on IPv6 address \n::1\n, port 5432\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG:  listening on IPv4 address \n127.0.0.1\n, port 5432\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.442 JST [24858] LOG:  listening on Unix socket \n/var/run/postgresql/.s.PGSQL.5432\n\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.492 JST [24858] LOG:  listening on Unix socket \n/tmp/.s.PGSQL.5432\n\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] LOG:  redirecting log output to logging collector process\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] HINT:  Future log output will appear in directory \nlog\n.\nMar 03 15:45:23 saba.heterodb.com systemd[1]: Started PostgreSQL 10 database server.\n\n\n\n\nCreation of PG-Strom related objects\n\n\nAt the last, create database objects related to PG-Strom, like SQL functions.\nThis steps are packaged using EXTENSION feature of PostgreSQL. So, all you needs to run is \nCREATE EXTENSION\n on the SQL command line.\n\n\nPlease note that this step is needed for each new database.\nIf you want PG-Strom is pre-configured on new database creation, you can create PG-Strom extension on the \ntemplate1\n database, its configuration will be copied to the new database on \nCREATE DATABASE\n command.\n\n\n$ psql postgres -U postgres\npsql (10.2)\nType \nhelp\n for help.\n\npostgres=# CREATE EXTENSION pg_strom ;\nCREATE EXTENSION\n\n\n\n\nThat's all for the installation.\n\n\nNVME-Strom module\n\n\nThis section also introduces NVME-Strom Linux kernel module which is closely cooperating with core features of PG-Strom like SSD-to-GPU Direct SQL Execution, even if it is an independent software module.\n\n\nGetting the module and installation\n\n\nLike other PG-Strom related modules, NVME-Strom is distributed at the (https://heterodb.github.io/swdc/)[HeteroDB Software Distribution Center] as a free software. In other words, it is not an open source software.\n\n\nIf your system already setup \nheterodb-swdc\n package, \nyum install\n command downloads the RPM file and install the \nnvme_strom\n package.\n\n\n$ sudo yum install nvme_strom\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.cat.net\n * epel: ftp.iij.ad.jp\n * extras: mirrors.cat.net\n * ius: mirrors.kernel.org\n * updates: mirrors.cat.net\nResolving Dependencies\n--\n Running transaction check\n---\n Package nvme_strom.x86_64 0:1.3-1.el7 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch            Version            Repository         Size\n================================================================================\nInstalling:\n nvme_strom          x86_64          1.3-1.el7          heterodb          273 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n\nTotal download size: 273 k\nInstalled size: 1.5 M\nIs this ok [y/d/N]: y\nDownloading packages:\nNo Presto metadata available for heterodb\nnvme_strom-1.3-1.el7.x86_64.rpm                            | 273 kB   00:00\nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : nvme_strom-1.3-1.el7.x86_64                                  1/1\n  :\n\nsnip\n\n  :\nDKMS: install completed.\n  Verifying  : nvme_strom-1.3-1.el7.x86_64                                  1/1\n\nInstalled:\n  nvme_strom.x86_64 0:1.3-1.el7\n\nComplete!\n\n\n\n\nLicense activation\n\n\nLicense activation is needed to use all the features of NVME-Strom module, provided by HeteroDB,Inc. You can operate the system without license, but features below are restricted.\n- Striping support (md-raid0) at SSD-to-GPU Direct SQL Execution\n- Compression support at in-memory columnar cache\n\n\nYou can obtain a license file, like as a plain text below, from HeteroDB,Inc.\n\n\nIAwPOAC44m8LPMoV7bMykhxM27LAVrktspcaMHki8pI1fXrxq0KzqPDK4LzAA9n26IRAr/4ymB6QJ3/JxZOfYTVsbWq66vEtTAIuZVmJ/I888zRATj1hoofh1WbIwd3/ix28Cy1v16KCgLrlqPsra6NJScMOOHnuYoWWmWe4ml+n6GVEIb7ChUJvZbEZSO/DiLXosFc0N+MD4JTEU/XsBUP9ufacpbosW/YG2nOib3mpvhkfn7RQy2T5CVQeuGjM9Taj7DN5xipqU/Q0hZaZKA8EsZwsB6b4c7usdmPILyIpuTrWnEbjJ6worOQWHA+nL87xkDL1XYGH6UVc291QPLwk=\n----\nVERSION:1\nSERIAL_NR:HDB-TRIAL\nISSUED_AT:2018-08-16\nEXPIRED_AT:2018-09-15\nNR_GPUS:1\nLICENSEE_ORG:Capybara Kingdom\nLICENSEE_NAME:Herr.Wassershweine\nLICENSEE_MAIL:capybara@examplecom\n\n\n\n\nCopy the license file to \n/etc/heterodb.license\n, then restart PostgreSQL.\n\n\nThe startup log messages of PostgreSQL dumps the license information, and it tells us the license activation is successfully done.\n\n\n$ pg_ctl restart\n   :\nLOG:  PG-Strom built for PostgreSQL 11\nLOG:  PG-Strom: GPU0 Tesla V100-PCIE-16GB (5120 CUDA cores; 1380MHz, L2 6144kB), RAM 15.78GB (4096bits, 856MHz), CC 7.0\n   :\n   :\nLOG:  HeteroDB License: { \nversion\n : 1, \nserial_nr\n : \nHDB-TRIAL\n, \nissued_at\n : \n16-Aug-2018\n, \nexpired_at\n : \n15-Sep-2018\n, \nnr_gpus\n : 1, \nlicensee_org\n : \nCapybara Kingdom\n, \nlicensee_name\n : \nHerr.Wassershweine\n, \nlicensee_mail\n : \ncapybara@examplecom\n }\nLOG:  listening on IPv6 address \n::1\n, port 5432\nLOG:  listening on IPv4 address \n127.0.0.1\n, port 5432\nLOG:  listening on Unix socket \n/tmp/.s.PGSQL.5432\n\n   :\n\n\n\n\nKernel module parameters\n\n\nNVME-Strom Linux kernel module has some parameters.\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nverbose\n\n\nint\n\n\n0\n\n\nEnables detailed debug output\n\n\n\n\n\n\nstat_info\n\n\nint\n\n\n1\n\n\nEnables performance statistics\n\n\n\n\n\n\nfast_ssd_mode\n\n\nint\n\n\n0\n\n\nOperating mode for fast NVME-SSD\n\n\n\n\n\n\n\n\nHere is an extra explanation for \nfast_ssd_mode\n parameter.\n\n\nWhen NVME-Strom Linux kernel module get a request for SSD-to-GPU direct data transfer, first of all, it checks whether the required data blocks are caches on page-caches of operating system.\nIf \nfast_ssd_mode\n is \n0\n, NVME-Strom once writes back page caches of the required data blocks to the userspace buffer of the caller, then indicates application to invoke normal host--\ndevice data transfer by CUDA API. It is suitable for non-fast NVME-SSDs such as PCIe x4 grade.\n\n\nOn the other hands, SSD-to-GPU direct data transfer may be faster, if you use PCIe x8 grade fast NVME-SSD or use multiple SSDs in striping mode, than normal host--\ndevice data transfer after the buffer copy. If \nfast_ssd_mode\n is not \n0\n, NVME-Strom kicks SSD-to-GPU direct data transfer regardless of the page cache state.\n\n\nHowever, it shall never kicks SSD-to-GPU direct data transfer if page cache is dirty.", 
            "title": "Install"
        }, 
        {
            "location": "/install/#checklist", 
            "text": "Server Hardware  It requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices.  note002:HW Validation List  may help you to choose the hardware.  SSD-to-GPU Direct SQL Execution needs SSD devices which support NVMe specification, and to be installed under the same PCIe Root Complex where GPU is located on.    GPU Device  PG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later;  note001:GPU Availability Matrix  shows more detailed information. Check this list for the support status of SSD-to-GPU Direct SQL Execution.    Operating System  PG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or CentOS version 7.x series.    - SSD-to-GPU Direct SQL Execution needs Red Hat Enterprise Linux or CentOS version 7.3 or later.    PostgreSQL  PG-Strom requires PostgreSQL version 9.6 or later. PostgreSQL v9.6 renew the custom-scan interface for CPU-parallel execution or  GROUP BY  planning, thus, it allows cooperation of custom-plans provides by extension modules.    CUDA Toolkit  PG-Strom requires CUDA Toolkit version 9.1 or later.  PG-Strom provides half-precision floating point type ( float2 ), and it internally use  half_t  type of CUDA C, so we cannot build it with older CUDA Toolkit.", 
            "title": "Checklist"
        }, 
        {
            "location": "/install/#os-installation", 
            "text": "Choose a Linux distribution which is supported by CUDA Toolkit, then install the system according to the installation process of the distribution.  NVIDIA DEVELOPER ZONE  introduces the list of Linux distributions which are supported by CUDA Toolkit.  In case of Red Hat Enterprise Linux 7.x or CentOS 7.x series, choose \"Minimal installation\" as base environment, and also check the following add-ons.   Debugging Tools  Development Tools", 
            "title": "OS Installation"
        }, 
        {
            "location": "/install/#post-os-installation-configuration", 
            "text": "Next to the OS installation, a few additionsl configurations are required to install GPU-drivers and NVMe-Strom driver on the later steps.", 
            "title": "Post OS Installation Configuration"
        }, 
        {
            "location": "/install/#setup-epel-repository", 
            "text": "Several software modules required by PG-Strom are distributed as a part of EPEL (Extra Packages for Enterprise Linux).\nYou need to add a repository definition of EPEL packages for yum system to obtain these software.  One of the package we will get from EPEL repository is DKMS (Dynamic Kernel Module Support). It is a framework to build Linux kernel module for the running Linux kernel on demand; used for NVIDIA's GPU driver or NVMe-Strom which is a kernel module to support SSD-to-GPU Direct SQL Execution.  epel-release  package provides the repository definition of EPEL.\nYou can obtain this package from the public FTP site of Fedora Project. Downloads the  epel-release- distribution version .noarch.rpm , and install the package.\nOnce  epel-release  package gets installed, yum system configuration is updated to get software from the EPEL repository.   Fedora Project Public FTP Site  https://dl.fedoraproject.org/pub/epel/7/x86_64/      Tip  Walk down the directory:  Packages  --   e , from the above URL.   Install the  epel-release  package as follows.  $ sudo yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm\n          :\n================================================================================\n Package           Arch        Version     Repository                      Size\n================================================================================\nInstalling:\n epel-release      noarch      7-11        /epel-release-7-11.noarch       24 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n          :\nInstalled:\n  epel-release.noarch 0:7-11\n\nComplete!", 
            "title": "Setup EPEL Repository"
        }, 
        {
            "location": "/install/#heterodb-swdc-installation", 
            "text": "PG-Strom and related packages are distributed from  HeteroDB Software Distribution Center .\nYou need to add a repository definition of HeteroDB-SWDC for you system to obtain these software.  heterodb-swdc  package provides the repository definition of HeteroDB-SWDC.\nAccess to the  HeteroDB Software Distribution Center  using Web browser, download the  heterodb-swdc-1.0-1.el7.noarch.rpm  on top of the file list, then install this package.\nOnce heterodb-swdc package gets installed, yum system configuration is updated to get software from the HeteroDB-SWDC repository.  Install the  heterodb-swdc  package as follows.  $ sudo yum install https://heterodb.github.io/swdc/yum/rhel7-x86_64/heterodb-swdc-1.0-1.el7.noarch.rpm\n          :\n================================================================================\n Package         Arch     Version       Repository                         Size\n================================================================================\nInstalling:\n heterodb-swdc   noarch   1.0-1.el7     /heterodb-swdc-1.0-1.el7.noarch   2.4 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n          :\nInstalled:\n  heterodb-swdc.noarch 0:1.0-1.el7\n\nComplete!", 
            "title": "HeteroDB-SWDC Installation"
        }, 
        {
            "location": "/install/#cuda-toolkit-installation", 
            "text": "This section introduces the installation of CUDA Toolkit. If you already installed the latest CUDA Toolkit, you can skip this section.  NVIDIA offers two approach to install CUDA Toolkit; one is by self-extracting archive (called runfile), and the other is by RPM packages.\nWe recommend RPM installation because it allows simple software updates.  You can download the installation package for CUDA Toolkit from NVIDIA DEVELOPER ZONE. Choose your OS, architecture, distribution and version, then choose \"rpm(network)\" edition.   The \"rpm(network)\" edition contains only yum repositoty definition to distribute CUDA Toolkit. It is similar to the EPEL repository definition at the OS installation.\nSo, you needs to installa the related RPM packages over network after the resistoration of CUDA repository. Run the following command.  $ sudo rpm -i cuda-repo- distribution - version .x86_64.rpm\n$ sudo yum clean all\n$ sudo yum install cuda  Once installation completed successfully, CUDA Toolkit is deployed at  /usr/local/cuda .  $ ls /usr/local/cuda\nbin     include  libnsight         nvml       samples  tools\ndoc     jre      libnvvp           nvvm       share    version.txt\nextras  lib64    nsightee_plugins  pkgconfig  src  Once installation gets completed, ensure the system recognizes the GPU devices correctly. nvidia-smi  command shows GPU information installed on your system, as follows.  $ nvidia-smi\nWed Feb 14 09:43:48 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   41C    P0    37W / 250W |      0MiB / 16152MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+   Tip  If nouveau driver which conflicts to nvidia driver is loaded, system cannot load the nvidia driver immediately.\nIn this case, reboot the operating system once, then confirm whether you can run nvidia-smi command successfully, or not. CUDA installer also disables nouveau driver, nouveau driver will not be loaded on the next boot.", 
            "title": "CUDA Toolkit Installation"
        }, 
        {
            "location": "/install/#postgresql-installation", 
            "text": "This section introduces PostgreSQL installation with RPM.\nWe don't introduce the installation steps from the source because there are many documents for this approach, and there are also various options for the  ./configure  script.  PostgreSQL is also distributed in the packages of Linux distributions, however, it is not the latest one, and often older than the version which supports PG-Strom. For example, Red Hat Enterprise Linux 7.x or CentOS 7.x distributes PostgreSQL v9.2.x series. This version had been EOL by the PostgreSQL community.  PostgreSQL Global Development Group provides yum repository to distribute the latest PostgreSQL and related packages.\nLike the configuration of EPEL, you can install a small package to set up yum repository, then install PostgreSQL and related software.  Here is the list of yum repository definition:  http://yum.postgresql.org/repopackages.php .  Repository definitions are per PostgreSQL major version and Linux distribution. You need to choose the one for your Linux distribution, and for PostgreSQL v9.6 or later.  All you need to install are yum repository definition, and PostgreSQL packages. If you choose PostgreSQL v10, the pakages below are required to install PG-Strom.   postgresql10-devel  postgresql10-server   $ sudo yum install -y https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm\n$ sudo yum install -y postgresql10-server postgresql10-devel\n          :\n================================================================================\n Package                  Arch        Version                 Repository   Size\n================================================================================\nInstalling:\n postgresql10-devel       x86_64      10.2-1PGDG.rhel7        pgdg10      2.0 M\n postgresql10-server      x86_64      10.2-1PGDG.rhel7        pgdg10      4.4 M\nInstalling for dependencies:\n postgresql10             x86_64      10.2-1PGDG.rhel7        pgdg10      1.5 M\n postgresql10-libs        x86_64      10.2-1PGDG.rhel7        pgdg10      354 k\n\nTransaction Summary\n================================================================================\nInstall  2 Packages (+2 Dependent packages)\n          :\nInstalled:\n  postgresql10-devel.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-server.x86_64 0:10.2-1PGDG.rhel7\n\nDependency Installed:\n  postgresql10.x86_64 0:10.2-1PGDG.rhel7\n  postgresql10-libs.x86_64 0:10.2-1PGDG.rhel7\n\nComplete!  The RPM packages provided by PostgreSQL Global Development Group installs software under the  /usr/pgsql- version  directory, so you may pay attention whether the PATH environment variable is configured appropriately.  postgresql-alternative  package set up symbolic links to the related commands under  /usr/local/bin , so allows to simplify the operations. Also, it enables to switch target version using  alternatives  command even if multiple version of PostgreSQL.  $ sudo yum install postgresql-alternatives\n          :\nResolving Dependencies\n--  Running transaction check\n---  Package postgresql-alternatives.noarch 0:1.0-1.el7 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n          :\n================================================================================\n Package                      Arch        Version           Repository     Size\n================================================================================\nInstalling:\n postgresql-alternatives      noarch      1.0-1.el7         heterodb      9.2 k\n\nTransaction Summary\n================================================================================\n          :\nInstalled:\n  postgresql-alternatives.noarch 0:1.0-1.el7\n\nComplete!", 
            "title": "PostgreSQL Installation"
        }, 
        {
            "location": "/install/#pg-strom-installation", 
            "text": "", 
            "title": "PG-Strom Installation"
        }, 
        {
            "location": "/install/#rpm-installation", 
            "text": "PG-Strom and related packages are distributed from  HeteroDB Software Distribution Center .\nIf you repository definition has been added, not many tasks are needed.  We provide individual RPM packages of PG-Strom for each base PostgreSQL version.  pg_strom-PG96  package is built for PostgreSQL 9.6, and  pg_strom-PG10  is also built for PostgreSQL v10.  $ sudo yum install pg_strom-PG10\n          :\n================================================================================\n Package              Arch          Version               Repository       Size\n================================================================================\nInstalling:\n pg_strom-PG10        x86_64        1.9-180301.el7        heterodb        320 k\n\nTransaction Summary\n================================================================================\n          :\nInstalled:\n  pg_strom-PG10.x86_64 0:1.9-180301.el7\n\nComplete!  That's all for package installation.", 
            "title": "RPM Installation"
        }, 
        {
            "location": "/install/#installation-from-the-source", 
            "text": "For developers, we also introduces the steps to build and install PG-Strom from the source code.", 
            "title": "Installation from the source"
        }, 
        {
            "location": "/install/#getting-the-source-code", 
            "text": "Like RPM packages, you can download tarball of the source code from  HeteroDB Software Distribution Center .\nOn the other hands, here is a certain time-lags to release the tarball, it may be preferable to checkout the master branch of  PG-Strom on GitHub  to use the latest development branch.  $ git clone https://github.com/heterodb/pg-strom.git\nCloning into 'pg-strom'...\nremote: Counting objects: 13797, done.\nremote: Compressing objects: 100% (215/215), done.\nremote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400\nReceiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done.\nResolving deltas: 100% (10504/10504), done.", 
            "title": "Getting the source code"
        }, 
        {
            "location": "/install/#building-the-pg-strom", 
            "text": "Configuration to build PG-Strom must match to the target PostgreSQL strictly. For example, if a particular  strcut  has inconsistent layout by the configuration at build, it may lead problematic bugs; not easy to find out.\nThus, not to have inconsistency, PG-Strom does not have own configure script, but references the build configuration of PostgreSQL using  pg_config  command.  If PATH environment variable is set to the  pg_config  command of the target PostgreSQL, run  make  and  make install .\nElsewhere, give  PG_CONFIG=...  parameter on  make  command to tell the full path of the  pg_config  command.  $ cd pg-strom\n$ make PG_CONFIG=/usr/pgsql-10/bin/pg_config\n$ sudo make install PG_CONFIG=/usr/pgsql-10/bin/pg_config", 
            "title": "Building the PG-Strom"
        }, 
        {
            "location": "/install/#post-installation-setup", 
            "text": "", 
            "title": "Post Installation Setup"
        }, 
        {
            "location": "/install/#creation-of-database-cluster", 
            "text": "Database cluster is not constructed yet, run  initdb  command to set up initial database of PostgreSQL.  The default path of the database cluster on RPM installation is  /var/lib/pgsql/ version number /data .\nIf you install  postgresql-alternatives  package, this default path can be referenced by  /var/lib/pgdata  regardless of the PostgreSQL version.  $ sudo su - postgres\n$ initdb -D /var/lib/pgdata/\nThe files belonging to this database system will be owned by user  postgres .\nThis user must also own the server process.\n\nThe database cluster will be initialized with locale  en_US.UTF-8 .\nThe default database encoding has accordingly been set to  UTF8 .\nThe default text search configuration will be set to  english .\n\nData page checksums are disabled.\n\nfixing permissions on existing directory /var/lib/pgdata ... ok\ncreating subdirectories ... ok\nselecting default max_connections ... 100\nselecting default shared_buffers ... 128MB\nselecting dynamic shared memory implementation ... posix\ncreating configuration files ... ok\nrunning bootstrap script ... ok\nperforming post-bootstrap initialization ... ok\nsyncing data to disk ... ok\n\nWARNING: enabling  trust  authentication for local connections\nYou can change this by editing pg_hba.conf or using the option -A, or\n--auth-local and --auth-host, the next time you run initdb.\n\nSuccess. You can now start the database server using:\n\n    pg_ctl -D /var/lib/pgdata/ -l logfile start", 
            "title": "Creation of database cluster"
        }, 
        {
            "location": "/install/#setup-postgresqlconf", 
            "text": "Next, edit  postgresql.conf  which is a configuration file of PostgreSQL.\nThe parameters below should be edited at least to work PG-Strom.\nInvestigate other parameters according to usage of the system and expected workloads.   shared_preload_libraries  PG-Strom module must be loaded on startup of the postmaster process by the  shared_preload_libraries . Unable to load it on demand. Therefore, you must add the configuration below.  shared_preload_libraries = '$libdir/pg_strom'    max_worker_processes  PG-Strom internally uses several background workers, so the default configuration (= 8) is too small for other usage. So, we recommand to expand the variable for a certain margin.  max_worker_processes = 100    shared_buffers  Although it depends on the workloads, the initial configuration of  shared_buffers  is too small for the data size where PG-Strom tries to work, thus storage workloads restricts the entire performance, and may be unable to work GPU efficiently.  So, we recommend to expand the variable for a certain margin.  shared_buffers = 10GB  Please consider to apply  SSD-to-GPU Direct SQL Execution  to process larger than system's physical RAM size.  Please consider to apply  Columnar Cache  if you want to cache particular tables.    work_mem  Although it depends on the workloads, the initial configuration of  work_mem  is too small to choose the optimal query execution plan on analytic queries.  An typical example is, disk-based merge sort may be chosen instead of the in-memory quick-sorting.  So, we recommend to expand the variable for a certain margin.  work_mem = 1GB", 
            "title": "Setup postgresql.conf"
        }, 
        {
            "location": "/install/#start-postgresql", 
            "text": "Start PostgreSQL service.  If PG-Strom is set up appropriately, it writes out log message which shows PG-Strom recognized GPU devices.\nThe example below recognized the Tesla V100(PCIe; 16GB edition) device.  # systemctl start postgresql-10\n# systemctl status -l postgresql-10\n* postgresql-10.service - PostgreSQL 10 database server\n   Loaded: loaded (/usr/lib/systemd/system/postgresql-10.service; disabled; vendor preset: disabled)\n   Active: active (running) since Sat 2018-03-03 15:45:23 JST; 2min 21s ago\n     Docs: https://www.postgresql.org/docs/10/static/\n  Process: 24851 ExecStartPre=/usr/pgsql-10/bin/postgresql-10-check-db-dir ${PGDATA} (code=exited, status=0/SUCCESS)\n Main PID: 24858 (postmaster)\n   CGroup: /system.slice/postgresql-10.service\n           |-24858 /usr/pgsql-10/bin/postmaster -D /var/lib/pgsql/10/data/\n           |-24890 postgres: logger process\n           |-24892 postgres: bgworker: PG-Strom GPU memory keeper\n           |-24896 postgres: checkpointer process\n           |-24897 postgres: writer process\n           |-24898 postgres: wal writer process\n           |-24899 postgres: autovacuum launcher process\n           |-24900 postgres: stats collector process\n           |-24901 postgres: bgworker: PG-Strom ccache-builder2\n           |-24902 postgres: bgworker: PG-Strom ccache-builder1\n           `-24903 postgres: bgworker: logical replication launcher\n\nMar 03 15:45:19 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:19.195 JST [24858] HINT:  Run 'nvidia-cuda-mps-control -d', then start server process. Check 'man nvidia-cuda-mps-control' for more details.\nMar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.509 JST [24858] LOG:  PG-Strom: GPU0 Tesla V100-PCIE-16GB (5120 CUDA cores; 1380MHz, L2 6144kB), RAM 15.78GB (4096bits, 856MHz), CC 7.0\nMar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.510 JST [24858] LOG:  NVRTC - CUDA Runtime Compilation vertion 9.1\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG:  listening on IPv6 address  ::1 , port 5432\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG:  listening on IPv4 address  127.0.0.1 , port 5432\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.442 JST [24858] LOG:  listening on Unix socket  /var/run/postgresql/.s.PGSQL.5432 \nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.492 JST [24858] LOG:  listening on Unix socket  /tmp/.s.PGSQL.5432 \nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] LOG:  redirecting log output to logging collector process\nMar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] HINT:  Future log output will appear in directory  log .\nMar 03 15:45:23 saba.heterodb.com systemd[1]: Started PostgreSQL 10 database server.", 
            "title": "Start PostgreSQL"
        }, 
        {
            "location": "/install/#creation-of-pg-strom-related-objects", 
            "text": "At the last, create database objects related to PG-Strom, like SQL functions.\nThis steps are packaged using EXTENSION feature of PostgreSQL. So, all you needs to run is  CREATE EXTENSION  on the SQL command line.  Please note that this step is needed for each new database.\nIf you want PG-Strom is pre-configured on new database creation, you can create PG-Strom extension on the  template1  database, its configuration will be copied to the new database on  CREATE DATABASE  command.  $ psql postgres -U postgres\npsql (10.2)\nType  help  for help.\n\npostgres=# CREATE EXTENSION pg_strom ;\nCREATE EXTENSION  That's all for the installation.", 
            "title": "Creation of PG-Strom related objects"
        }, 
        {
            "location": "/install/#nvme-strom-module", 
            "text": "This section also introduces NVME-Strom Linux kernel module which is closely cooperating with core features of PG-Strom like SSD-to-GPU Direct SQL Execution, even if it is an independent software module.", 
            "title": "NVME-Strom module"
        }, 
        {
            "location": "/install/#getting-the-module-and-installation", 
            "text": "Like other PG-Strom related modules, NVME-Strom is distributed at the (https://heterodb.github.io/swdc/)[HeteroDB Software Distribution Center] as a free software. In other words, it is not an open source software.  If your system already setup  heterodb-swdc  package,  yum install  command downloads the RPM file and install the  nvme_strom  package.  $ sudo yum install nvme_strom\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.cat.net\n * epel: ftp.iij.ad.jp\n * extras: mirrors.cat.net\n * ius: mirrors.kernel.org\n * updates: mirrors.cat.net\nResolving Dependencies\n--  Running transaction check\n---  Package nvme_strom.x86_64 0:1.3-1.el7 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch            Version            Repository         Size\n================================================================================\nInstalling:\n nvme_strom          x86_64          1.3-1.el7          heterodb          273 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n\nTotal download size: 273 k\nInstalled size: 1.5 M\nIs this ok [y/d/N]: y\nDownloading packages:\nNo Presto metadata available for heterodb\nnvme_strom-1.3-1.el7.x86_64.rpm                            | 273 kB   00:00\nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : nvme_strom-1.3-1.el7.x86_64                                  1/1\n  : snip \n  :\nDKMS: install completed.\n  Verifying  : nvme_strom-1.3-1.el7.x86_64                                  1/1\n\nInstalled:\n  nvme_strom.x86_64 0:1.3-1.el7\n\nComplete!", 
            "title": "Getting the module and installation"
        }, 
        {
            "location": "/install/#license-activation", 
            "text": "License activation is needed to use all the features of NVME-Strom module, provided by HeteroDB,Inc. You can operate the system without license, but features below are restricted.\n- Striping support (md-raid0) at SSD-to-GPU Direct SQL Execution\n- Compression support at in-memory columnar cache  You can obtain a license file, like as a plain text below, from HeteroDB,Inc.  IAwPOAC44m8LPMoV7bMykhxM27LAVrktspcaMHki8pI1fXrxq0KzqPDK4LzAA9n26IRAr/4ymB6QJ3/JxZOfYTVsbWq66vEtTAIuZVmJ/I888zRATj1hoofh1WbIwd3/ix28Cy1v16KCgLrlqPsra6NJScMOOHnuYoWWmWe4ml+n6GVEIb7ChUJvZbEZSO/DiLXosFc0N+MD4JTEU/XsBUP9ufacpbosW/YG2nOib3mpvhkfn7RQy2T5CVQeuGjM9Taj7DN5xipqU/Q0hZaZKA8EsZwsB6b4c7usdmPILyIpuTrWnEbjJ6worOQWHA+nL87xkDL1XYGH6UVc291QPLwk=\n----\nVERSION:1\nSERIAL_NR:HDB-TRIAL\nISSUED_AT:2018-08-16\nEXPIRED_AT:2018-09-15\nNR_GPUS:1\nLICENSEE_ORG:Capybara Kingdom\nLICENSEE_NAME:Herr.Wassershweine\nLICENSEE_MAIL:capybara@examplecom  Copy the license file to  /etc/heterodb.license , then restart PostgreSQL.  The startup log messages of PostgreSQL dumps the license information, and it tells us the license activation is successfully done.  $ pg_ctl restart\n   :\nLOG:  PG-Strom built for PostgreSQL 11\nLOG:  PG-Strom: GPU0 Tesla V100-PCIE-16GB (5120 CUDA cores; 1380MHz, L2 6144kB), RAM 15.78GB (4096bits, 856MHz), CC 7.0\n   :\n   :\nLOG:  HeteroDB License: {  version  : 1,  serial_nr  :  HDB-TRIAL ,  issued_at  :  16-Aug-2018 ,  expired_at  :  15-Sep-2018 ,  nr_gpus  : 1,  licensee_org  :  Capybara Kingdom ,  licensee_name  :  Herr.Wassershweine ,  licensee_mail  :  capybara@examplecom  }\nLOG:  listening on IPv6 address  ::1 , port 5432\nLOG:  listening on IPv4 address  127.0.0.1 , port 5432\nLOG:  listening on Unix socket  /tmp/.s.PGSQL.5432 \n   :", 
            "title": "License activation"
        }, 
        {
            "location": "/install/#kernel-module-parameters", 
            "text": "NVME-Strom Linux kernel module has some parameters.     Parameter  Type  Default  Description      verbose  int  0  Enables detailed debug output    stat_info  int  1  Enables performance statistics    fast_ssd_mode  int  0  Operating mode for fast NVME-SSD     Here is an extra explanation for  fast_ssd_mode  parameter.  When NVME-Strom Linux kernel module get a request for SSD-to-GPU direct data transfer, first of all, it checks whether the required data blocks are caches on page-caches of operating system.\nIf  fast_ssd_mode  is  0 , NVME-Strom once writes back page caches of the required data blocks to the userspace buffer of the caller, then indicates application to invoke normal host-- device data transfer by CUDA API. It is suitable for non-fast NVME-SSDs such as PCIe x4 grade.  On the other hands, SSD-to-GPU direct data transfer may be faster, if you use PCIe x8 grade fast NVME-SSD or use multiple SSDs in striping mode, than normal host-- device data transfer after the buffer copy. If  fast_ssd_mode  is not  0 , NVME-Strom kicks SSD-to-GPU direct data transfer regardless of the page cache state.  However, it shall never kicks SSD-to-GPU direct data transfer if page cache is dirty.", 
            "title": "Kernel module parameters"
        }, 
        {
            "location": "/operations/", 
            "text": "Basic operations\n\n\n\nConfirmation of GPU off-loading\n\n\nYou can use \nEXPLAIN\n command to check whether query is executed on GPU device or not.\nA query is internally split into multiple elements and executed, and PG-Strom is capable to run SCAN, JOIN and GROUP BY in parallel on GPU device. If you can find out GpuScan, GpuJoin or GpuPreAgg was displayed instead of the standard operations by PostgreSQL, it means the query is partially executed on GPU device.\n\n\nBelow is an example of \nEXPLAIN\n command output.\n\n\npostgres=# EXPLAIN SELECT cat,count(*),avg(ax)\n                     FROM t0 NATURAL JOIN t1 NATURAL JOIN t2\n                    GROUP BY cat;\n                                  QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=989186.82..989190.94 rows=27 width=20)\n   Group Key: t0.cat\n   -\n  Sort  (cost=989186.82..989187.29 rows=189 width=44)\n         Sort Key: t0.cat\n         -\n  Custom Scan (GpuPreAgg)  (cost=989175.89..989179.67 rows=189 width=44)\n               Reduction: Local\n               GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n               Combined GpuJoin: enabled\n               -\n  Custom Scan (GpuJoin) on t0  (cost=14744.40..875804.46 rows=99996736 width=12)\n                     GPU Projection: t0.cat, t1.ax\n                     Outer Scan: t0  (cost=0.00..1833360.36 rows=99996736 width=12)\n                     Depth 1: GpuHashJoin  (nrows 99996736...99996736)\n                              HashKeys: t0.aid\n                              JoinQuals: (t0.aid = t1.aid)\n                              KDS-Hash (size: 10.39MB)\n                     Depth 2: GpuHashJoin  (nrows 99996736...99996736)\n                              HashKeys: t0.bid\n                              JoinQuals: (t0.bid = t2.bid)\n                              KDS-Hash (size: 10.78MB)\n                     -\n  Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n                     -\n  Seq Scan on t2  (cost=0.00..1935.00 rows=100000 width=4)\n(21 rows)\n\n\n\n\nYou can notice some unusual query execution plans.\nGpuJoin and GpuPreAgg are implemented on the CustomScan mechanism. In this example, GpuJoin runs JOIN operation on \nt0\n, \nt1\n and \nt1\n, then GpuPreAgg which receives the result of GpuJoin runs GROUP BY operation by the \ncat\n column on GPU device.\n\n\nPG-Strom interacts with the query optimizer during PostgreSQL is building a query execution plan, and it offers alternative query execution plan with estimated cost for PostgreSQL's optimizer, if any of SCAN, JOIN, or GROUP BY are executable on GPU device.\nThis estimated cost is better than other query execution plans that run on CPU, it chooses the alternative execution plan that shall run on GPU device.\n\n\nFor GPU execution, it requires operators, functions and data types in use must be supported by PG-Strom.\nIt supports numeric types like \nint\n or \nfloat\n, date and time types like \ndate\n or \ntimestamp\n, variable length string like \ntext\n and so on. It also supports arithmetic operations, comparison operators and many built-in operators.\nSee \nReferences\n for the detailed list.\n\n\nCPU+GPU Hybrid Parallel\n\n\nPG-Strom also supports PostgreSQL's CPU parallel execution.\n\n\nIn the CPU parallel execution mode, Gather node launches several background worker processes, then it gathers the result of \"partial\" execution by individual background workers.\nCustomScan execution plan provided by PG-Strom, like GpuJoin or GpuPreAgg, support execution at the background workers. They process their partial task using GPU individually. A CPU core usually needs much more time to set up buffer to supply data for GPU than execution of SQL workloads on GPU, so hybrid usage of CPU and GPU parallel can expect higher performance.\nOn the other hands, each process creates CUDA context that is required to communicate GPU and consumes a certain amount of GPU resources, so higher parallelism on CPU-side is not always better.\n\n\nLook at the query execution plan below.\nExecution plan tree under the Gather is executable on background worker process. It scans \nt0\n table which has 100million rows using four background worker processes and the coordinator process, in other words, 20million rows are handled per process by GpuJoin and GpuPreAgg, then its results are merged at Gather node.\n\n\n# EXPLAIN SELECT cat,count(*),avg(ax)\n            FROM t0 NATURAL JOIN t1\n           GROUP by cat;\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=955705.47..955720.93 rows=27 width=20)\n   Group Key: t0.cat\n   -\n  Sort  (cost=955705.47..955707.36 rows=756 width=44)\n         Sort Key: t0.cat\n         -\n  Gather  (cost=955589.95..955669.33 rows=756 width=44)\n               Workers Planned: 4\n               -\n  Parallel Custom Scan (GpuPreAgg)  (cost=954589.95..954593.73 rows=189 width=44)\n                     Reduction: Local\n                     GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n                     Combined GpuJoin: enabled\n                     -\n  Parallel Custom Scan (GpuJoin) on t0  (cost=27682.82..841218.52 rows=99996736 width=12)\n                           GPU Projection: t0.cat, t1.ax\n                           Outer Scan: t0  (cost=0.00..1083384.84 rows=24999184 width=8)\n                           Depth 1: GpuHashJoin  (nrows 24999184...99996736)\n                                    HashKeys: t0.aid\n                                    JoinQuals: (t0.aid = t1.aid)\n                                    KDS-Hash (size: 10.39MB)\n                           -\n  Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n(18 rows)\n\n\n\n\nPullup underlying plans\n\n\nPG-Strom can run SCAN, JOIN and GROUP BY workloads on GPU, however, it does not work with best performance if these custom execution plan simply replace the standard operations at PostgreSQL.\nAn example of problematic scenario is that SCAN once writes back its result data set to the host buffer then send the same data into GPU again to execute JOIN. Once again, JOIN results are written back and send to GPU to execute GROUP BY. It causes data ping-pong between CPU and GPU.\n\n\nTo avoid such inefficient jobs, PG-Strom has a special mode which pulls up its sub-plan to execute a bunch of jobs in a single GPU kernel invocation. Combination of the operations blow can cause pull-up of sub-plans.\n\n\n\n\nSCAN + JOIN\n\n\nSCAN + GROUP BY\n\n\nSCAN + JOIN + GROUP BY\n\n\n\n\n\n\nThe execution plan example below never pulls up the sub-plans.\n\n\nGpuJoin receives the result of GpuScan, then its results are passed to GpuPreAgg to generate the final results.\n\n\n# EXPLAIN SELECT cat,count(*),avg(ax)\n            FROM t0 NATURAL JOIN t1\n           WHERE aid \n bid\n           GROUP BY cat;\n                              QUERY PLAN\n\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=1239991.03..1239995.15 rows=27 width=20)\n   Group Key: t0.cat\n   -\n  Sort  (cost=1239991.03..1239991.50 rows=189 width=44)\n         Sort Key: t0.cat\n         -\n  Custom Scan (GpuPreAgg)  (cost=1239980.10..1239983.88 rows=189 width=44)\n               Reduction: Local\n               GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n               -\n  Custom Scan (GpuJoin)  (cost=50776.43..1199522.96 rows=33332245 width=12)\n                     GPU Projection: t0.cat, t1.ax\n                     Depth 1: GpuHashJoin  (nrows 33332245...33332245)\n                              HashKeys: t0.aid\n                              JoinQuals: (t0.aid = t1.aid)\n                              KDS-Hash (size: 10.39MB)\n                     -\n  Custom Scan (GpuScan) on t0  (cost=12634.49..1187710.85 rows=33332245 width=8)\n                           GPU Projection: cat, aid\n                           GPU Filter: (aid \n bid)\n                     -\n  Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n(18 rows)\n\n\n\n\nThis example causes data ping-pong between GPU and host buffers for each execution stage, so not efficient and less performance.\n\n\nOn the other hands, the query execution plan below pulls up sub-plans.\n\n\n# EXPLAIN ANALYZE SELECT cat,count(*),avg(ax)\n                    FROM t0 NATURAL JOIN t1\n                   WHERE aid \n bid\n                   GROUP BY cat;\n                              QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=903669.50..903673.62 rows=27 width=20)\n                 (actual time=7761.630..7761.644 rows=27 loops=1)\n   Group Key: t0.cat\n   -\n  Sort  (cost=903669.50..903669.97 rows=189 width=44)\n             (actual time=7761.621..7761.626 rows=27 loops=1)\n         Sort Key: t0.cat\n         Sort Method: quicksort  Memory: 28kB\n         -\n  Custom Scan (GpuPreAgg)  (cost=903658.57..903662.35 rows=189 width=44)\n                                      (actual time=7761.531..7761.540 rows=27 loops=1)\n               Reduction: Local\n               GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n               Combined GpuJoin: enabled\n               -\n  Custom Scan (GpuJoin) on t0  (cost=12483.41..863201.43 rows=33332245 width=12)\n                                                (never executed)\n                     GPU Projection: t0.cat, t1.ax\n                     Outer Scan: t0  (cost=12634.49..1187710.85 rows=33332245 width=8)\n                                     (actual time=59.623..5557.052 rows=100000000 loops=1)\n                     Outer Scan Filter: (aid \n bid)\n                     Rows Removed by Outer Scan Filter: 50002874\n                     Depth 1: GpuHashJoin  (plan nrows: 33332245...33332245, actual nrows: 49997126...49997126)\n                              HashKeys: t0.aid\n                              JoinQuals: (t0.aid = t1.aid)\n                              KDS-Hash (size plan: 10.39MB, exec: 64.00MB)\n                     -\n  Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n                                         (actual time=0.013..15.303 rows=100000 loops=1)\n Planning time: 0.506 ms\n Execution time: 8495.391 ms\n(21 rows)\n\n\n\n\nYou may notice that SCAN on the table \nt0\n is embedded into GpuJoin, and GpuScan gets vanished.\nIt means GpuJoin pulls up the underlying GpuScan, then combined GPU kernel function is also responsible for evaluation of the supplied WHERE-clause.\n\n\nIn addition, here is a strange output in \nEXPLAIN ANALYZE\n result - it displays \n(never executed)\n for GpuJoin.\nIt means GpuJoin is never executed during the query execution, and it is right. GpuPreAgg pulls up the underlying GpuJoin, then its combined GPU kernel function runs JOIN and GROUP BY.\n\n\nThe \npg_strom.pullup_outer_scan\n parameter controls whether SCAN is pulled up, and the \npg_strom.pullup_outer_join\n parameter also controls whether JOIN is pulled up.\nBoth parameters are configured to \non\n. Usually, no need to disable them, however, you can use the parameters to identify the problems on system troubles.", 
            "title": "Basic Operations"
        }, 
        {
            "location": "/operations/#confirmation-of-gpu-off-loading", 
            "text": "You can use  EXPLAIN  command to check whether query is executed on GPU device or not.\nA query is internally split into multiple elements and executed, and PG-Strom is capable to run SCAN, JOIN and GROUP BY in parallel on GPU device. If you can find out GpuScan, GpuJoin or GpuPreAgg was displayed instead of the standard operations by PostgreSQL, it means the query is partially executed on GPU device.  Below is an example of  EXPLAIN  command output.  postgres=# EXPLAIN SELECT cat,count(*),avg(ax)\n                     FROM t0 NATURAL JOIN t1 NATURAL JOIN t2\n                    GROUP BY cat;\n                                  QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=989186.82..989190.94 rows=27 width=20)\n   Group Key: t0.cat\n   -   Sort  (cost=989186.82..989187.29 rows=189 width=44)\n         Sort Key: t0.cat\n         -   Custom Scan (GpuPreAgg)  (cost=989175.89..989179.67 rows=189 width=44)\n               Reduction: Local\n               GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n               Combined GpuJoin: enabled\n               -   Custom Scan (GpuJoin) on t0  (cost=14744.40..875804.46 rows=99996736 width=12)\n                     GPU Projection: t0.cat, t1.ax\n                     Outer Scan: t0  (cost=0.00..1833360.36 rows=99996736 width=12)\n                     Depth 1: GpuHashJoin  (nrows 99996736...99996736)\n                              HashKeys: t0.aid\n                              JoinQuals: (t0.aid = t1.aid)\n                              KDS-Hash (size: 10.39MB)\n                     Depth 2: GpuHashJoin  (nrows 99996736...99996736)\n                              HashKeys: t0.bid\n                              JoinQuals: (t0.bid = t2.bid)\n                              KDS-Hash (size: 10.78MB)\n                     -   Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n                     -   Seq Scan on t2  (cost=0.00..1935.00 rows=100000 width=4)\n(21 rows)  You can notice some unusual query execution plans.\nGpuJoin and GpuPreAgg are implemented on the CustomScan mechanism. In this example, GpuJoin runs JOIN operation on  t0 ,  t1  and  t1 , then GpuPreAgg which receives the result of GpuJoin runs GROUP BY operation by the  cat  column on GPU device.  PG-Strom interacts with the query optimizer during PostgreSQL is building a query execution plan, and it offers alternative query execution plan with estimated cost for PostgreSQL's optimizer, if any of SCAN, JOIN, or GROUP BY are executable on GPU device.\nThis estimated cost is better than other query execution plans that run on CPU, it chooses the alternative execution plan that shall run on GPU device.  For GPU execution, it requires operators, functions and data types in use must be supported by PG-Strom.\nIt supports numeric types like  int  or  float , date and time types like  date  or  timestamp , variable length string like  text  and so on. It also supports arithmetic operations, comparison operators and many built-in operators.\nSee  References  for the detailed list.", 
            "title": "Confirmation of GPU off-loading"
        }, 
        {
            "location": "/operations/#cpugpu-hybrid-parallel", 
            "text": "PG-Strom also supports PostgreSQL's CPU parallel execution.  In the CPU parallel execution mode, Gather node launches several background worker processes, then it gathers the result of \"partial\" execution by individual background workers.\nCustomScan execution plan provided by PG-Strom, like GpuJoin or GpuPreAgg, support execution at the background workers. They process their partial task using GPU individually. A CPU core usually needs much more time to set up buffer to supply data for GPU than execution of SQL workloads on GPU, so hybrid usage of CPU and GPU parallel can expect higher performance.\nOn the other hands, each process creates CUDA context that is required to communicate GPU and consumes a certain amount of GPU resources, so higher parallelism on CPU-side is not always better.  Look at the query execution plan below.\nExecution plan tree under the Gather is executable on background worker process. It scans  t0  table which has 100million rows using four background worker processes and the coordinator process, in other words, 20million rows are handled per process by GpuJoin and GpuPreAgg, then its results are merged at Gather node.  # EXPLAIN SELECT cat,count(*),avg(ax)\n            FROM t0 NATURAL JOIN t1\n           GROUP by cat;\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=955705.47..955720.93 rows=27 width=20)\n   Group Key: t0.cat\n   -   Sort  (cost=955705.47..955707.36 rows=756 width=44)\n         Sort Key: t0.cat\n         -   Gather  (cost=955589.95..955669.33 rows=756 width=44)\n               Workers Planned: 4\n               -   Parallel Custom Scan (GpuPreAgg)  (cost=954589.95..954593.73 rows=189 width=44)\n                     Reduction: Local\n                     GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n                     Combined GpuJoin: enabled\n                     -   Parallel Custom Scan (GpuJoin) on t0  (cost=27682.82..841218.52 rows=99996736 width=12)\n                           GPU Projection: t0.cat, t1.ax\n                           Outer Scan: t0  (cost=0.00..1083384.84 rows=24999184 width=8)\n                           Depth 1: GpuHashJoin  (nrows 24999184...99996736)\n                                    HashKeys: t0.aid\n                                    JoinQuals: (t0.aid = t1.aid)\n                                    KDS-Hash (size: 10.39MB)\n                           -   Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n(18 rows)", 
            "title": "CPU+GPU Hybrid Parallel"
        }, 
        {
            "location": "/operations/#pullup-underlying-plans", 
            "text": "PG-Strom can run SCAN, JOIN and GROUP BY workloads on GPU, however, it does not work with best performance if these custom execution plan simply replace the standard operations at PostgreSQL.\nAn example of problematic scenario is that SCAN once writes back its result data set to the host buffer then send the same data into GPU again to execute JOIN. Once again, JOIN results are written back and send to GPU to execute GROUP BY. It causes data ping-pong between CPU and GPU.  To avoid such inefficient jobs, PG-Strom has a special mode which pulls up its sub-plan to execute a bunch of jobs in a single GPU kernel invocation. Combination of the operations blow can cause pull-up of sub-plans.   SCAN + JOIN  SCAN + GROUP BY  SCAN + JOIN + GROUP BY    The execution plan example below never pulls up the sub-plans.  GpuJoin receives the result of GpuScan, then its results are passed to GpuPreAgg to generate the final results.  # EXPLAIN SELECT cat,count(*),avg(ax)\n            FROM t0 NATURAL JOIN t1\n           WHERE aid   bid\n           GROUP BY cat;\n                              QUERY PLAN\n\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=1239991.03..1239995.15 rows=27 width=20)\n   Group Key: t0.cat\n   -   Sort  (cost=1239991.03..1239991.50 rows=189 width=44)\n         Sort Key: t0.cat\n         -   Custom Scan (GpuPreAgg)  (cost=1239980.10..1239983.88 rows=189 width=44)\n               Reduction: Local\n               GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n               -   Custom Scan (GpuJoin)  (cost=50776.43..1199522.96 rows=33332245 width=12)\n                     GPU Projection: t0.cat, t1.ax\n                     Depth 1: GpuHashJoin  (nrows 33332245...33332245)\n                              HashKeys: t0.aid\n                              JoinQuals: (t0.aid = t1.aid)\n                              KDS-Hash (size: 10.39MB)\n                     -   Custom Scan (GpuScan) on t0  (cost=12634.49..1187710.85 rows=33332245 width=8)\n                           GPU Projection: cat, aid\n                           GPU Filter: (aid   bid)\n                     -   Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n(18 rows)  This example causes data ping-pong between GPU and host buffers for each execution stage, so not efficient and less performance.  On the other hands, the query execution plan below pulls up sub-plans.  # EXPLAIN ANALYZE SELECT cat,count(*),avg(ax)\n                    FROM t0 NATURAL JOIN t1\n                   WHERE aid   bid\n                   GROUP BY cat;\n                              QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=903669.50..903673.62 rows=27 width=20)\n                 (actual time=7761.630..7761.644 rows=27 loops=1)\n   Group Key: t0.cat\n   -   Sort  (cost=903669.50..903669.97 rows=189 width=44)\n             (actual time=7761.621..7761.626 rows=27 loops=1)\n         Sort Key: t0.cat\n         Sort Method: quicksort  Memory: 28kB\n         -   Custom Scan (GpuPreAgg)  (cost=903658.57..903662.35 rows=189 width=44)\n                                      (actual time=7761.531..7761.540 rows=27 loops=1)\n               Reduction: Local\n               GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax)\n               Combined GpuJoin: enabled\n               -   Custom Scan (GpuJoin) on t0  (cost=12483.41..863201.43 rows=33332245 width=12)\n                                                (never executed)\n                     GPU Projection: t0.cat, t1.ax\n                     Outer Scan: t0  (cost=12634.49..1187710.85 rows=33332245 width=8)\n                                     (actual time=59.623..5557.052 rows=100000000 loops=1)\n                     Outer Scan Filter: (aid   bid)\n                     Rows Removed by Outer Scan Filter: 50002874\n                     Depth 1: GpuHashJoin  (plan nrows: 33332245...33332245, actual nrows: 49997126...49997126)\n                              HashKeys: t0.aid\n                              JoinQuals: (t0.aid = t1.aid)\n                              KDS-Hash (size plan: 10.39MB, exec: 64.00MB)\n                     -   Seq Scan on t1  (cost=0.00..1972.85 rows=103785 width=12)\n                                         (actual time=0.013..15.303 rows=100000 loops=1)\n Planning time: 0.506 ms\n Execution time: 8495.391 ms\n(21 rows)  You may notice that SCAN on the table  t0  is embedded into GpuJoin, and GpuScan gets vanished.\nIt means GpuJoin pulls up the underlying GpuScan, then combined GPU kernel function is also responsible for evaluation of the supplied WHERE-clause.  In addition, here is a strange output in  EXPLAIN ANALYZE  result - it displays  (never executed)  for GpuJoin.\nIt means GpuJoin is never executed during the query execution, and it is right. GpuPreAgg pulls up the underlying GpuJoin, then its combined GPU kernel function runs JOIN and GROUP BY.  The  pg_strom.pullup_outer_scan  parameter controls whether SCAN is pulled up, and the  pg_strom.pullup_outer_join  parameter also controls whether JOIN is pulled up.\nBoth parameters are configured to  on . Usually, no need to disable them, however, you can use the parameters to identify the problems on system troubles.", 
            "title": "Pullup underlying plans"
        }, 
        {
            "location": "/sys_admin/", 
            "text": "System Administration\n\n\n\n\n\nUsage of MPS daemon\n\n\nIn case when multi-process application like PostgreSQL uses GPU device, it is a well known solution to use MPS (Multi-Process Service) to reduce context switch on GPU side and resource consumption for device management.\n\n\nhttps://docs.nvidia.com/deploy/mps/index.html\n\n\nIt is also recommended for PG-Strom to apply MPS, however, you need to pay attention for several limitations below.\n\n\nOne MPS daemon can provide its service for up to 48 clients (16 clients if Pascal or older).\nSo, DB administration must ensure number of PostgreSQL processes using GPU (including background workers in CPU parallelism) is less than 48 (or 16 if Pascal).\n\n\nMPS does not support dynamic parallelism, and load GPU programs using the feature.\nGPU programs automatically generated from SQL will never use dynamic parallelism, however, PL/CUDA user defined function may use dynamic parallelism if it links CUDA device runtime to invoke sub-kernels.\nSo, we don't use MPS for invocation of PL/CUDA functions.\n\n\nMPS document recommends to set compute-mode \nEXCLUSIVE_PROCESS\n, however, PG-Strom requires \nDEFAULT\n mode.\nSeveral operations, including PL/CUDA above, call CUDA APIs with MPS disabled explicitly, so other processes than MPS daemon must be able to use GPU devices.\n\n\nThe following commands start MPS daemon. Replace \nUID\n with user-id of PostgreSQL process.\n\n\n$ nvidia-cuda-mps-control -d\n$ echo start_server -uid \nUID\n | nvidia-cuda-mps-control\n\n\n\n\nnvidia-smi\n command shows MPS daemon is using GPU device.\n\n\n$ nvidia-smi\nSat Nov  3 12:22:26 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   45C    P0    38W / 250W |     40MiB / 16130MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     11080      C   nvidia-cuda-mps-server                        29MiB |\n+-----------------------------------------------------------------------------+\n\n\n\n\nKnowledge base\n\n\nWe publish several articles, just called \"notes\", on the project wiki-site of PG-Strom.\n\n\nhttps://github.com/heterodb/pg-strom/wiki", 
            "title": "System Administration"
        }, 
        {
            "location": "/sys_admin/#system-administration", 
            "text": "", 
            "title": "System Administration"
        }, 
        {
            "location": "/sys_admin/#usage-of-mps-daemon", 
            "text": "In case when multi-process application like PostgreSQL uses GPU device, it is a well known solution to use MPS (Multi-Process Service) to reduce context switch on GPU side and resource consumption for device management.  https://docs.nvidia.com/deploy/mps/index.html  It is also recommended for PG-Strom to apply MPS, however, you need to pay attention for several limitations below.  One MPS daemon can provide its service for up to 48 clients (16 clients if Pascal or older).\nSo, DB administration must ensure number of PostgreSQL processes using GPU (including background workers in CPU parallelism) is less than 48 (or 16 if Pascal).  MPS does not support dynamic parallelism, and load GPU programs using the feature.\nGPU programs automatically generated from SQL will never use dynamic parallelism, however, PL/CUDA user defined function may use dynamic parallelism if it links CUDA device runtime to invoke sub-kernels.\nSo, we don't use MPS for invocation of PL/CUDA functions.  MPS document recommends to set compute-mode  EXCLUSIVE_PROCESS , however, PG-Strom requires  DEFAULT  mode.\nSeveral operations, including PL/CUDA above, call CUDA APIs with MPS disabled explicitly, so other processes than MPS daemon must be able to use GPU devices.  The following commands start MPS daemon. Replace  UID  with user-id of PostgreSQL process.  $ nvidia-cuda-mps-control -d\n$ echo start_server -uid  UID  | nvidia-cuda-mps-control  nvidia-smi  command shows MPS daemon is using GPU device.  $ nvidia-smi\nSat Nov  3 12:22:26 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla V100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   45C    P0    38W / 250W |     40MiB / 16130MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     11080      C   nvidia-cuda-mps-server                        29MiB |\n+-----------------------------------------------------------------------------+", 
            "title": "Usage of MPS daemon"
        }, 
        {
            "location": "/sys_admin/#knowledge-base", 
            "text": "We publish several articles, just called \"notes\", on the project wiki-site of PG-Strom.  https://github.com/heterodb/pg-strom/wiki", 
            "title": "Knowledge base"
        }, 
        {
            "location": "/brin/", 
            "text": "Index Support\n\n\n\nOverview\n\n\nPostgreSQL supports several index strategies. The default is B-tree that can rapidly fetch records with a particular value. Elsewhere, it also supports Hash, BRIN, GiST, GIN and others that have own characteristics for each.\nPG-Strom supports only BRIN-index right now.\n\n\nBRIN-index works efficiently on the dataset we can expect physically neighbor records have similar key values, like timestamp values of time-series data.\nIt allows to skip blocks-range if any records in the range obviously don't match to the scan qualifiers, then, also enables to reduce the amount of I/O due to full table scan.\n\n\nPG-Strom also utilizes the feature of BRIN-index, to skip obviously unnecessary blocks from the ones to be loaded to GPU.\n\n\n\n\nConfiguration\n\n\nNo special configurations are needed to use BRIN-index.\n\n\nPG-Strom automatically applies BRIN-index based scan if BRIN-index is configured on the referenced columns and scan qualifiers are suitable to the index.\n\n\nAlso see the \nPostgreSQL Documentation\n for the BRIN-index feature.\n\n\nBy the GUC parameters below, PG-Strom enables/disables usage of BRIN-index. It usually don't need to change from the default configuration, except for debugging or trouble shooting.\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.enable_brin\n\n\nbool\n\n\non\n\n\nenables/disables usage of BRIN-index\n\n\n\n\n\n\n\n\nOperations\n\n\nBy \nEXPLAIN\n, we can check whether BRIN-index is in use.\n\n\npostgres=# EXPLAIN ANALYZE\n           SELECT * FROM dt\n            WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31'\n              AND cat LIKE '%aaa%';\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n Custom Scan (GpuScan) on dt  (cost=94810.93..176275.00 rows=169992 width=44)\n                              (actual time=1777.819..1901.537 rows=175277 loops=1)\n   GPU Filter: ((ymd \n= '2018-01-01'::date) AND (ymd \n= '2018-12-31'::date) AND (cat ~~ '%aaa%'::text))\n   Rows Removed by GPU Filter: 4385491\n   BRIN cond: ((ymd \n= '2018-01-01'::date) AND (ymd \n= '2018-12-31'::date))\n   BRIN skipped: 424704\n Planning time: 0.529 ms\n Execution time: 2323.063 ms\n(7 rows)\n\n\n\n\nIn the example above, BRIN-index is configured on the \nymd\n column. \nBRIN cond\n shows the qualifier of BRIN-index for concentration. \nBRIN skipped\n shows the number of skipped blocks actually.\n\n\nIn this case, 424704 blocks are skipped, then, it filters out 4385491 rows in the loaded blocks by the scan qualifiers.\n\n\nGpuJoin and GpuPreAgg often pulls up its underlying table scan and runs the scan by itself, to reduce inefficient data transfer. In this case, it also uses the BRIN-index to concentrate the scan.\n\n\nThe example below shows a usage of BRIN-index in a query which includes GROUP BY.\n\n\npostgres=# EXPLAIN ANALYZE\n           SELECT cat,count(*)\n             FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31'\n         GROUP BY cat;\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=6149.78..6151.86 rows=26 width=12)\n                 (actual time=427.482..427.499 rows=26 loops=1)\n   Group Key: cat\n   -\n  Sort  (cost=6149.78..6150.24 rows=182 width=12)\n             (actual time=427.465..427.467 rows=26 loops=1)\n         Sort Key: cat\n         Sort Method: quicksort  Memory: 26kB\n         -\n  Custom Scan (GpuPreAgg) on dt  (cost=6140.68..6142.95 rows=182 width=12)\n                                            (actual time=427.331..427.339 rows=26 loops=1)\n               Reduction: Local\n               Outer Scan: dt  (cost=4000.00..4011.99 rows=4541187 width=4)\n                               (actual time=78.573..415.961 rows=4560768 loops=1)\n               Outer Scan Filter: ((ymd \n= '2018-01-01'::date) AND (ymd \n= '2018-12-31'::date))\n               Rows Removed by Outer Scan Filter: 15564\n               BRIN cond: ((ymd \n= '2018-01-01'::date) AND (ymd \n= '2018-12-31'::date))\n               BRIN skipped: 424704\n Planning time: 30.992 ms\n Execution time: 818.994 ms\n(14 rows)", 
            "title": "Index Support"
        }, 
        {
            "location": "/brin/#overview", 
            "text": "PostgreSQL supports several index strategies. The default is B-tree that can rapidly fetch records with a particular value. Elsewhere, it also supports Hash, BRIN, GiST, GIN and others that have own characteristics for each.\nPG-Strom supports only BRIN-index right now.  BRIN-index works efficiently on the dataset we can expect physically neighbor records have similar key values, like timestamp values of time-series data.\nIt allows to skip blocks-range if any records in the range obviously don't match to the scan qualifiers, then, also enables to reduce the amount of I/O due to full table scan.  PG-Strom also utilizes the feature of BRIN-index, to skip obviously unnecessary blocks from the ones to be loaded to GPU.", 
            "title": "Overview"
        }, 
        {
            "location": "/brin/#configuration", 
            "text": "No special configurations are needed to use BRIN-index.  PG-Strom automatically applies BRIN-index based scan if BRIN-index is configured on the referenced columns and scan qualifiers are suitable to the index.  Also see the  PostgreSQL Documentation  for the BRIN-index feature.  By the GUC parameters below, PG-Strom enables/disables usage of BRIN-index. It usually don't need to change from the default configuration, except for debugging or trouble shooting.     Parameter  Type  Default  Description      pg_strom.enable_brin  bool  on  enables/disables usage of BRIN-index", 
            "title": "Configuration"
        }, 
        {
            "location": "/brin/#operations", 
            "text": "By  EXPLAIN , we can check whether BRIN-index is in use.  postgres=# EXPLAIN ANALYZE\n           SELECT * FROM dt\n            WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31'\n              AND cat LIKE '%aaa%';\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n Custom Scan (GpuScan) on dt  (cost=94810.93..176275.00 rows=169992 width=44)\n                              (actual time=1777.819..1901.537 rows=175277 loops=1)\n   GPU Filter: ((ymd  = '2018-01-01'::date) AND (ymd  = '2018-12-31'::date) AND (cat ~~ '%aaa%'::text))\n   Rows Removed by GPU Filter: 4385491\n   BRIN cond: ((ymd  = '2018-01-01'::date) AND (ymd  = '2018-12-31'::date))\n   BRIN skipped: 424704\n Planning time: 0.529 ms\n Execution time: 2323.063 ms\n(7 rows)  In the example above, BRIN-index is configured on the  ymd  column.  BRIN cond  shows the qualifier of BRIN-index for concentration.  BRIN skipped  shows the number of skipped blocks actually.  In this case, 424704 blocks are skipped, then, it filters out 4385491 rows in the loaded blocks by the scan qualifiers.  GpuJoin and GpuPreAgg often pulls up its underlying table scan and runs the scan by itself, to reduce inefficient data transfer. In this case, it also uses the BRIN-index to concentrate the scan.  The example below shows a usage of BRIN-index in a query which includes GROUP BY.  postgres=# EXPLAIN ANALYZE\n           SELECT cat,count(*)\n             FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31'\n         GROUP BY cat;\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n GroupAggregate  (cost=6149.78..6151.86 rows=26 width=12)\n                 (actual time=427.482..427.499 rows=26 loops=1)\n   Group Key: cat\n   -   Sort  (cost=6149.78..6150.24 rows=182 width=12)\n             (actual time=427.465..427.467 rows=26 loops=1)\n         Sort Key: cat\n         Sort Method: quicksort  Memory: 26kB\n         -   Custom Scan (GpuPreAgg) on dt  (cost=6140.68..6142.95 rows=182 width=12)\n                                            (actual time=427.331..427.339 rows=26 loops=1)\n               Reduction: Local\n               Outer Scan: dt  (cost=4000.00..4011.99 rows=4541187 width=4)\n                               (actual time=78.573..415.961 rows=4560768 loops=1)\n               Outer Scan Filter: ((ymd  = '2018-01-01'::date) AND (ymd  = '2018-12-31'::date))\n               Rows Removed by Outer Scan Filter: 15564\n               BRIN cond: ((ymd  = '2018-01-01'::date) AND (ymd  = '2018-12-31'::date))\n               BRIN skipped: 424704\n Planning time: 30.992 ms\n Execution time: 818.994 ms\n(14 rows)", 
            "title": "Operations"
        }, 
        {
            "location": "/partition/", 
            "text": "Partitioning\n\n\n\nThis chapter introduces the way to use PG-Strom and the partitioning feature of PostgreSQL.\nNote that this chapter is only valid when PG-Strom works on PostgreSQL v10 or later.\n\n\nAlso see \nPostgreSQL Document: Table Partitioning\n for more details of the partitioning feature of PostgreSQL.\n\n\nBrief overview\n\n\nPostgreSQL v10 newly support table partitioning.\nThis mechanism splits one logically large table into physically small pieces. It is valuable because it can skip partitioned child tables which is obviously unnecessary to scan from the search qualification, and it can offer broader I/O bandwidth by physically distributed storage and so on.\n\n\nPostgreSQL v10 supports two kinds of them: range-partitioning and list-partitioning. Then, PostgreSQL v11 newly supports hash-partitioning.\n\n\nThe diagram below shows a range-partitioning configuration with \ndate\n-type key values.\nA record which has \n2018-05-30\n as key is distributed to the partition child table \ntbl_2018\n, in the same way, a record which has \n2014-03-21\n is distributed to the partition child table \ntbl_2014\n, and so on.\n\n\nIn case when scan qualifier \nWHERE ymd \n '2016-07-01'::date\n is added on scan of the partitioned table for example, it is obvious that \ntbl_2014\n and \ntbl_2015\n contains no records to match, therefore, PostgreSQL' optimizer constructs query execution plan which runs on only \ntbl_2016\n, \ntbl_2017\n and \ntbl_2018\n then merges their results by \nAppend\n node. It shall perform as if records are read from one logical table.\n\n\n\n\nWhen PG-Strom is used with table partitioning of PostgreSQL together, its optimizer may choose \nGpuScan\n to scan the individual partition child tables to be scanned, in the result of cost estimation. In this case, \nAppend\n node merges the results of \nGpuScan\n.\n\n\nOn the other hands, if query runs JOIN or GROUP BY, which can be accelerated by PG-Strom, next to the scan on partitioned table, it needs consideration from the standpoint of performance optimization.\n\n\nFor example, in case when query scans non-partitioned table then runs JOIN with other tables and GROUP BY, under some conditions, it can handle step-step data exchange on GPU device memory. It is an optimal workload for PG-Strom due to minimized data exchange between GPU and CPU.\n\n\nIn case when query runs corresponding workload on the partitioned table, it is problematic that \nAppend\n node is injected into between the child tables scan and JOIN/GROUP BY. Under the query execution plan, the result of GpuScan must be written back to the host system, then \nAppend\n merges them and send back the data to GPU to run the following GpuJoin and GpuPreAgg. It is never efficient query execution.\n\n\n\n\nThe example below shows a query execution plan to the query which includes JOIN and GROUP BY towards the partitioned table \npt\n by the key field \nymd\n of \ndate\n type; per year distribution.\n\n\nDue to the scan qualification, it omits scan on the partition child tables for 2016 or prior, in addition, a combined JOIN and GROUP BY on the \npt_2017\n, \npt_2018\n and \npt_2019\n shall be executed prior to the \nAppend\n.\n\n\n# EXPLAIN SELECT cat,count(*),avg(ax)\n            FROM pt NATURAL JOIN t1\n           WHERE ymd \n '2017-01-01'::date\n           GROUP BY cat;\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n HashAggregate  (cost=196410.07..196412.57 rows=200 width=48)\n   Group Key: pt_2017.cat\n   -\n  Gather  (cost=66085.69..196389.07 rows=1200 width=72)\n         Workers Planned: 2\n         -\n  Parallel Append  (cost=65085.69..195269.07 rows=600 width=72)\n               -\n  Parallel Custom Scan (GpuPreAgg)  (cost=65085.69..65089.69 rows=200 width=72)\n                     Reduction: Local\n                     Combined GpuJoin: enabled\n                     -\n  Parallel Custom Scan (GpuJoin) on pt_2017  (cost=32296.64..74474.20 rows=1050772 width=40)\n                           Outer Scan: pt_2017  (cost=28540.80..66891.11 rows=1050772 width=36)\n                           Outer Scan Filter: (ymd \n '2017-01-01'::date)\n                           Depth 1: GpuHashJoin  (nrows 1050772...2521854)\n                                    HashKeys: pt_2017.aid\n                                    JoinQuals: (pt_2017.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n               -\n  Parallel Custom Scan (GpuPreAgg)  (cost=65078.35..65082.35 rows=200 width=72)\n                     Reduction: Local\n                     Combined GpuJoin: enabled\n                     -\n  Parallel Custom Scan (GpuJoin) on pt_2018  (cost=32296.65..74465.75 rows=1050649 width=40)\n                           Outer Scan: pt_2018  (cost=28540.81..66883.43 rows=1050649 width=36)\n                           Outer Scan Filter: (ymd \n '2017-01-01'::date)\n                           Depth 1: GpuHashJoin  (nrows 1050649...2521557)\n                                    HashKeys: pt_2018.aid\n                                    JoinQuals: (pt_2018.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n               -\n  Parallel Custom Scan (GpuPreAgg)  (cost=65093.03..65097.03 rows=200 width=72)\n                     Reduction: Local\n                     Combined GpuJoin: enabled\n                     -\n  Parallel Custom Scan (GpuJoin) on pt_2019  (cost=32296.65..74482.64 rows=1050896 width=40)\n                           Outer Scan: pt_2019  (cost=28540.80..66898.79 rows=1050896 width=36)\n                           Outer Scan Filter: (ymd \n '2017-01-01'::date)\n                           Depth 1: GpuHashJoin  (nrows 1050896...2522151)\n                                    HashKeys: pt_2019.aid\n                                    JoinQuals: (pt_2019.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(38 rows)\n\n\n\n\nConfiguration and Operation\n\n\nBy the GUC parameters below, PG-Strom enables/disables the push-down of JOIN/GROUP BY under the partition child tables.\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.enable_partitionwise_gpujoin\n\n\nbool\n\n\non\n\n\nEnables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later.\n\n\n\n\n\n\npg_strom.enable_partitionwise_gpupreagg\n\n\nbool\n\n\non\n\n\nEnables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later.\n\n\n\n\n\n\n\n\nDefault of the parameters are \non\n. Once set to \noff\n, push-down is disabled.\n\n\nThe query execution plan is changed as follows, by EXPLAIN command for the query above section.\nIt uses \nGpuScan\n to scan the partition child tables, however, their results are once written back to the host system, then merged by \nAppend\n and moved to GPU again to process \nGpuJoin\n.\n\n\npostgres=# set pg_strom.enable_partitionwise_gpujoin = off;\nSET\npostgres=# set pg_strom.enable_partitionwise_gpupreagg = off;\nSET\npostgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd \n '2017-01-01'::date group by cat;\n                                                           QUERY PLAN\n--------------------------------------------------------------------------------------------------------------------------------\n Finalize GroupAggregate  (cost=341392.92..341399.42 rows=200 width=48)\n   Group Key: pt.cat\n   -\n  Sort  (cost=341392.92..341393.92 rows=400 width=72)\n         Sort Key: pt.cat\n         -\n  Gather  (cost=341333.63..341375.63 rows=400 width=72)\n               Workers Planned: 2\n               -\n  Partial HashAggregate  (cost=340333.63..340335.63 rows=200 width=72)\n                     Group Key: pt.cat\n                     -\n  Parallel Custom Scan (GpuJoin)  (cost=283591.92..283591.92 rows=7565562 width=40)\n                           Depth 1: GpuHashJoin  (nrows 3152318...7565562)\n                                    HashKeys: pt.aid\n                                    JoinQuals: (pt.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -\n  Append  (cost=28540.80..200673.34 rows=3152318 width=36)\n                                 -\n  Parallel Custom Scan (GpuScan) on pt_2017  (cost=28540.80..66891.11 rows=1050772 width=36)\n                                       GPU Filter: (ymd \n '2017-01-01'::date)\n                                 -\n  Parallel Custom Scan (GpuScan) on pt_2018  (cost=28540.81..66883.43 rows=1050649 width=36)\n                                       GPU Filter: (ymd \n '2017-01-01'::date)\n                                 -\n  Parallel Custom Scan (GpuScan) on pt_2019  (cost=28540.80..66898.79 rows=1050896 width=36)\n                                       GPU Filter: (ymd \n '2017-01-01'::date)\n                           -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(21 rows)\n\n\n\n\nConsideration for SSD/GPU location\n\n\nLimitations\n\n\nExperimental Feature\n\n\nIt is an experimental feature to push down GpuJoin and GpuPreAgg to the partitioned child tables, so it may lead unexpected behavior or system crash. In such case, disable the feature using \npg_strom.enable_partitionwise_gpujoin\n or \npg_strom.enable_partitionwise_gpupreagg\n. And report your case to \nPG-Strom Issues\n.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/partition/#brief-overview", 
            "text": "PostgreSQL v10 newly support table partitioning.\nThis mechanism splits one logically large table into physically small pieces. It is valuable because it can skip partitioned child tables which is obviously unnecessary to scan from the search qualification, and it can offer broader I/O bandwidth by physically distributed storage and so on.  PostgreSQL v10 supports two kinds of them: range-partitioning and list-partitioning. Then, PostgreSQL v11 newly supports hash-partitioning.  The diagram below shows a range-partitioning configuration with  date -type key values.\nA record which has  2018-05-30  as key is distributed to the partition child table  tbl_2018 , in the same way, a record which has  2014-03-21  is distributed to the partition child table  tbl_2014 , and so on.  In case when scan qualifier  WHERE ymd   '2016-07-01'::date  is added on scan of the partitioned table for example, it is obvious that  tbl_2014  and  tbl_2015  contains no records to match, therefore, PostgreSQL' optimizer constructs query execution plan which runs on only  tbl_2016 ,  tbl_2017  and  tbl_2018  then merges their results by  Append  node. It shall perform as if records are read from one logical table.   When PG-Strom is used with table partitioning of PostgreSQL together, its optimizer may choose  GpuScan  to scan the individual partition child tables to be scanned, in the result of cost estimation. In this case,  Append  node merges the results of  GpuScan .  On the other hands, if query runs JOIN or GROUP BY, which can be accelerated by PG-Strom, next to the scan on partitioned table, it needs consideration from the standpoint of performance optimization.  For example, in case when query scans non-partitioned table then runs JOIN with other tables and GROUP BY, under some conditions, it can handle step-step data exchange on GPU device memory. It is an optimal workload for PG-Strom due to minimized data exchange between GPU and CPU.  In case when query runs corresponding workload on the partitioned table, it is problematic that  Append  node is injected into between the child tables scan and JOIN/GROUP BY. Under the query execution plan, the result of GpuScan must be written back to the host system, then  Append  merges them and send back the data to GPU to run the following GpuJoin and GpuPreAgg. It is never efficient query execution.   The example below shows a query execution plan to the query which includes JOIN and GROUP BY towards the partitioned table  pt  by the key field  ymd  of  date  type; per year distribution.  Due to the scan qualification, it omits scan on the partition child tables for 2016 or prior, in addition, a combined JOIN and GROUP BY on the  pt_2017 ,  pt_2018  and  pt_2019  shall be executed prior to the  Append .  # EXPLAIN SELECT cat,count(*),avg(ax)\n            FROM pt NATURAL JOIN t1\n           WHERE ymd   '2017-01-01'::date\n           GROUP BY cat;\n                                   QUERY PLAN\n--------------------------------------------------------------------------------\n HashAggregate  (cost=196410.07..196412.57 rows=200 width=48)\n   Group Key: pt_2017.cat\n   -   Gather  (cost=66085.69..196389.07 rows=1200 width=72)\n         Workers Planned: 2\n         -   Parallel Append  (cost=65085.69..195269.07 rows=600 width=72)\n               -   Parallel Custom Scan (GpuPreAgg)  (cost=65085.69..65089.69 rows=200 width=72)\n                     Reduction: Local\n                     Combined GpuJoin: enabled\n                     -   Parallel Custom Scan (GpuJoin) on pt_2017  (cost=32296.64..74474.20 rows=1050772 width=40)\n                           Outer Scan: pt_2017  (cost=28540.80..66891.11 rows=1050772 width=36)\n                           Outer Scan Filter: (ymd   '2017-01-01'::date)\n                           Depth 1: GpuHashJoin  (nrows 1050772...2521854)\n                                    HashKeys: pt_2017.aid\n                                    JoinQuals: (pt_2017.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n               -   Parallel Custom Scan (GpuPreAgg)  (cost=65078.35..65082.35 rows=200 width=72)\n                     Reduction: Local\n                     Combined GpuJoin: enabled\n                     -   Parallel Custom Scan (GpuJoin) on pt_2018  (cost=32296.65..74465.75 rows=1050649 width=40)\n                           Outer Scan: pt_2018  (cost=28540.81..66883.43 rows=1050649 width=36)\n                           Outer Scan Filter: (ymd   '2017-01-01'::date)\n                           Depth 1: GpuHashJoin  (nrows 1050649...2521557)\n                                    HashKeys: pt_2018.aid\n                                    JoinQuals: (pt_2018.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n               -   Parallel Custom Scan (GpuPreAgg)  (cost=65093.03..65097.03 rows=200 width=72)\n                     Reduction: Local\n                     Combined GpuJoin: enabled\n                     -   Parallel Custom Scan (GpuJoin) on pt_2019  (cost=32296.65..74482.64 rows=1050896 width=40)\n                           Outer Scan: pt_2019  (cost=28540.80..66898.79 rows=1050896 width=36)\n                           Outer Scan Filter: (ymd   '2017-01-01'::date)\n                           Depth 1: GpuHashJoin  (nrows 1050896...2522151)\n                                    HashKeys: pt_2019.aid\n                                    JoinQuals: (pt_2019.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(38 rows)", 
            "title": "Brief overview"
        }, 
        {
            "location": "/partition/#configuration-and-operation", 
            "text": "By the GUC parameters below, PG-Strom enables/disables the push-down of JOIN/GROUP BY under the partition child tables.     Parameter  Type  Default  Description      pg_strom.enable_partitionwise_gpujoin  bool  on  Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later.    pg_strom.enable_partitionwise_gpupreagg  bool  on  Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later.     Default of the parameters are  on . Once set to  off , push-down is disabled.  The query execution plan is changed as follows, by EXPLAIN command for the query above section.\nIt uses  GpuScan  to scan the partition child tables, however, their results are once written back to the host system, then merged by  Append  and moved to GPU again to process  GpuJoin .  postgres=# set pg_strom.enable_partitionwise_gpujoin = off;\nSET\npostgres=# set pg_strom.enable_partitionwise_gpupreagg = off;\nSET\npostgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd   '2017-01-01'::date group by cat;\n                                                           QUERY PLAN\n--------------------------------------------------------------------------------------------------------------------------------\n Finalize GroupAggregate  (cost=341392.92..341399.42 rows=200 width=48)\n   Group Key: pt.cat\n   -   Sort  (cost=341392.92..341393.92 rows=400 width=72)\n         Sort Key: pt.cat\n         -   Gather  (cost=341333.63..341375.63 rows=400 width=72)\n               Workers Planned: 2\n               -   Partial HashAggregate  (cost=340333.63..340335.63 rows=200 width=72)\n                     Group Key: pt.cat\n                     -   Parallel Custom Scan (GpuJoin)  (cost=283591.92..283591.92 rows=7565562 width=40)\n                           Depth 1: GpuHashJoin  (nrows 3152318...7565562)\n                                    HashKeys: pt.aid\n                                    JoinQuals: (pt.aid = t1.aid)\n                                    KDS-Hash (size: 10.78MB)\n                           -   Append  (cost=28540.80..200673.34 rows=3152318 width=36)\n                                 -   Parallel Custom Scan (GpuScan) on pt_2017  (cost=28540.80..66891.11 rows=1050772 width=36)\n                                       GPU Filter: (ymd   '2017-01-01'::date)\n                                 -   Parallel Custom Scan (GpuScan) on pt_2018  (cost=28540.81..66883.43 rows=1050649 width=36)\n                                       GPU Filter: (ymd   '2017-01-01'::date)\n                                 -   Parallel Custom Scan (GpuScan) on pt_2019  (cost=28540.80..66898.79 rows=1050896 width=36)\n                                       GPU Filter: (ymd   '2017-01-01'::date)\n                           -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(21 rows)", 
            "title": "Configuration and Operation"
        }, 
        {
            "location": "/partition/#consideration-for-ssdgpu-location", 
            "text": "", 
            "title": "Consideration for SSD/GPU location"
        }, 
        {
            "location": "/partition/#limitations", 
            "text": "Experimental Feature  It is an experimental feature to push down GpuJoin and GpuPreAgg to the partitioned child tables, so it may lead unexpected behavior or system crash. In such case, disable the feature using  pg_strom.enable_partitionwise_gpujoin  or  pg_strom.enable_partitionwise_gpupreagg . And report your case to  PG-Strom Issues .", 
            "title": "Limitations"
        }, 
        {
            "location": "/troubles/", 
            "text": "Trouble Shooting\n\n\n\nIdentify the problem\n\n\nIn case when a particular workloads produce problems, it is the first step to identify which stuff may cause the problem.\n\n\nUnfortunately, much smaller number of developer supports the PG-Strom development community than PostgreSQL developer's community, thus, due to the standpoint of software quality and history, it is a reasonable estimation to suspect PG-Strom first.\n\n\nThe \npg_strom.enabled\n parameter allows to turn on/off all the functionality of PG-Strom at once.\nThe configuration below disables PG-Strom, thus identically performs with the standard PostgreSQL.\n\n\n# SET pg_strom.enabled = off;\n\n\n\n\nIn addition, we provide parameters to disable particular execution plan like GpuScan, GpuJoin and GpuPreAgg.\n\n\nSee \nreferences/GUC Parameters\n for more details.\n\n\nCollecting crash dump\n\n\nCrash dump is very helpful for analysis of serious problems which lead system crash for example.\nThis session introduces the way to collect crash dump of the PostgreSQL and PG-Strom process (CPU side) and PG-Strom's GPU kernel, and show the back trace on the serious problems.\n\n\nAdd configuration on PostgreSQL startup\n\n\nFor generation of crash dump (CPU-side) on process crash, you need to change the resource limitation of the operating system for size of core file  PostgreSQL server process can generate.\n\n\nFor generation of crash dump (GPU-size) on errors of GPU kernel, PostgreSQL server process has \nCUDA_ENABLE_COREDUMP_ON_EXCEPTION\nenvironment variable, and its value has \n1\n.\n\n\nYou can put a configuration file at \n/etc/systemd/system/postgresql-\nversion\n.service.d/\n when PostgreSQL is kicked by systemd.\n\n\nIn case of RPM installation, a configuration file \npg_strom.conf\n is also installed on the directory, and contains the following initial configuration.\n\n\n[Service]\nLimitNOFILE=65536\nLimitCORE=infinity\n#Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1\n\n\n\n\nIn CUDA 9.1, it usually takes more than several minutes to generate crash dump of GPU kernel, and it entirely stops response of the PostgreSQL session which causes an error.\nSo, we recommend to set \nCUDA_ENABLE_COREDUMP_ON_EXCEPTION\n environment variable only if you investigate errors of GPU kernels which happen on a certain query.\nThe default configuration on RPM installation comments out the line of \nCUDA_ENABLE_COREDUMP_ON_EXCEPTION\n environment variable.\n\n\nPostgreSQL server process should have unlimited \nMax core file size\n configuration, after the next restart.\n\n\nYou can check it as follows.\n\n\n# cat /proc/\nPID of postmaster\n/limits\nLimit                     Soft Limit           Hard Limit           Units\n    :                         :                    :                  :\nMax core file size        unlimited            unlimited            bytes\n    :                         :                    :                  :\n\n\n\n\nInstallation of debuginfo package\n\n\n# yum install postgresql10-debuginfo pg_strom-PG10-debuginfo\n            :\n================================================================================\n Package                  Arch    Version             Repository           Size\n================================================================================\nInstalling:\n pg_strom-PG10-debuginfo  x86_64  1.9-180301.el7      heterodb-debuginfo  766 k\n postgresql10-debuginfo   x86_64  10.3-1PGDG.rhel7    pgdg10              9.7 M\n\nTransaction Summary\n================================================================================\nInstall  2 Packages\n            :\nInstalled:\n  pg_strom-PG10-debuginfo.x86_64 0:1.9-180301.el7\n  postgresql10-debuginfo.x86_64 0:10.3-1PGDG.rhel7\n\nComplete!\n\n\n\n\nChecking the back-trace on CPU side\n\n\nThe kernel parameter \nkernel.core_pattern\n and \nkernel.core_uses_pid\n determine the path where crash dump is written out.\nIt is usually created on the current working directory of the process, check \n/var/lib/pgdata\n where the database cluster is deployed, if you start PostgreSQL server using systemd.\n\n\nOnce \ncore.\nPID\n file gets generated, you can check its back-trace to reach system crash using \ngdb\n.\n\n\ngdb\n speficies the core file by \n-c\n option, and the crashed program by \n-f\n option.\n\n\n# gdb -c /var/lib/pgdata/core.134680 -f /usr/pgsql-10/bin/postgres\nGNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7_4.1\n       :\n(gdb) bt\n#0  0x00007fb942af3903 in __epoll_wait_nocancel () from /lib64/libc.so.6\n#1  0x00000000006f71ae in WaitEventSetWaitBlock (nevents=1,\n    occurred_events=0x7ffee51e1d70, cur_timeout=-1, set=0x2833298)\n    at latch.c:1048\n#2  WaitEventSetWait (set=0x2833298, timeout=timeout@entry-1,\n    occurred_events=occurred_events@entry0x7ffee51e1d70,\n    nevents=nevents@entry1, wait_event_info=wait_event_info@entry100663296)\n    at latch.c:1000\n#3  0x00000000006210fb in secure_read (port=0x2876120,\n    ptr=0xcaa7e0 \nPqRecvBuffer\n, len=8192) at be-secure.c:166\n#4  0x000000000062b6e8 in pq_recvbuf () at pqcomm.c:963\n#5  0x000000000062c345 in pq_getbyte () at pqcomm.c:1006\n#6  0x0000000000718682 in SocketBackend (inBuf=0x7ffee51e1ef0)\n    at postgres.c:328\n#7  ReadCommand (inBuf=0x7ffee51e1ef0) at postgres.c:501\n#8  PostgresMain (argc=\noptimized out\n, argv=argv@entry0x287bb68,\n    dbname=0x28333f8 \npostgres\n, username=\noptimized out\n) at postgres.c:4030\n#9  0x000000000047adbc in BackendRun (port=0x2876120) at postmaster.c:4405\n#10 BackendStartup (port=0x2876120) at postmaster.c:4077\n#11 ServerLoop () at postmaster.c:1755\n#12 0x00000000006afb7f in PostmasterMain (argc=argc@entry3,\n    argv=argv@entry0x2831280) at postmaster.c:1363\n#13 0x000000000047bbef in main (argc=3, argv=0x2831280) at main.c:228\n\n\n\n\nbt\n command of \ngdb\n displays the backtrace.\nIn this case, I sent \nSIGSEGV\n signal to the PostgreSQL backend which is waiting for queries from the client for intentional crash, the process got crashed at \n__epoll_wait_nocancel\n invoked by \nWaitEventSetWait\n.\n\n\nChecking the backtrace on GPU\n\n\nCrash dump of GPU kernel is generated on the current working directory of PostgreSQL server process, unless you don't specify the path using \nCUDA_COREDUMP_FILE\n environment variable explicitly.\nCheck \n/var/lib/pgdata\n where the database cluster is deployed, if systemd started PostgreSQL. Dump file will have the following naming convension.\n\n\ncore_\ntimestamp\n_\nhostname\n_\nPID\n.nvcudmp\n\n\nNote that the dump-file of GPU kernel contains no debug information like symbol information in the default configuration.\nIt is nearly impossible to investigate the problem, so enable inclusion of debug information for the GPU programs generated by PG-Strom, as follows.\n\n\nAlso note than we don't recommend to turn on the configuration for daily usage, because it makes query execution performan slow down.\nTurn on only when you investigate the troubles.\n\n\nnvme=# set pg_strom.debug_jit_compile_options = on;\nSET\n\n\n\n\nYou can check crash dump of the GPU kernel using \ncuda-gdb\n command.\n\n\n# /usr/local/cuda/bin/cuda-gdb\nNVIDIA (R) CUDA Debugger\n9.1 release\nPortions Copyright (C) 2007-2017 NVIDIA Corporation\n        :\nFor help, type \nhelp\n.\nType \napropos word\n to search for commands related to \nword\n.\n(cuda-gdb)\n\n\n\n\nRun \ncuda-gdb\n command, then load the crash dump file above using \ntarget\n command on the prompt.\n\n\n(cuda-gdb) target cudacore /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp\nOpening GPU coredump: /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp\n[New Thread 216240]\n\nCUDA Exception: Warp Illegal Address\nThe exception was triggered at PC 0x7ff4dc82f930 (cuda_gpujoin.h:1159)\n[Current focus set to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]\n#0  0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078,\n    kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48,\n    matched=0x7ff4f7fffc7c \n) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159\n1159            while (khitem \n khitem-\nhash != hash_value)\n\n\n\n\nYou can check backtrace where the error happened on GPU kernel using \nbt\n command.\n\n\n(cuda-gdb) bt\n#0  0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078,\n    kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48,\n    matched=0x7ff4f7fffc7c \n) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159\n#1  0x00007ff4dc9428f0 in gpujoin_main\n(30,1,1),(256,1,1)\n (kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030,\n    kds_dst=0x7fe9e8800030, kparams_gpreagg=0x0) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1347\n\n\n\n\nPlease check \nCUDA Toolkit Documentation - CUDA-GDB\n for more detailed usage of \ncuda-gdb\n command.", 
            "title": "Trouble Shooting"
        }, 
        {
            "location": "/troubles/#identify-the-problem", 
            "text": "In case when a particular workloads produce problems, it is the first step to identify which stuff may cause the problem.  Unfortunately, much smaller number of developer supports the PG-Strom development community than PostgreSQL developer's community, thus, due to the standpoint of software quality and history, it is a reasonable estimation to suspect PG-Strom first.  The  pg_strom.enabled  parameter allows to turn on/off all the functionality of PG-Strom at once.\nThe configuration below disables PG-Strom, thus identically performs with the standard PostgreSQL.  # SET pg_strom.enabled = off;  In addition, we provide parameters to disable particular execution plan like GpuScan, GpuJoin and GpuPreAgg.  See  references/GUC Parameters  for more details.", 
            "title": "Identify the problem"
        }, 
        {
            "location": "/troubles/#collecting-crash-dump", 
            "text": "Crash dump is very helpful for analysis of serious problems which lead system crash for example.\nThis session introduces the way to collect crash dump of the PostgreSQL and PG-Strom process (CPU side) and PG-Strom's GPU kernel, and show the back trace on the serious problems.", 
            "title": "Collecting crash dump"
        }, 
        {
            "location": "/troubles/#add-configuration-on-postgresql-startup", 
            "text": "For generation of crash dump (CPU-side) on process crash, you need to change the resource limitation of the operating system for size of core file  PostgreSQL server process can generate.  For generation of crash dump (GPU-size) on errors of GPU kernel, PostgreSQL server process has  CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable, and its value has  1 .  You can put a configuration file at  /etc/systemd/system/postgresql- version .service.d/  when PostgreSQL is kicked by systemd.  In case of RPM installation, a configuration file  pg_strom.conf  is also installed on the directory, and contains the following initial configuration.  [Service]\nLimitNOFILE=65536\nLimitCORE=infinity\n#Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1  In CUDA 9.1, it usually takes more than several minutes to generate crash dump of GPU kernel, and it entirely stops response of the PostgreSQL session which causes an error.\nSo, we recommend to set  CUDA_ENABLE_COREDUMP_ON_EXCEPTION  environment variable only if you investigate errors of GPU kernels which happen on a certain query.\nThe default configuration on RPM installation comments out the line of  CUDA_ENABLE_COREDUMP_ON_EXCEPTION  environment variable.  PostgreSQL server process should have unlimited  Max core file size  configuration, after the next restart.  You can check it as follows.  # cat /proc/ PID of postmaster /limits\nLimit                     Soft Limit           Hard Limit           Units\n    :                         :                    :                  :\nMax core file size        unlimited            unlimited            bytes\n    :                         :                    :                  :", 
            "title": "Add configuration on PostgreSQL startup"
        }, 
        {
            "location": "/troubles/#installation-of-debuginfo-package", 
            "text": "# yum install postgresql10-debuginfo pg_strom-PG10-debuginfo\n            :\n================================================================================\n Package                  Arch    Version             Repository           Size\n================================================================================\nInstalling:\n pg_strom-PG10-debuginfo  x86_64  1.9-180301.el7      heterodb-debuginfo  766 k\n postgresql10-debuginfo   x86_64  10.3-1PGDG.rhel7    pgdg10              9.7 M\n\nTransaction Summary\n================================================================================\nInstall  2 Packages\n            :\nInstalled:\n  pg_strom-PG10-debuginfo.x86_64 0:1.9-180301.el7\n  postgresql10-debuginfo.x86_64 0:10.3-1PGDG.rhel7\n\nComplete!", 
            "title": "Installation of debuginfo package"
        }, 
        {
            "location": "/troubles/#checking-the-back-trace-on-cpu-side", 
            "text": "The kernel parameter  kernel.core_pattern  and  kernel.core_uses_pid  determine the path where crash dump is written out.\nIt is usually created on the current working directory of the process, check  /var/lib/pgdata  where the database cluster is deployed, if you start PostgreSQL server using systemd.  Once  core. PID  file gets generated, you can check its back-trace to reach system crash using  gdb .  gdb  speficies the core file by  -c  option, and the crashed program by  -f  option.  # gdb -c /var/lib/pgdata/core.134680 -f /usr/pgsql-10/bin/postgres\nGNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7_4.1\n       :\n(gdb) bt\n#0  0x00007fb942af3903 in __epoll_wait_nocancel () from /lib64/libc.so.6\n#1  0x00000000006f71ae in WaitEventSetWaitBlock (nevents=1,\n    occurred_events=0x7ffee51e1d70, cur_timeout=-1, set=0x2833298)\n    at latch.c:1048\n#2  WaitEventSetWait (set=0x2833298, timeout=timeout@entry-1,\n    occurred_events=occurred_events@entry0x7ffee51e1d70,\n    nevents=nevents@entry1, wait_event_info=wait_event_info@entry100663296)\n    at latch.c:1000\n#3  0x00000000006210fb in secure_read (port=0x2876120,\n    ptr=0xcaa7e0  PqRecvBuffer , len=8192) at be-secure.c:166\n#4  0x000000000062b6e8 in pq_recvbuf () at pqcomm.c:963\n#5  0x000000000062c345 in pq_getbyte () at pqcomm.c:1006\n#6  0x0000000000718682 in SocketBackend (inBuf=0x7ffee51e1ef0)\n    at postgres.c:328\n#7  ReadCommand (inBuf=0x7ffee51e1ef0) at postgres.c:501\n#8  PostgresMain (argc= optimized out , argv=argv@entry0x287bb68,\n    dbname=0x28333f8  postgres , username= optimized out ) at postgres.c:4030\n#9  0x000000000047adbc in BackendRun (port=0x2876120) at postmaster.c:4405\n#10 BackendStartup (port=0x2876120) at postmaster.c:4077\n#11 ServerLoop () at postmaster.c:1755\n#12 0x00000000006afb7f in PostmasterMain (argc=argc@entry3,\n    argv=argv@entry0x2831280) at postmaster.c:1363\n#13 0x000000000047bbef in main (argc=3, argv=0x2831280) at main.c:228  bt  command of  gdb  displays the backtrace.\nIn this case, I sent  SIGSEGV  signal to the PostgreSQL backend which is waiting for queries from the client for intentional crash, the process got crashed at  __epoll_wait_nocancel  invoked by  WaitEventSetWait .", 
            "title": "Checking the back-trace on CPU side"
        }, 
        {
            "location": "/troubles/#checking-the-backtrace-on-gpu", 
            "text": "Crash dump of GPU kernel is generated on the current working directory of PostgreSQL server process, unless you don't specify the path using  CUDA_COREDUMP_FILE  environment variable explicitly.\nCheck  /var/lib/pgdata  where the database cluster is deployed, if systemd started PostgreSQL. Dump file will have the following naming convension.  core_ timestamp _ hostname _ PID .nvcudmp  Note that the dump-file of GPU kernel contains no debug information like symbol information in the default configuration.\nIt is nearly impossible to investigate the problem, so enable inclusion of debug information for the GPU programs generated by PG-Strom, as follows.  Also note than we don't recommend to turn on the configuration for daily usage, because it makes query execution performan slow down.\nTurn on only when you investigate the troubles.  nvme=# set pg_strom.debug_jit_compile_options = on;\nSET  You can check crash dump of the GPU kernel using  cuda-gdb  command.  # /usr/local/cuda/bin/cuda-gdb\nNVIDIA (R) CUDA Debugger\n9.1 release\nPortions Copyright (C) 2007-2017 NVIDIA Corporation\n        :\nFor help, type  help .\nType  apropos word  to search for commands related to  word .\n(cuda-gdb)  Run  cuda-gdb  command, then load the crash dump file above using  target  command on the prompt.  (cuda-gdb) target cudacore /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp\nOpening GPU coredump: /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp\n[New Thread 216240]\n\nCUDA Exception: Warp Illegal Address\nThe exception was triggered at PC 0x7ff4dc82f930 (cuda_gpujoin.h:1159)\n[Current focus set to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]\n#0  0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078,\n    kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48,\n    matched=0x7ff4f7fffc7c  ) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159\n1159            while (khitem   khitem- hash != hash_value)  You can check backtrace where the error happened on GPU kernel using  bt  command.  (cuda-gdb) bt\n#0  0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078,\n    kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48,\n    matched=0x7ff4f7fffc7c  ) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159\n#1  0x00007ff4dc9428f0 in gpujoin_main (30,1,1),(256,1,1)  (kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030,\n    kds_dst=0x7fe9e8800030, kparams_gpreagg=0x0) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1347  Please check  CUDA Toolkit Documentation - CUDA-GDB  for more detailed usage of  cuda-gdb  command.", 
            "title": "Checking the backtrace on GPU"
        }, 
        {
            "location": "/ssd2gpu/", 
            "text": "SSD-to-GPU Direct SQL Execution\n\n\n\nOverview\n\n\nFor the fast execution of SQL workloads, it needs to provide processors rapid data stream from storage or memory, in addition to processor's execution efficiency. Processor will run idle if data stream would not be delivered.\n\n\nSSD-to-GPU Direct SQL Execution directly connects NVMe-SSD which enables high-speed I/O processing by direct attach to the PCIe bus and GPU device that is also attached on the same PCIe bus, and runs SQL workloads very high speed by supplying data stream close to the wired speed of the hardware.\n\n\nUsually, PostgreSQL data blocks on the storage shall be once loaded to CPU/RAM through the PCIe bus, then, PostgreSQL runs WHERE-clause for filtering or JOIN/GROUP BY according to the query execution plan. Due to the characteristics of analytic workloads, the amount of result data set is much smaller than the source data set. For example, it is not rare case to read billions rows but output just hundreds rows after the aggregation operations with GROUP BY.\n\n\nIn the other words, we consume bandwidth of the PCIe bus to move junk data, however, we cannot determine whether rows are necessary or not prior to the evaluation by SQL workloads on CPU. So, it is not avoidable restriction in usual implementation.\n\n\n\n\nSSD-to-GPU Direct SQL Execution changes the flow to read blocks from the storage sequentially. It directly loads data blocks to GPU using peer-to-peer DMA over PCIe bus, then runs SQL workloads on GPU device to reduce number of rows to be processed by CPU. In other words, it utilizes GPU as a pre-processor of SQL which locates in the middle of the storage and CPU/RAM for reduction of CPU's load, then tries to accelerate I/O processing in the results.\n\n\nThis feature internally uses NVIDIA GPUDirect RDMA. It allows peer-to-peer data transfer over PCIe bus between GPU device memory and third parth device by coordination using a custom Linux kernel module.\nSo, this feature requires NVMe-Strom driver which is a Linux kernel module in addition to PG-Strom which is a PostgreSQL extension module.\n\n\nAlso note that this feature supports only NVMe-SSD. It does not support SAS or SATA SSD.\nWe have tested several NVMe-SSD models. You can refer \n002: HW Validation List\n for your information.\n\n\nSystem Setup\n\n\nDriver Installation\n\n\nnvme_strom\n package is required to activate SSD-to-GPU Direct SQL Execution. This package contains a custom Linux kernel module which intermediates P2P DMA from NVME-SSD to GPU. You can obtain the package from the \nHeteroDB Software Distribution Center\n.\n\n\nIf \nheterodb-swdc\n package is already installed, you can install the package by \nyum\n command.\n\n\n$ sudo yum install nvme_strom\n            :\n================================================================================\n Package             Arch            Version            Repository         Size\n================================================================================\nInstalling:\n nvme_strom          x86_64          0.8-1.el7          heterodb          178 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n            :\nDKMS: install completed.\n  Verifying  : nvme_strom-0.8-1.el7.x86_64                                  1/1\n\nInstalled:\n  nvme_strom.x86_64 0:0.8-1.el7\n\nComplete!\n\n\n\n\nOnce \nnvme_strom\n package gets installed, you can see \nnvme_strom\n module using \nlsmod\n command below.\n\n\n$ lsmod | grep nvme\nnvme_strom             12625  0\nnvme                   27722  4\nnvme_core              52964  9 nvme\n\n\n\n\nDesigning Tablespace\n\n\nSSD-to-GPU Direct SQL Execution shall be invoked in the following case.\n\n\n\n\nThe target table to be scanned locates on the partition being consist of NVMe-SSD.\n\n\n/dev/nvmeXXXX\n block device, or md-raid0 volume which consists of NVMe-SSDs only.\n\n\n\n\n\n\nThe target table size is larger than \npg_strom.nvme_strom_threshold\n.\n\n\nYou can adjust this configuration. Its default is physical RAM size of the system plus 1/3 of \nshared_buffers\n configuration.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nStriped read from multiple NVMe-SSD using md-raid0 requires the enterprise subscription provided by HeteroDB,Inc.\n\n\n\n\nIn order to deploy the tables on the partition consists of NVMe-SSD, you can use the tablespace function of PostgreSQL to specify particular tables or databases to place them on NVMe-SSD volume, in addition to construction of the entire database cluster on the NVMe-SSD volume.\n\n\nFor example, you can create a new tablespace below, if NVMe-SSD is mounted at \n/opt/nvme\n.\n\n\nCREATE TABLESPACE my_nvme LOCATION '/opt/nvme';\n\n\n\n\nIn order to create a new table on the tablespace, specify the \nTABLESPACE\n option at the \nCREATE TABLE\n command below.\n\n\nCREATE TABLE my_table (...) TABLESPACE my_nvme;\n\n\n\n\nOr, use \nALTER DATABASE\n command as follows, to change the default tablespace of the database.\nNote that tablespace of the existing tables are not changed in thie case.\n\n\nALTER DATABASE my_database SET TABLESPACE my_nvme;\n\n\n\n\nOperations\n\n\nControls using GUC parameters\n\n\nThere are two GPU parameters related to SSD-to-GPU Direct SQL Execution.\n\n\nThe first is \npg_strom.nvme_strom_enabled\n that simply turn on/off the function of SSD-to-GPU Direct SQL Execution.\nIf \noff\n, SSD-to-GPU Direct SQL Execution should not be used regardless of the table size or physical location. Default is \non\n.\n\n\nThe other one is \npg_strom.nvme_strom_threshold\n which specifies the least table size to invoke SSD-to-GPU Direct SQL Execution.\n\n\nPG-Strom will choose SSD-to-GPU Direct SQL Execution when target table is located on NVMe-SSD volume (or md-raid0 volume which consists of NVMe-SSD only), and the table size is larger than this parameter.\nIts default is sum of the physical memory size and 1/3 of the \nshared_buffers\n. It means default configuration invokes SSD-to-GPU Direct SQL Execution only for the tables where we certainly cannot process them on memory.\n\n\nEven if SSD-to-GPU Direct SQL Execution has advantages on a single table scan workload, usage of disk cache may work better on the second or later trial for the tables which are available to load onto the main memory.\n\n\nOn course, this assumption is not always right depending on the workload charasteristics.\n\n\nEnsure usage of SSD-to-GPU Direct SQL Execution\n\n\nEXPLAIN\n command allows to ensure whether SSD-to-GPU Direct SQL Execution shall be used in the target query, or not.\n\n\nIn the example below, a scan on the \nlineorder\n table by \nCustom Scan (GpuJoin)\n shows \nNVMe-Strom: enabled\n. In this case, SSD-to-GPU Direct SQL Execution shall be used to read from the \nlineorder\n table.\n\n\n# explain (costs off)\nselect sum(lo_revenue), d_year, p_brand1\nfrom lineorder, date1, part, supplier\nwhere lo_orderdate = d_datekey\nand lo_partkey = p_partkey\nand lo_suppkey = s_suppkey\nand p_category = 'MFGR#12'\nand s_region = 'AMERICA'\n  group by d_year, p_brand1\n  order by d_year, p_brand1;\n                                          QUERY PLAN\n----------------------------------------------------------------------------------------------\n GroupAggregate\n   Group Key: date1.d_year, part.p_brand1\n   -\n  Sort\n         Sort Key: date1.d_year, part.p_brand1\n         -\n  Custom Scan (GpuPreAgg)\n               Reduction: Local\n               GPU Projection: pgstrom.psum((lo_revenue)::double precision), d_year, p_brand1\n               Combined GpuJoin: enabled\n               -\n  Custom Scan (GpuJoin) on lineorder\n                     GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue\n                     Outer Scan: lineorder\n                     Depth 1: GpuHashJoin  (nrows 2406009600...97764190)\n                              HashKeys: lineorder.lo_partkey\n                              JoinQuals: (lineorder.lo_partkey = part.p_partkey)\n                              KDS-Hash (size: 10.67MB)\n                     Depth 2: GpuHashJoin  (nrows 97764190...18544060)\n                              HashKeys: lineorder.lo_suppkey\n                              JoinQuals: (lineorder.lo_suppkey = supplier.s_suppkey)\n                              KDS-Hash (size: 131.59MB)\n                     Depth 3: GpuHashJoin  (nrows 18544060...18544060)\n                              HashKeys: lineorder.lo_orderdate\n                              JoinQuals: (lineorder.lo_orderdate = date1.d_datekey)\n                              KDS-Hash (size: 461.89KB)\n                     NVMe-Strom: enabled\n                     -\n  Custom Scan (GpuScan) on part\n                           GPU Projection: p_brand1, p_partkey\n                           GPU Filter: (p_category = 'MFGR#12'::bpchar)\n                     -\n  Custom Scan (GpuScan) on supplier\n                           GPU Projection: s_suppkey\n                           GPU Filter: (s_region = 'AMERICA'::bpchar)\n                     -\n  Seq Scan on date1\n(31 rows)\n\n\n\n\nAttension for visibility map\n\n\nRight now, GPU routines of PG-Strom cannot run MVCC visibility checks per row, because only host code has a special data structure for visibility checks. It also leads a problem.\n\n\nWe cannot know which row is visible, or invisible at the time when PG-Strom requires P2P DMA for NVMe-SSD, because contents of the storage blocks are not yet loaded to CPU/RAM, and MVCC related attributes are written with individual records. PostgreSQL had similar problem when it supports IndexOnlyScan.\n\n\nTo address the problem, PostgreSQL has an infrastructure of visibility map which is a bunch of flags to indicate whether any records in a particular data block are visible from all the transactions. If associated bit is set, we can know the associated block has no invisible records without reading the block itself.\n\n\nSSD-to-GPU Direct SQL Execution utilizes this infrastructure. It checks the visibility map first, then only \"all-visible\" blocks are required to read with SSD-to-GPU P2P DMA.\n\n\nVACUUM constructs visibility map, so you can enforce PostgreSQL to construct visibility map by explicit launch of VACUUM command.\n\n\nVACUUM ANALYZE linerorder;", 
            "title": "SSD2GPU Direct SQL Exec"
        }, 
        {
            "location": "/ssd2gpu/#overview", 
            "text": "For the fast execution of SQL workloads, it needs to provide processors rapid data stream from storage or memory, in addition to processor's execution efficiency. Processor will run idle if data stream would not be delivered.  SSD-to-GPU Direct SQL Execution directly connects NVMe-SSD which enables high-speed I/O processing by direct attach to the PCIe bus and GPU device that is also attached on the same PCIe bus, and runs SQL workloads very high speed by supplying data stream close to the wired speed of the hardware.  Usually, PostgreSQL data blocks on the storage shall be once loaded to CPU/RAM through the PCIe bus, then, PostgreSQL runs WHERE-clause for filtering or JOIN/GROUP BY according to the query execution plan. Due to the characteristics of analytic workloads, the amount of result data set is much smaller than the source data set. For example, it is not rare case to read billions rows but output just hundreds rows after the aggregation operations with GROUP BY.  In the other words, we consume bandwidth of the PCIe bus to move junk data, however, we cannot determine whether rows are necessary or not prior to the evaluation by SQL workloads on CPU. So, it is not avoidable restriction in usual implementation.   SSD-to-GPU Direct SQL Execution changes the flow to read blocks from the storage sequentially. It directly loads data blocks to GPU using peer-to-peer DMA over PCIe bus, then runs SQL workloads on GPU device to reduce number of rows to be processed by CPU. In other words, it utilizes GPU as a pre-processor of SQL which locates in the middle of the storage and CPU/RAM for reduction of CPU's load, then tries to accelerate I/O processing in the results.  This feature internally uses NVIDIA GPUDirect RDMA. It allows peer-to-peer data transfer over PCIe bus between GPU device memory and third parth device by coordination using a custom Linux kernel module.\nSo, this feature requires NVMe-Strom driver which is a Linux kernel module in addition to PG-Strom which is a PostgreSQL extension module.  Also note that this feature supports only NVMe-SSD. It does not support SAS or SATA SSD.\nWe have tested several NVMe-SSD models. You can refer  002: HW Validation List  for your information.", 
            "title": "Overview"
        }, 
        {
            "location": "/ssd2gpu/#system-setup", 
            "text": "", 
            "title": "System Setup"
        }, 
        {
            "location": "/ssd2gpu/#driver-installation", 
            "text": "nvme_strom  package is required to activate SSD-to-GPU Direct SQL Execution. This package contains a custom Linux kernel module which intermediates P2P DMA from NVME-SSD to GPU. You can obtain the package from the  HeteroDB Software Distribution Center .  If  heterodb-swdc  package is already installed, you can install the package by  yum  command.  $ sudo yum install nvme_strom\n            :\n================================================================================\n Package             Arch            Version            Repository         Size\n================================================================================\nInstalling:\n nvme_strom          x86_64          0.8-1.el7          heterodb          178 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n            :\nDKMS: install completed.\n  Verifying  : nvme_strom-0.8-1.el7.x86_64                                  1/1\n\nInstalled:\n  nvme_strom.x86_64 0:0.8-1.el7\n\nComplete!  Once  nvme_strom  package gets installed, you can see  nvme_strom  module using  lsmod  command below.  $ lsmod | grep nvme\nnvme_strom             12625  0\nnvme                   27722  4\nnvme_core              52964  9 nvme", 
            "title": "Driver Installation"
        }, 
        {
            "location": "/ssd2gpu/#designing-tablespace", 
            "text": "SSD-to-GPU Direct SQL Execution shall be invoked in the following case.   The target table to be scanned locates on the partition being consist of NVMe-SSD.  /dev/nvmeXXXX  block device, or md-raid0 volume which consists of NVMe-SSDs only.    The target table size is larger than  pg_strom.nvme_strom_threshold .  You can adjust this configuration. Its default is physical RAM size of the system plus 1/3 of  shared_buffers  configuration.      Note  Striped read from multiple NVMe-SSD using md-raid0 requires the enterprise subscription provided by HeteroDB,Inc.   In order to deploy the tables on the partition consists of NVMe-SSD, you can use the tablespace function of PostgreSQL to specify particular tables or databases to place them on NVMe-SSD volume, in addition to construction of the entire database cluster on the NVMe-SSD volume.  For example, you can create a new tablespace below, if NVMe-SSD is mounted at  /opt/nvme .  CREATE TABLESPACE my_nvme LOCATION '/opt/nvme';  In order to create a new table on the tablespace, specify the  TABLESPACE  option at the  CREATE TABLE  command below.  CREATE TABLE my_table (...) TABLESPACE my_nvme;  Or, use  ALTER DATABASE  command as follows, to change the default tablespace of the database.\nNote that tablespace of the existing tables are not changed in thie case.  ALTER DATABASE my_database SET TABLESPACE my_nvme;", 
            "title": "Designing Tablespace"
        }, 
        {
            "location": "/ssd2gpu/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/ssd2gpu/#controls-using-guc-parameters", 
            "text": "There are two GPU parameters related to SSD-to-GPU Direct SQL Execution.  The first is  pg_strom.nvme_strom_enabled  that simply turn on/off the function of SSD-to-GPU Direct SQL Execution.\nIf  off , SSD-to-GPU Direct SQL Execution should not be used regardless of the table size or physical location. Default is  on .  The other one is  pg_strom.nvme_strom_threshold  which specifies the least table size to invoke SSD-to-GPU Direct SQL Execution.  PG-Strom will choose SSD-to-GPU Direct SQL Execution when target table is located on NVMe-SSD volume (or md-raid0 volume which consists of NVMe-SSD only), and the table size is larger than this parameter.\nIts default is sum of the physical memory size and 1/3 of the  shared_buffers . It means default configuration invokes SSD-to-GPU Direct SQL Execution only for the tables where we certainly cannot process them on memory.  Even if SSD-to-GPU Direct SQL Execution has advantages on a single table scan workload, usage of disk cache may work better on the second or later trial for the tables which are available to load onto the main memory.  On course, this assumption is not always right depending on the workload charasteristics.", 
            "title": "Controls using GUC parameters"
        }, 
        {
            "location": "/ssd2gpu/#ensure-usage-of-ssd-to-gpu-direct-sql-execution", 
            "text": "EXPLAIN  command allows to ensure whether SSD-to-GPU Direct SQL Execution shall be used in the target query, or not.  In the example below, a scan on the  lineorder  table by  Custom Scan (GpuJoin)  shows  NVMe-Strom: enabled . In this case, SSD-to-GPU Direct SQL Execution shall be used to read from the  lineorder  table.  # explain (costs off)\nselect sum(lo_revenue), d_year, p_brand1\nfrom lineorder, date1, part, supplier\nwhere lo_orderdate = d_datekey\nand lo_partkey = p_partkey\nand lo_suppkey = s_suppkey\nand p_category = 'MFGR#12'\nand s_region = 'AMERICA'\n  group by d_year, p_brand1\n  order by d_year, p_brand1;\n                                          QUERY PLAN\n----------------------------------------------------------------------------------------------\n GroupAggregate\n   Group Key: date1.d_year, part.p_brand1\n   -   Sort\n         Sort Key: date1.d_year, part.p_brand1\n         -   Custom Scan (GpuPreAgg)\n               Reduction: Local\n               GPU Projection: pgstrom.psum((lo_revenue)::double precision), d_year, p_brand1\n               Combined GpuJoin: enabled\n               -   Custom Scan (GpuJoin) on lineorder\n                     GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue\n                     Outer Scan: lineorder\n                     Depth 1: GpuHashJoin  (nrows 2406009600...97764190)\n                              HashKeys: lineorder.lo_partkey\n                              JoinQuals: (lineorder.lo_partkey = part.p_partkey)\n                              KDS-Hash (size: 10.67MB)\n                     Depth 2: GpuHashJoin  (nrows 97764190...18544060)\n                              HashKeys: lineorder.lo_suppkey\n                              JoinQuals: (lineorder.lo_suppkey = supplier.s_suppkey)\n                              KDS-Hash (size: 131.59MB)\n                     Depth 3: GpuHashJoin  (nrows 18544060...18544060)\n                              HashKeys: lineorder.lo_orderdate\n                              JoinQuals: (lineorder.lo_orderdate = date1.d_datekey)\n                              KDS-Hash (size: 461.89KB)\n                     NVMe-Strom: enabled\n                     -   Custom Scan (GpuScan) on part\n                           GPU Projection: p_brand1, p_partkey\n                           GPU Filter: (p_category = 'MFGR#12'::bpchar)\n                     -   Custom Scan (GpuScan) on supplier\n                           GPU Projection: s_suppkey\n                           GPU Filter: (s_region = 'AMERICA'::bpchar)\n                     -   Seq Scan on date1\n(31 rows)", 
            "title": "Ensure usage of SSD-to-GPU Direct SQL Execution"
        }, 
        {
            "location": "/ssd2gpu/#attension-for-visibility-map", 
            "text": "Right now, GPU routines of PG-Strom cannot run MVCC visibility checks per row, because only host code has a special data structure for visibility checks. It also leads a problem.  We cannot know which row is visible, or invisible at the time when PG-Strom requires P2P DMA for NVMe-SSD, because contents of the storage blocks are not yet loaded to CPU/RAM, and MVCC related attributes are written with individual records. PostgreSQL had similar problem when it supports IndexOnlyScan.  To address the problem, PostgreSQL has an infrastructure of visibility map which is a bunch of flags to indicate whether any records in a particular data block are visible from all the transactions. If associated bit is set, we can know the associated block has no invisible records without reading the block itself.  SSD-to-GPU Direct SQL Execution utilizes this infrastructure. It checks the visibility map first, then only \"all-visible\" blocks are required to read with SSD-to-GPU P2P DMA.  VACUUM constructs visibility map, so you can enforce PostgreSQL to construct visibility map by explicit launch of VACUUM command.  VACUUM ANALYZE linerorder;", 
            "title": "Attension for visibility map"
        }, 
        {
            "location": "/ccache/", 
            "text": "In-memory Columnar Cache\n\n\n\n\n\nWarning\n\n\n\u30a4\u30f3\u30e1\u30e2\u30ea\u5217\u30ad\u30e3\u30c3\u30b7\u30e5\u306fv2.2\u3067\u5ec3\u6b62\u304c\u4e88\u5b9a\u3055\u308c\u3066\u304a\u308a\u3001\u4ee3\u308f\u308a\u306b\u3001\u73fe\u5728\u958b\u767a\u4e2d\u306eParquet_fdw\u306b\u3088\u3063\u3066\u5217\u6307\u5411\u306e\u30c7\u30fc\u30bf\u30b9\u30c8\u30a2\u304c\u30a4\u30f3\u30e1\u30e2\u30ea\uff5e\u30b9\u30c8\u30ec\u30fc\u30b8\u7d1a\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u306b\u5bfe\u3057\u3066\u63d0\u4f9b\u4e88\u5b9a\u3067\u3059\u3002\n\n\n\n\n\n\nWarning\n\n\nIn-memory Columnar Cache shall be deprecated at v2.2, instead of the Parquet_fdw (under the development) that offers columnar data store for on-memory or on-storage class data size.\n\n\n\n\n}\n\n\nOverview\n\n\nPG-Strom has one another feature related to storage to supply processors data stream.\n\n\nIn-memory columnar cache reads data blocks of the target table, convert the row-format of PostgreSQL to columnar format which is suitable for summary and analytics, and cache them on memory.\n\n\nThis feature requires no special hardware like SSD-to-GPU Direct SQL Execution, on the other hands, RAM capacity is still smaller than SSD, so this feature is suitable to handle \"not a large scale data set\" up to 60%-75% of the system RAM size.\n\n\nThis feature is not \"a columnar store\". It means cached and converted data blocks are flashed once PostgreSQL server process has restarted for example. When any cached rows get updated, PG-Strom invalidates the columnar cache block which contains the updated rows.\nThis design on the basis that columnar format is vulnerable to updating workloads. If we try to update columnar-store with keeping consistency towards update of row-store, huge degradation of write performance is not avoidable. On the other hands, it is lightweight operation to invalidate the columnar cache block which contains the updated row.\nPG-Strom can switch GPU kernels to be invoked for row- or columnar-format according to format of the loading data blocks. So, it works flexibly, even if a columnar cache block gets invalidated thus PG-Strom has to load data blocks from the shared buffer of PostgreSQL.\n\n\n\n\nSystem Setup\n\n\nLocation of the columnar cache\n\n\nThe \npg_strom.ccache_base_dir\n parameter allows to specify the path to store the columnar cache. The default is \n/dev/shm\n where general Linux distribution mounts \ntmpfs\n filesystem, so files under the directory are \"volatile\", with no backing store.\n\n\nCustom configuration of the parameter enables to construct columnar cache on larger and reasonably fast storage, like NVMe-SSD, as backing store. However, note that update of the cached rows invalidates whole of the chunk (128MB) which contains the updated rows. It may lead unexpected performance degradation, if workloads have frequent read / write involving I/O operations.\n\n\nSource Table Configuration\n\n\nDBA needs to specify the target tables to build columnar cache.\n\n\nA SQL function \npgstrom_ccache_enabled(regclass)\n adds the supplied table as target to build columnar cache.\nOther way round, a SQL function \npgstrom_ccache_disabled(regclass)\n drops the supplied table from the target to build.\n\n\nInternally, it is implemented as a special trigger function which invalidate columnar cache on write to the target tables.\nIt means we don't build columnar cache on the tables which have no way to invalidate columnar cache.\n\n\npostgres=# select pgstrom_ccache_enabled('t0');\n pgstrom_ccache_enabled\n------------------------\n enabled\n(1 row)\n\n\n\n\nOperations\n\n\nLoading the columnar cache\n\n\nThe \npgstrom_ccache_prewarm()\n loads the specified table onto the columnar cache.\nIf specified table has the above trigger function, it tries to load the table contents until it reached to the table end or exceeds to the configured total size of columnar cache.\n\n\npostgres=# select pgstrom_ccache_prewarm('t0');\n pgstrom_ccache_prewarm\n------------------------\n                     35\n(1 row)\n\n\n\n\nCheck status of columnar cache\n\n\npgstrom.ccache_info\n provides the status of the current columnar cache.\n\n\nYou can check the table, block number, cache creation time and last access time per chunk.\n\n\ncontrib_regression_pg_strom=# SELECT * FROM pgstrom.ccache_info ;\n database_id | table_id | block_nr | nitems  |  length   |             ctime             |             atime\n-------------+----------+----------+---------+-----------+-------------------------------+-------------------------------\n       13323 | 25887    |   622592 | 1966080 | 121897472 | 2018-02-18 14:31:30.898389+09 | 2018-02-18 14:38:43.711287+09\n       13323 | 25887    |   425984 | 1966080 | 121897472 | 2018-02-18 14:28:39.356952+09 | 2018-02-18 14:38:43.514788+09\n       13323 | 25887    |    98304 | 1966080 | 121897472 | 2018-02-18 14:28:01.542261+09 | 2018-02-18 14:38:42.930281+09\n         :       :             :         :          :                :                               :\n       13323 | 25887    |    16384 | 1963079 | 121711472 | 2018-02-18 14:28:00.647021+09 | 2018-02-18 14:38:42.909112+09\n       13323 | 25887    |   737280 | 1966080 | 121897472 | 2018-02-18 14:34:32.249899+09 | 2018-02-18 14:38:43.882029+09\n       13323 | 25887    |   770048 | 1966080 | 121897472 | 2018-02-18 14:28:57.321121+09 | 2018-02-18 14:38:43.90157+09\n(50 rows)\n\n\n\n\nCheck usage of columnar cache\n\n\nYou can check whether a particular query may reference columnar cache, or not, using \nEXPLAIN\n command.\n\n\nThe query below joins the table \nt0\n and \nt1\n, and the \nCustom Scan (GpuJoin)\n which contains scan on the \nt0\n shows \nCCache: enabled\n.\nIt means columnar cache may be referenced at the scan on \nt0\n, however, it is not certain whether it is actually referenced until query execution. Columnar cache may be invalidated by the concurrent updates.\n\n\npostgres=# EXPLAIN SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid \n 1000;\n\n                                  QUERY PLAN\n-------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n   Outer Scan Filter: (aid \n 1000)\n   Depth 1: GpuHashJoin  (nrows 1029348...1029348)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size: 10.78MB)\n   CCache: enabled\n   -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(10 rows)\n\n\n\n\nEXPLAIN ANALYZE\n command tells how many times columnar cache is referenced during the query execution.\n\n\nAfter the execution of this query, \nCustom Scan (GpuJoin)\n which contains scan on \nt0\n shows \nCCache Hits: 50\n.\nIt means that columnar cache is referenced 50 times. Because the chunk size of columnar cache is 128MB, storage access is replaced to the columnar cache by 6.4GB.\n\n\npostgres=# EXPLAIN ANALYZE SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid \n 1000;\n\n                                    QUERY PLAN\n\n-------------------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n                              (actual time=91.766..723.549 rows=1000224 loops=1)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n                   (actual time=7.129..398.270 rows=100000000 loops=1)\n   Outer Scan Filter: (aid \n 1000)\n   Rows Removed by Outer Scan Filter: 98999776\n   Depth 1: GpuHashJoin  (plan nrows: 1029348...1029348, actual nrows: 1000224...1000224)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size plan: 10.78MB, exec: 64.00MB)\n   CCache Hits: 50\n   -\n  Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n                       (actual time=0.011..13.542 rows=100000 loops=1)\n Planning time: 23.390 ms\n Execution time: 1409.073 ms\n(13 rows)", 
            "title": "Columnar Cache"
        }, 
        {
            "location": "/ccache/#overview", 
            "text": "PG-Strom has one another feature related to storage to supply processors data stream.  In-memory columnar cache reads data blocks of the target table, convert the row-format of PostgreSQL to columnar format which is suitable for summary and analytics, and cache them on memory.  This feature requires no special hardware like SSD-to-GPU Direct SQL Execution, on the other hands, RAM capacity is still smaller than SSD, so this feature is suitable to handle \"not a large scale data set\" up to 60%-75% of the system RAM size.  This feature is not \"a columnar store\". It means cached and converted data blocks are flashed once PostgreSQL server process has restarted for example. When any cached rows get updated, PG-Strom invalidates the columnar cache block which contains the updated rows.\nThis design on the basis that columnar format is vulnerable to updating workloads. If we try to update columnar-store with keeping consistency towards update of row-store, huge degradation of write performance is not avoidable. On the other hands, it is lightweight operation to invalidate the columnar cache block which contains the updated row.\nPG-Strom can switch GPU kernels to be invoked for row- or columnar-format according to format of the loading data blocks. So, it works flexibly, even if a columnar cache block gets invalidated thus PG-Strom has to load data blocks from the shared buffer of PostgreSQL.", 
            "title": "Overview"
        }, 
        {
            "location": "/ccache/#system-setup", 
            "text": "", 
            "title": "System Setup"
        }, 
        {
            "location": "/ccache/#location-of-the-columnar-cache", 
            "text": "The  pg_strom.ccache_base_dir  parameter allows to specify the path to store the columnar cache. The default is  /dev/shm  where general Linux distribution mounts  tmpfs  filesystem, so files under the directory are \"volatile\", with no backing store.  Custom configuration of the parameter enables to construct columnar cache on larger and reasonably fast storage, like NVMe-SSD, as backing store. However, note that update of the cached rows invalidates whole of the chunk (128MB) which contains the updated rows. It may lead unexpected performance degradation, if workloads have frequent read / write involving I/O operations.", 
            "title": "Location of the columnar cache"
        }, 
        {
            "location": "/ccache/#source-table-configuration", 
            "text": "DBA needs to specify the target tables to build columnar cache.  A SQL function  pgstrom_ccache_enabled(regclass)  adds the supplied table as target to build columnar cache.\nOther way round, a SQL function  pgstrom_ccache_disabled(regclass)  drops the supplied table from the target to build.  Internally, it is implemented as a special trigger function which invalidate columnar cache on write to the target tables.\nIt means we don't build columnar cache on the tables which have no way to invalidate columnar cache.  postgres=# select pgstrom_ccache_enabled('t0');\n pgstrom_ccache_enabled\n------------------------\n enabled\n(1 row)", 
            "title": "Source Table Configuration"
        }, 
        {
            "location": "/ccache/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/ccache/#loading-the-columnar-cache", 
            "text": "The  pgstrom_ccache_prewarm()  loads the specified table onto the columnar cache.\nIf specified table has the above trigger function, it tries to load the table contents until it reached to the table end or exceeds to the configured total size of columnar cache.  postgres=# select pgstrom_ccache_prewarm('t0');\n pgstrom_ccache_prewarm\n------------------------\n                     35\n(1 row)", 
            "title": "Loading the columnar cache"
        }, 
        {
            "location": "/ccache/#check-status-of-columnar-cache", 
            "text": "pgstrom.ccache_info  provides the status of the current columnar cache.  You can check the table, block number, cache creation time and last access time per chunk.  contrib_regression_pg_strom=# SELECT * FROM pgstrom.ccache_info ;\n database_id | table_id | block_nr | nitems  |  length   |             ctime             |             atime\n-------------+----------+----------+---------+-----------+-------------------------------+-------------------------------\n       13323 | 25887    |   622592 | 1966080 | 121897472 | 2018-02-18 14:31:30.898389+09 | 2018-02-18 14:38:43.711287+09\n       13323 | 25887    |   425984 | 1966080 | 121897472 | 2018-02-18 14:28:39.356952+09 | 2018-02-18 14:38:43.514788+09\n       13323 | 25887    |    98304 | 1966080 | 121897472 | 2018-02-18 14:28:01.542261+09 | 2018-02-18 14:38:42.930281+09\n         :       :             :         :          :                :                               :\n       13323 | 25887    |    16384 | 1963079 | 121711472 | 2018-02-18 14:28:00.647021+09 | 2018-02-18 14:38:42.909112+09\n       13323 | 25887    |   737280 | 1966080 | 121897472 | 2018-02-18 14:34:32.249899+09 | 2018-02-18 14:38:43.882029+09\n       13323 | 25887    |   770048 | 1966080 | 121897472 | 2018-02-18 14:28:57.321121+09 | 2018-02-18 14:38:43.90157+09\n(50 rows)", 
            "title": "Check status of columnar cache"
        }, 
        {
            "location": "/ccache/#check-usage-of-columnar-cache", 
            "text": "You can check whether a particular query may reference columnar cache, or not, using  EXPLAIN  command.  The query below joins the table  t0  and  t1 , and the  Custom Scan (GpuJoin)  which contains scan on the  t0  shows  CCache: enabled .\nIt means columnar cache may be referenced at the scan on  t0 , however, it is not certain whether it is actually referenced until query execution. Columnar cache may be invalidated by the concurrent updates.  postgres=# EXPLAIN SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid   1000;\n\n                                  QUERY PLAN\n-------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n   Outer Scan Filter: (aid   1000)\n   Depth 1: GpuHashJoin  (nrows 1029348...1029348)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size: 10.78MB)\n   CCache: enabled\n   -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n(10 rows)  EXPLAIN ANALYZE  command tells how many times columnar cache is referenced during the query execution.  After the execution of this query,  Custom Scan (GpuJoin)  which contains scan on  t0  shows  CCache Hits: 50 .\nIt means that columnar cache is referenced 50 times. Because the chunk size of columnar cache is 128MB, storage access is replaced to the columnar cache by 6.4GB.  postgres=# EXPLAIN ANALYZE SELECT id,ax FROM t0 NATURAL JOIN t1 WHERE aid   1000;\n\n                                    QUERY PLAN\n\n-------------------------------------------------------------------------------------------\n Custom Scan (GpuJoin) on t0  (cost=12398.65..858048.45 rows=1029348 width=12)\n                              (actual time=91.766..723.549 rows=1000224 loops=1)\n   GPU Projection: t0.id, t1.ax\n   Outer Scan: t0  (cost=10277.55..864623.44 rows=1029348 width=8)\n                   (actual time=7.129..398.270 rows=100000000 loops=1)\n   Outer Scan Filter: (aid   1000)\n   Rows Removed by Outer Scan Filter: 98999776\n   Depth 1: GpuHashJoin  (plan nrows: 1029348...1029348, actual nrows: 1000224...1000224)\n            HashKeys: t0.aid\n            JoinQuals: (t0.aid = t1.aid)\n            KDS-Hash (size plan: 10.78MB, exec: 64.00MB)\n   CCache Hits: 50\n   -   Seq Scan on t1  (cost=0.00..1935.00 rows=100000 width=12)\n                       (actual time=0.011..13.542 rows=100000 loops=1)\n Planning time: 23.390 ms\n Execution time: 1409.073 ms\n(13 rows)", 
            "title": "Check usage of columnar cache"
        }, 
        {
            "location": "/gstore_fdw/", 
            "text": "GPU Memory Store(gstore_fdw)\n\n\n\nOverview\n\n\nUsually, PG-Strom uses GPU device memory for temporary purpose only. It allocates a certain amount of device memory needed for query execution, then transfers data blocks and launch GPU kernel to process SQL workloads. Once GPU kernel gets finished, these device memory regison shall be released soon, to re-allocate unused device memory for other workloads.\n\n\nThis design allows concurrent multiple session or scan workloads on the tables larger than GPU device memory. It may not be optimal depending on circumstances.\n\n\nA typical example is, repeated calculation under various conditions for data with a scale large enough to fit in the GPU device memory, not so large. This applies to workloads such as machine-learning, pattern matching or similarity search.\n\n\nFor modern GPUs, it is not so difficult to process a few gigabytes data on memory at most, but it is a costly process to setup data to be loaded onto GPU device memory and transfer them.\n\n\nIn addition, since variable length data in PostgreSQL has size limitation up to 1GB, it restricts the data format when it is givrn as an argument of PL/CUDA function, even if the data size itself is sufficient in the GPU device memory.\n\n\nGPU memory store (gstore_fdw) is a feature to preserve GPU device memory and to load data to the memory preliminary.\nIt makes unnecessary to setup arguments and load for each invocation of PL/CUDA function, and eliminates 1GB limitation of variable length data because it allows GPU device memory allocation up to the capacity.\n\n\nAs literal, gstore_fdw is implemented using foreign-data-wrapper of PostgreSQL.\nYou can modify the data structure on GPU device memory using \nINSERT\n, \nUPDATE\n or \nDELETE\n commands on the foreign table managed by gstore_fdw. In the similar way, you can also read the data using \nSELECT\n command.\n\n\nPL/CUDA function can reference the data stored onto GPU device memory through the foreign table.\nRight now, GPU programs which is transparently generated from SQL statement cannot reference this device memory region, however, we plan to enhance the feature in the future release.\n\n\n\n\nSetup\n\n\nUsually it takes the 3 steps below to create a foreign table.\n\n\n\n\nDefine a foreign-data-wrapper using \nCREATE FOREIGN DATA WRAPPER\n command\n\n\nDefine a foreign server using \nCREATE SERVER\n command\n\n\nDefine a foreign table using \nCREATE FOREIGN TABLE\n command\n\n\n\n\nThe first 2 steps above are included in the \nCREATE EXTENSION pg_strom\n command. All you need to run individually is \nCREATE FOREIGN TABLE\n command last.\n\n\nCREATE FOREIGN TABLE ft (\n    id int,\n    signature smallint[] OPTIONS (compression 'pglz')\n)\nSERVER gstore_fdw OPTIONS(pinning '0', format 'pgstrom');\n\n\n\n\nYou can specify some options on creation of foreign table using \nCREATE FOREIGN TABLE\n command.\n\n\nSERVER gstore_fdw\n is a mandatory option. It indicates the new foreign table is managed by gstore_fdw.\n\n\nThe options below are supported in the \nOPTIONS\n clause.\n\n\n\n\n\n\n\n\nname\n\n\ntarget\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\npinning\n\n\ntable\n\n\nSpecifies device number of the GPU where device memory is preserved.\n\n\n\n\n\n\nformat\n\n\ntable\n\n\nSpecifies the internal data format on GPU device memory. Default is \npgstrom\n\n\n\n\n\n\ncompression\n\n\ncolumn\n\n\nSpecifies whether variable length data is compressed, or not. Default is uncompressed.\n\n\n\n\n\n\n\n\nRight now, only \npgstrom\n is supported for \nformat\n option. It is identical data format with what PG-Strom uses for in-memory columnar cache.\nIn most cases, no need to pay attention to internal data format on writing / reading GPU data store using SQL. On the other hands, you need to consider when you program PL/CUDA function or share the GPU device memory with external applications using IPC handle.\n\n\nRight now, only \npglz\n is supported for \ncompression\n option. This compression logic adopts an identical data format and algorithm used by PostgreSQL to compress variable length data larger than its threshold.\nIt can be decompressed by GPU internal function \npglz_decompress()\n from PL/CUDA function. Due to the characteristics of the compression algorithm, it is valuable to represent sparse matrix that is mostly zero.\n\n\nOperations\n\n\nLoading data\n\n\nLike normal tables, you can write GPU device memory on behalf of the foreign table using \nINSERT\n, \nUPDATE\n and \nDELETE\n command.\n\n\nNote that gstore_fdw acquires \nSHARE UPDATE EXCLUSIVE\n lock on the beginning of these commands. It means only single transaction can update the gstore_fdw foreign table at a certain point.\nIt is a trade-off. We don't need to check visibility per record when PL/CUDA function references gstore_fdw foreign table.\n\n\nAny contents written to the gstore_fdw foreign table is not visible to other sessions until transaction getting committed, like regular tables.\nThis is a significant feature to ensure atomicity of transaction, however, it also means the older revision of gstore_fdw foreign table contents must be kept on the GPU device memory until any concurrent transaction which may reference the older revision gets committed or aborted.\n\n\nSo, even though you can run \nINSERT\n, \nUPDATE\n or \nDELETE\n commands as if it is regular tables, you should avoidto update several rows then commit transaction many times. Basically, \nINSERT\n of massive rows at once (bulk loading) is recommended.\n\n\nUnlike regular tables, contents of the gstore_fdw foreign table is vollatile. So, it is very easy to loose contents of the gstore_fdw foreign table by power-down or PostgreSQL restart. So, what we load onto gstore_fdw foreign table should be reconstructable by other data source.\n\n\nChecking the memory consumption\n\n\nSee \npgstrom.gstore_fdw_chunk_info\n system view to see amount of the device memory consumed by gstore_fdw.\n\n\npostgres=# select * from pgstrom.gstore_fdw_chunk_info ;\n database_oid | table_oid | revision | xmin | xmax | pinning | format  |  rawsize  |  nitems\n--------------+-----------+----------+------+------+---------+---------+-----------+----------\n        13806 |     26800 |        3 |    2 |    0 |       0 | pgstrom | 660000496 | 15000000\n        13806 |     26797 |        2 |    2 |    0 |       0 | pgstrom | 440000496 | 10000000\n(2 rows)\n\n\n\n\nBy \nnvidia-smi\n command, you can check how much device memory is consumed for each GPU device.\n\"PG-Strom GPU memory keeper\" process actually keeps and manages the device memory area acquired by Gstore_fdw. In this example, 1211MB is preliminary allocated for total of the above rawsize (about 1100MB) and CUDA internal usage.\n\n\n$ nvidia-smi\nWed Apr  4 15:11:50 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   39C    P0    52W / 250W |   1221MiB / 22919MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      6885      C   ...bgworker: PG-Strom GPU memory keeper     1211MiB |\n+-----------------------------------------------------------------------------+\n\n\n\n\nInternal Data Format\n\n\nSee the notes for details of the internal data format when gstore_fdw write on GPU device memory.\n\n\n\n\nDetail of the \npgstrom\n format\n\n\n\n\nInter-process Data Collaboration\n\n\nCUDA provides special APIs \ncuIpcGetMemHandle()\n and \ncuIpcOpenMemHandle()\n.\nThe first allows to get a unique identifier of GPU device memory allocated by applications. The other one allows to reference a shared GPU device memory region from other applications. In the other words, it supports something like a shared memory on the host system.\n\n\nThis unique identifier is \nCUipcMemHandle\n object; which is simple binary data in 64bytes.\nThis session introduces SQL functions which exchange GPU device memory with other applications using \nCUipcMemHandle\n identifier.\n\n\nSQL Functions to\n\n\ngstore_export_ipchandle(reggstore)\n\n\nThis function gets \nCUipcMemHandle\n identifier of the GPU device memory which is preserved by gstore_fdw foreign table, then returns as a binary data in \nbytea\n type.\nIf foreign table is empty and has no GPU device memory, it returns NULL.\n\n\n\n\n1st arg(\nftable_oid\n): OID of the foreign table. Because it is \nreggstore\n type, you can specify the foreign table by name string.\n\n\nresult: \nCUipcMemHandle\n identifier in the bytea type.\n\n\n\n\n# select gstore_export_ipchandle('ft');\n                                                      gstore_export_ipchandle\n\n------------------------------------------------------------------------------------------------------------------------------------\n \\xe057880100000000de3a000000000000904e7909000000000000800900000000000000000000000000020000000000005c000000000000001200d0c10101005c\n(1 row)\n\n\n\n\nlo_import_gpu(int, bytea, bigint, bigint, oid=0)\n\n\nThis function temporary opens the GPU device memory region acquired by external applications, then read this region and writes out as a largeobject of PostgreSQL.\nIf largeobject already exists, its contents is replaced by the data read from the GPU device memory. It keeps owner and permission configuration. Elsewhere, it creates a new largeobject, then write out the data which is read from GPU device memory.\n\n\n\n\n1st arg(\ndevice_nr\n): GPU device number where device memory is acquired\n\n\n2nd arg(\nipc_mhandle\n): \nCUipcMemHandle\n identifier in bytea type\n\n\n3rd(\noffset\n): offset of the head position to read, from the GPU device memory region.\n\n\n4th(\nlength\n): size to read in bytes\n\n\n5th(\nloid\n): OID of the largeobject to be written. 0 is assumed, if no valid value is supplied.\n\n\nresult: OID of the written largeobject\n\n\n\n\nlo_export_gpu(oid, int, bytea, bigint, bigint)\n\n\n\n\n1st arg(\nloid\n): OID of the largeobject to be read\n\n\n2nd arg(\ndevice_nr\n): GPU device number where device memory is acquired\n\n\n3rd arg(\nipc_mhandle\n): \nCUipcMemHandle\n identifier in bytea type\n\n\n4th arg(\noffset\n): offset of the head position to write, from the GPU device memory region.\n\n\n5th arg(\nlength\n): size to write in bytes\n\n\nresult: Length of bytes actually written. If length of the largeobject is less then \nlength\n, it may return the value less than \nlength\n.", 
            "title": "Gstore_fdw"
        }, 
        {
            "location": "/gstore_fdw/#overview", 
            "text": "Usually, PG-Strom uses GPU device memory for temporary purpose only. It allocates a certain amount of device memory needed for query execution, then transfers data blocks and launch GPU kernel to process SQL workloads. Once GPU kernel gets finished, these device memory regison shall be released soon, to re-allocate unused device memory for other workloads.  This design allows concurrent multiple session or scan workloads on the tables larger than GPU device memory. It may not be optimal depending on circumstances.  A typical example is, repeated calculation under various conditions for data with a scale large enough to fit in the GPU device memory, not so large. This applies to workloads such as machine-learning, pattern matching or similarity search.  For modern GPUs, it is not so difficult to process a few gigabytes data on memory at most, but it is a costly process to setup data to be loaded onto GPU device memory and transfer them.  In addition, since variable length data in PostgreSQL has size limitation up to 1GB, it restricts the data format when it is givrn as an argument of PL/CUDA function, even if the data size itself is sufficient in the GPU device memory.  GPU memory store (gstore_fdw) is a feature to preserve GPU device memory and to load data to the memory preliminary.\nIt makes unnecessary to setup arguments and load for each invocation of PL/CUDA function, and eliminates 1GB limitation of variable length data because it allows GPU device memory allocation up to the capacity.  As literal, gstore_fdw is implemented using foreign-data-wrapper of PostgreSQL.\nYou can modify the data structure on GPU device memory using  INSERT ,  UPDATE  or  DELETE  commands on the foreign table managed by gstore_fdw. In the similar way, you can also read the data using  SELECT  command.  PL/CUDA function can reference the data stored onto GPU device memory through the foreign table.\nRight now, GPU programs which is transparently generated from SQL statement cannot reference this device memory region, however, we plan to enhance the feature in the future release.", 
            "title": "Overview"
        }, 
        {
            "location": "/gstore_fdw/#setup", 
            "text": "Usually it takes the 3 steps below to create a foreign table.   Define a foreign-data-wrapper using  CREATE FOREIGN DATA WRAPPER  command  Define a foreign server using  CREATE SERVER  command  Define a foreign table using  CREATE FOREIGN TABLE  command   The first 2 steps above are included in the  CREATE EXTENSION pg_strom  command. All you need to run individually is  CREATE FOREIGN TABLE  command last.  CREATE FOREIGN TABLE ft (\n    id int,\n    signature smallint[] OPTIONS (compression 'pglz')\n)\nSERVER gstore_fdw OPTIONS(pinning '0', format 'pgstrom');  You can specify some options on creation of foreign table using  CREATE FOREIGN TABLE  command.  SERVER gstore_fdw  is a mandatory option. It indicates the new foreign table is managed by gstore_fdw.  The options below are supported in the  OPTIONS  clause.     name  target  description      pinning  table  Specifies device number of the GPU where device memory is preserved.    format  table  Specifies the internal data format on GPU device memory. Default is  pgstrom    compression  column  Specifies whether variable length data is compressed, or not. Default is uncompressed.     Right now, only  pgstrom  is supported for  format  option. It is identical data format with what PG-Strom uses for in-memory columnar cache.\nIn most cases, no need to pay attention to internal data format on writing / reading GPU data store using SQL. On the other hands, you need to consider when you program PL/CUDA function or share the GPU device memory with external applications using IPC handle.  Right now, only  pglz  is supported for  compression  option. This compression logic adopts an identical data format and algorithm used by PostgreSQL to compress variable length data larger than its threshold.\nIt can be decompressed by GPU internal function  pglz_decompress()  from PL/CUDA function. Due to the characteristics of the compression algorithm, it is valuable to represent sparse matrix that is mostly zero.", 
            "title": "Setup"
        }, 
        {
            "location": "/gstore_fdw/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/gstore_fdw/#loading-data", 
            "text": "Like normal tables, you can write GPU device memory on behalf of the foreign table using  INSERT ,  UPDATE  and  DELETE  command.  Note that gstore_fdw acquires  SHARE UPDATE EXCLUSIVE  lock on the beginning of these commands. It means only single transaction can update the gstore_fdw foreign table at a certain point.\nIt is a trade-off. We don't need to check visibility per record when PL/CUDA function references gstore_fdw foreign table.  Any contents written to the gstore_fdw foreign table is not visible to other sessions until transaction getting committed, like regular tables.\nThis is a significant feature to ensure atomicity of transaction, however, it also means the older revision of gstore_fdw foreign table contents must be kept on the GPU device memory until any concurrent transaction which may reference the older revision gets committed or aborted.  So, even though you can run  INSERT ,  UPDATE  or  DELETE  commands as if it is regular tables, you should avoidto update several rows then commit transaction many times. Basically,  INSERT  of massive rows at once (bulk loading) is recommended.  Unlike regular tables, contents of the gstore_fdw foreign table is vollatile. So, it is very easy to loose contents of the gstore_fdw foreign table by power-down or PostgreSQL restart. So, what we load onto gstore_fdw foreign table should be reconstructable by other data source.", 
            "title": "Loading data"
        }, 
        {
            "location": "/gstore_fdw/#checking-the-memory-consumption", 
            "text": "See  pgstrom.gstore_fdw_chunk_info  system view to see amount of the device memory consumed by gstore_fdw.  postgres=# select * from pgstrom.gstore_fdw_chunk_info ;\n database_oid | table_oid | revision | xmin | xmax | pinning | format  |  rawsize  |  nitems\n--------------+-----------+----------+------+------+---------+---------+-----------+----------\n        13806 |     26800 |        3 |    2 |    0 |       0 | pgstrom | 660000496 | 15000000\n        13806 |     26797 |        2 |    2 |    0 |       0 | pgstrom | 440000496 | 10000000\n(2 rows)  By  nvidia-smi  command, you can check how much device memory is consumed for each GPU device.\n\"PG-Strom GPU memory keeper\" process actually keeps and manages the device memory area acquired by Gstore_fdw. In this example, 1211MB is preliminary allocated for total of the above rawsize (about 1100MB) and CUDA internal usage.  $ nvidia-smi\nWed Apr  4 15:11:50 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           Off  | 00000000:02:00.0 Off |                    0 |\n| N/A   39C    P0    52W / 250W |   1221MiB / 22919MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      6885      C   ...bgworker: PG-Strom GPU memory keeper     1211MiB |\n+-----------------------------------------------------------------------------+", 
            "title": "Checking the memory consumption"
        }, 
        {
            "location": "/gstore_fdw/#internal-data-format", 
            "text": "See the notes for details of the internal data format when gstore_fdw write on GPU device memory.   Detail of the  pgstrom  format", 
            "title": "Internal Data Format"
        }, 
        {
            "location": "/gstore_fdw/#inter-process-data-collaboration", 
            "text": "CUDA provides special APIs  cuIpcGetMemHandle()  and  cuIpcOpenMemHandle() .\nThe first allows to get a unique identifier of GPU device memory allocated by applications. The other one allows to reference a shared GPU device memory region from other applications. In the other words, it supports something like a shared memory on the host system.  This unique identifier is  CUipcMemHandle  object; which is simple binary data in 64bytes.\nThis session introduces SQL functions which exchange GPU device memory with other applications using  CUipcMemHandle  identifier.", 
            "title": "Inter-process Data Collaboration"
        }, 
        {
            "location": "/gstore_fdw/#sql-functions-to", 
            "text": "", 
            "title": "SQL Functions to"
        }, 
        {
            "location": "/gstore_fdw/#gstore_export_ipchandlereggstore", 
            "text": "This function gets  CUipcMemHandle  identifier of the GPU device memory which is preserved by gstore_fdw foreign table, then returns as a binary data in  bytea  type.\nIf foreign table is empty and has no GPU device memory, it returns NULL.   1st arg( ftable_oid ): OID of the foreign table. Because it is  reggstore  type, you can specify the foreign table by name string.  result:  CUipcMemHandle  identifier in the bytea type.   # select gstore_export_ipchandle('ft');\n                                                      gstore_export_ipchandle\n\n------------------------------------------------------------------------------------------------------------------------------------\n \\xe057880100000000de3a000000000000904e7909000000000000800900000000000000000000000000020000000000005c000000000000001200d0c10101005c\n(1 row)", 
            "title": "gstore_export_ipchandle(reggstore)"
        }, 
        {
            "location": "/gstore_fdw/#lo_import_gpuint-bytea-bigint-bigint-oid0", 
            "text": "This function temporary opens the GPU device memory region acquired by external applications, then read this region and writes out as a largeobject of PostgreSQL.\nIf largeobject already exists, its contents is replaced by the data read from the GPU device memory. It keeps owner and permission configuration. Elsewhere, it creates a new largeobject, then write out the data which is read from GPU device memory.   1st arg( device_nr ): GPU device number where device memory is acquired  2nd arg( ipc_mhandle ):  CUipcMemHandle  identifier in bytea type  3rd( offset ): offset of the head position to read, from the GPU device memory region.  4th( length ): size to read in bytes  5th( loid ): OID of the largeobject to be written. 0 is assumed, if no valid value is supplied.  result: OID of the written largeobject", 
            "title": "lo_import_gpu(int, bytea, bigint, bigint, oid=0)"
        }, 
        {
            "location": "/gstore_fdw/#lo_export_gpuoid-int-bytea-bigint-bigint", 
            "text": "1st arg( loid ): OID of the largeobject to be read  2nd arg( device_nr ): GPU device number where device memory is acquired  3rd arg( ipc_mhandle ):  CUipcMemHandle  identifier in bytea type  4th arg( offset ): offset of the head position to write, from the GPU device memory region.  5th arg( length ): size to write in bytes  result: Length of bytes actually written. If length of the largeobject is less then  length , it may return the value less than  length .", 
            "title": "lo_export_gpu(oid, int, bytea, bigint, bigint)"
        }, 
        {
            "location": "/plcuda/", 
            "text": "This chapter introduces the way to implement GPU executable native program as SQL functions, using PL/CUDA procedural language.\n\n\nPL/CUDA Overview\n\n\nPG-Strom internally constructs GPU programs by CUDA language, according to the supplied SQL, then generates GPU's native binary using just-in-time compile. CUDA is a programming environment provided by NVIDIA. It allows implementing parallel program which is executable on GPU device, using C-like statement. This transformation process from SQL statement to CUDA program is an internal process, thus, no need to pay attention what GPU programs are generated and executed from the standpoint of users.\n\n\nOn the other hands, PostgreSQL supports to add programming language to implement SQL functions by \nCREATE LANGUAGE\n statement. PL/CUDA is a language handler to supports \nCREATE LANGUAGE\n command. It also allows users to run arbitrary GPU programs manually implemented as SQL functions, but not only GPU programs automatically generated by PG-Strom based on SQL.\n\n\nIts argument can take the data types supported by PG-Strom, like numeric, text, or array-matrix data type. These arguments are implicitly loaded onto GPU device memory by the PL/CUDA infrastructure, so users don't need to pay attention for data loading between the database and GPU devices. In a similar fashion, the return value of PL/CUDA function (including the case of variable length data type) will be written back to CPU from GPU, then decode to the result of SQL function.\n\n\nYou can also use foreign tables defined with \ngstore_fdw\n as arguments of PL/CUDA function. In this case, no need to load the data onto GPU for each invocation because foreign table already keeps the data, and available to use larger data than 1GB which is a restriction of variable length data in PostgreSQL.\n\n\nTherefore, users can focus on productive tasks like implementation of statistical analysis, code optimization and so on, without routine process like data input/output between GPU and databases.\n\n\n\n\nOnce a PL/CUDA function is declared using \nCREATE FUNCTION\n, it generates a CUDA program source code that embeds the definition of this function, then build it for the target GPU device.\nThis CUDA program is almost identical to usual GPU software based on CUDA runtime, except for the auxiliary code to receive arguments of SQL function and to write back its results. It also allows to include/link some libraries for CUDA device runtime.\n\n\nNative CUDA programs implemented by PL/CUDA are executed as child-processes of PostgreSQL backend.\nTherefore, it has independent address space and OS/GPU resources from PostgreSQL.\nCUDA program contains host code for the host system and device code to be executed on GPU devices.\nThe host code can execute any logic we can program using C-language, so we restrict only database superuser can define PL/CUDA function from the standpoint of security.\n\n\nBelow is an example of simple PL/CUDA function. This function takes two same length \nreal[]\n array as arguments, then returns its dot product in \nfloat\n data type.\n\n\nCREATE OR REPLACE FUNCTION\ngpu_dot_product(real[], real[])\nRETURNS float\nAS $$\n#plcuda_decl\n#include \ncuda_matrix.h\n\n\nKERNEL_FUNCTION_MAXTHREADS(void)\ngpu_dot_product(double *p_dot,\n                VectorTypeFloat *X,\n                VectorTypeFloat *Y)\n{\n    size_t      index = get_global_id();\n    size_t      nitems = X-\nheight;\n    float       v[MAXTHREADS_PER_BLOCK];\n    float       sum;\n\n    if (index \n nitems)\n        v[get_local_id()] = X-\nvalues[index] * Y-\nvalues[index];\n    else\n        v[get_local_id()] = 0.0;\n\n    sum = pgstromTotalSum(v, MAXTHREADS_PER_BLOCK);\n    if (get_local_id() == 0)\n        atomicAdd(p_dot, (double)sum);\n    __syncthreads();\n}\n#plcuda_begin\n{\n    size_t      nitems;\n    int         blockSz;\n    int         gridSz;\n    double     *dot;\n    cudaError_t rc;\n\n    if (!VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg1, PG_FLOAT4OID) ||\n        !VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg2, PG_FLOAT4OID))\n        EEXIT(\narguments are not vector like array\n);\n    nitems = ARRAY_VECTOR_HEIGHT(arg1);\n    if (nitems != ARRAY_VECTOR_HEIGHT(arg2))\n        EEXIT(\nlength of arguments mismatch\n);\n\n    rc = cudaMallocManaged(\ndot, sizeof(double));\n    if (rc != cudaSuccess)\n        CUEXIT(rc, \nfailed on cudaMallocManaged\n);\n    memset(dot, 0, sizeof(double));\n\n    blockSz = MAXTHREADS_PER_BLOCK;\n    gridSz = (nitems + MAXTHREADS_PER_BLOCK - 1) / MAXTHREADS_PER_BLOCK;\n    gpu_dot_product\ngridSz,blockSz\n(dot,\n                                        (VectorTypeFloat *)arg1,\n                                        (VectorTypeFloat *)arg2);\n    rc = cudaStreamSynchronize(NULL);\n    if (rc != cudaSuccess)\n        CUEXIT(rc, \nfailed on cudaStreamSynchronize\n);\n\n    return *dot;\n}\n#plcuda_end\n$$ LANGUAGE 'plcuda';\n\n\n\n\nPL/CUDA infrastructure makes entrypoint function of CUDA program by the block between \n#plcuda_begin\n and \n#plcuda_end\n with extra code to exchange arguments of SQL function.\nThe portion enclosed by \n#plcuda_decl\n and \n#plcuda_begin\n is a block for declaration of GPU device functions and other host functions. It is placed prior to the entrypoint above.\n\n\nAt the entrypoint of the CUDA program, you can refer the arguments of SQL function using \narg1\n, `arg2, and so on.\n\n\nIn the above example, the \narg1\n and \narg2\n, \nreal[]\n array type, are passed to the entrypoint, then \nVALIDATE_ARRAY_VECTOR_TYPE_STRICT\n macro checks whether it is 1-dimensional array of 32bit floating-point values without NULL.\n\n\nDitto with return value, the entrypoint returns a value in CUDA C representation corresponding to the SQL data type.\nIf entrypoint does not return any value (or, it exits the program with status code 1 by \nexit()\n), it is considered PL/CUDA function returns \nNULL\n. \n\n\nThe above sample program validates the array of \nreal\n values passed from SQL function, then it allocates the result buffer by \ncudaMallocManaged\n, and invokes \ngpu_dot_product\n, a GPU kernel function, to compute dot product with two vectors.\n\n\nThe result of this function is below. It computes the dot product of two vectors which contain 10,000 items randomly generated.\n\n\npostgres=# SELECT gpu_dot_product(array_matrix(random()::real),\n                                  array_matrix(random()::real))\n             FROM generate_series(1,10000);\n gpu_dot_product\n------------------\n 3.71461999509484\n(1 row)\n\n\n\n\nPL/CUDA Structure\n\n\nFunction declaration of PL/CUDA is consists of two code blocks split by the directives of \n#plcuda_decl\n, \n#plcuda_begin\n and \n#plcuda_end\n. Users can put their custom code on the code blocks according to the purpose, then PL/CUDA language handler reconstruct them into single source file with extra logic to exchange function arguments and results.\n\n\n#plcuda_decl\n  [...any declarations...]\n#plcuda_begin\n  [...host code in the entrypoint...]\n#plcuda_end\n\n\n\n\nThe code block, begins from \n#plcuda_decl\n, can have declaration of \n__host__\n and \n__device__\n functions and variables for CUDA C.\nThis code block locates in front of the entrypoint function which contains the code block between \n#plcuda_begin\n and \n#plcuda_end\n at the source file eventually constructed.\n\n\nIf external header files are included using \n#include\n statement of CUDA C, put the statement on this code block.\n\n\nThe code block between \n#plcuda_begin\n and \n#plcuda_end\n is embedded to a part of entrypoint function. Therefore, it does not describe function name, arguments definition and so on.\nPrior to execution of the code block, the entrypoint function receives arguments of the SQL function from PostgreSQL backend, and set up \narg1\n, \narg2\n, ... variables for further references.\n\n\nThese variables have the following CUDA C representation according to SQL data types.\n\n\n\n\n\n\n\n\nSQL data type\n\n\nCUDA C data type\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nreggstore\n\n\nvoid *\n\n\nOID of Gstore_fdw foreign table\n\n\n\n\n\n\nreal\n\n\nfloat\n\n\n32bit floating point\n\n\n\n\n\n\nfloat\n\n\ndouble\n\n\n64bit floating point\n\n\n\n\n\n\nOther inline data types\n\n\nDatum\n\n\nint\n, \ndate\n, ...\n\n\n\n\n\n\nFixed-length value by reference\n\n\nvoid *\n\n\nuuid\n, ...\n\n\n\n\n\n\nVariable-length value (varlena)\n\n\nvarlena *\n\n\ntext\n, \nreal[]\n, ...\n\n\n\n\n\n\n\n\nPL/CUDA language handler constructs a single CUDA C source file from the code blocks above, then builds it once by \nnvcc\n compiler at declaration or execution time.\nIf it contains any \n#plcuda_include\n directive, its source code is not fixed until execution time, so built at the execution time only. In case when identical CUDA program is already pre-built, we can reuse it without rebuild.\n\n\n\n\nWhen SQL command invokes PL/CUDA function, PL/CUDA language handler launch the pre-built CUDA program, then copies the arguments of SQL function over pipe. These are stored in the argument buffer of the CUDA program, so custom logic can refer them using \narg1\n or \narg2\n variables.\n\n\nThe data types by reference at CUDA C program, like variable-length datum, are initialized as pointers to the argument buffer. It is a managed memory region allocated by \ncudaMallocManaged()\n, these pointers are available without explicit DMA between host system and GPU devices.\n\n\nHere is a special case if argument has \nreggstore\n type. It is actually an OID (32bit integer) of Gstore_Fdw foreign table, however, it is replaced to the reference of GPU device memory acquired by the Gstore_Fdw foreign table if it is supplied as PL/CUDA argument.\n\n\nThe argument is setup to the pointer for \nGstoreIpcMapping\n object. \nGstoreIpcMapping::map\n holds the mapped address of the GPU device memory acquired by the Gstore_Fdw foreign table.\n\nGstoreIpcHandle::device_id\n indicates the device-id of GPU which physically holds the region, and GstoreIpcHandle::rawsize` is raw length of the region.\n\n\ntypedef struct\n{\n    cl_uint     __vl_len;       /* 4B varlena header */\n    cl_short    device_id;      /* GPU device where pinning on */\n    cl_char     format;         /* one of GSTORE_FDW_FORMAT__* */\n    cl_char     __padding__;    /* reserved */\n    cl_long     rawsize;        /* length in bytes */\n    union {\n#ifdef CU_IPC_HANDLE_SIZE\n        CUipcMemHandle      d;  /* CUDA driver API */\n#endif\n#ifdef CUDA_IPC_HANDLE_SIZE\n        cudaIpcMemHandle_t  r;  /* CUDA runtime API */\n#endif\n        char                data[64];\n    } ipc_mhandle;\n} GstoreIpcHandle;\n\ntypedef struct\n{\n    GstoreIpcHandle h; /* IPChandle of Gstore_Fdw */\n    void       *map;    /* mapped device pointer */\n} GstoreIpcMapping;\n\n\n\n\nPL/CUDA function can return its result using \nreturn\n of a CUDA C datum relevant to the SQL data type.\nIn case when no \nreturn\n clause is executed, \nNULL\n pointer is returned if CUDA C data type is pointer, or CUDA program is terminated with status code = 1 by \nexit(1)\n, PL/CUDA function returns \nnull\n to SQL.\n\n\nPL/CUDA References\n\n\nThis section is a reference for PL/CUDA function's directives and related SQL functions.\n\n\nAdvantage and disadvantage of PL/CUDA\n\n\nOn invocation of PL/CUDA function, it launches the relevant CUDA program on behalf of the invocation, then CUDA program initialize per process context of GPU device. The series of operations are never lightweight, so we don't recommend to implement a simple comparison of scalar values using PL/CUDA, and use for full table scan on billion rows.\n\n\nOn the other hands, once GPU device is correctly initialized, it allows to process massive amount of data using several thousands of processor cores on GPU device. Especially, it is suitable for computing intensive workloads, like machine-learning or advanced analytics that approach to the optimal values by repeated calculation for example.\n\n\nAccording to the growth of data size, we need to pay attention how to exchange data with CUDA program.\nPostgreSQL supports array types, and it is easy and simple way to exchange several millions of integer or real values at most.\n\n\nHowever, variable-length datum of PostgreSQL, including the array-types, is restricted to 1GB at a maximum.\nWe need to take a little idea to handle larger data, like separation of data-set. In addition, PostgreSQL backend process set up the argument of SQL functions in single thread, so it takes a certain amount of time to manipulate gigabytes-class memory object.\n\n\nPlease consider usage of Gstore_Fdw foreign-table when data size grows more than several hundreds megabytes.\nOnce you preload the large data-set onto GPU device memory through Gstore_Fdw, no need to set up large arguments on invocation of PL/CUDA function. It also allows to keep larger data than gigabytes, as lond as GPU device memory capacity allows.\n\n\nPL/CUDA Directives\n\n\n#plcuda_decl\n\n\nThis directive begins a code block which contains CUDA C functions and variables with both of \n__host__\n and \n__device__\n attributes. PL/CUDA language handler copies this code block in front of the program entrypoint as is.\n\n\nUse of this directive is optional, however, it makes no sense if here is no declaration of GPU kernel functions to be called from the entrypoint. So, we usually have more than one GPU kernel function.\n\n\n#plcuda_begin\n\n\nThis directive begins a code block which consists a part of the entrypoint of CUDA program.\nThe CUDA program setup the referable \narg1\n, \narg2\n, ... variables according to the arguments of PL/CUDA function, then switch control to the user defined portion. This code block is a host code; we can implement own control logic working on CPU or heavy calculation by GPU kernel invocation.\n\n\nResult of PL/CUDA function can be returned using \nreturn\n statement of CUDA C, according to the function definition.\n\n\n#plcuda_end\n\n\nIt marks end of the kernel function code block. By the way, if a directive to start code block was put inside of the different code block, the current code block is implicitly closed by the \n#plcuda_end\n directive.\n\n\n#plcuda_include \nfunction name\n\n\nThis directive is similar to \n#include\n of CUDA C, however, it injects result of the specified SQL function onto the location where the directive was written.\nThe SQL function should have identical arguments and return \ntext\n data.\n\n\nFor example, when we calculate similarity of massive items, we can generate multiple variant of the algorithm on the fly that is almost equivalent but only distance definitions are different. It makes maintenance of PL/CUDA function simplified.\n\n\n#plcuda_library \nlibrary name\n\n\nIt specifies the library name to be linked when CUDA program is built by \nnvcc\n.\nThe \nlibrary name\n portion is supplied to \nnvcc\n command as \n-l\n option.\nFor example, if \nlibcublas.co\n library is linked, you need to describe \ncublas\n without prefix (\nlib\n) and suffix (\n.so\n).\nRight now, we can specify the libraries only installed on the standard library path of CUDA Toolkit (`/usr/local/cuda/lib64).\n\n\n#plcuda_sanity_check \nfunction\n\n\nIt allows to specify the sanity check function that preliminary checks adequacy of the supplied arguments, prior to GPU kernel launch.\nNo sanity check function is configured on the default.\nUsually, launch of GPU kernel function is heavier task than call of another function on CPU, because it also involves initialization of GPU devices. If supplied arguments have unacceptable values from the specification of the PL/CUDA function, a few thousands or millions (or more in some cases) of GPU kernel threads shall be launched just to check the arguments and return an error status. If sanity check can be applied prior to the launch of GPU kernel function with enough small cost, it is a valuable idea to raise an error using sanity check function prior to the GPU kernel function. The sanity check function takes identical arguments with PL/CUDA function, and returns \nbool\n data type.\n\n\nPL/CUDA Related Functions\n\n\n\n\n\n\n\n\nDefinition\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nplcuda_function_source(regproc)\n\n\ntext\n\n\nIt returns source code of the GPU kernel generated from the PL/CUDA function, towards the OID input of PL/CUDA function as argument.\n\n\n\n\n\n\n\n\nSupport functions for PL/CUDA invocations\n\n\nThe functions below are provided to simplify invocation of PL/CUDA functions.\n\n\n\n\n\n\n\n\nDefinition\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nattnums_of(regclass,text[])\n\n\nsmallint[]\n\n\nIt returns attribute numbers for the column names (may be multiple) of the 2nd argument on the table of the 1st argument.\n\n\n\n\n\n\nattnum_of(regclass,text)\n\n\nsmallint\n\n\nIt returns attribute number for the column name of the 2nd argument on the table of the 1st argument.\n\n\n\n\n\n\natttypes_of(regclass,text[])\n\n\nregtype[]\n\n\nIt returns data types for the column names (may be multiple) of the 2nd argument on the table of the 1st argument.\n\n\n\n\n\n\natttype_of(regclass,text)\n\n\nregtype\n\n\nIt returns data type for the column name of the 2nd argument on the table of the 1st argument.\n\n\n\n\n\n\nattrs_types_check(regclass,text[],regtype[])\n\n\nbool\n\n\nIt checks whether the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data types of the 3rd argument for each.\n\n\n\n\n\n\nattrs_type_check(regclass,text[],regtype)\n\n\nbool\n\n\nIt checks whether all the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data type of the 3rd argument.\n\n\n\n\n\n\n\n\nArray-Matrix Functions\n\n\nThis section introduces the SQL functions that supports array-based matrix types provided by PG-Strom.\n\n\n\n\n2-dimensional Array\n\n\nElement of array begins from 1 for each dimension\n\n\nNo NULL value is contained\n\n\nLength of the array is less than 1GB, due to the restriction of variable length datum in PostgreSQL\n\n\nArray with \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data type\n\n\n\n\nIf and when the array satisfies the above terms, we can determine the location of (i,j) element of the array by the index uniquely, and it enables GPU thread to fetch the datum to be processed very efficiently. Also, array-based matrix packs only the data to be used for calculation, unlike usual row-based format, so it has advantaged on memory consumption and data transfer.\n\n\n\n\n\n\n\n\nDefinition\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\narray_matrix(variadic arg, ...)\n\n\narray\n\n\nIt is an aggregate function that combines all the rows supplied. For example, when 3 \nfloat\n arguments were supplied by 1000 rows, it returns an array-based matrix of 3 columns X 1000 rows, with \nfloat\n data type.\nThis function is declared to take variable length arguments. The \narg\n takes one or more scalar values of either \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n. All the arg must have same data types.\n\n\n\n\n\n\nmatrix_unnest(array)\n\n\nrecord\n\n\nIt is a set function that extracts the array-based matrix to set of records. \narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. It returns \nrecord\n type which consists of more than one columns according to the width of matrix. For example, in case of a matrix of 10 columns X 500 rows, each records contains 10 columns with element type of the matrix, then it generates 500 of the records. \nIt is similar to the standard \nunnest\n function, but generates \nrecord\n type, thus, it requires to specify the record type to be returned using \nAS (colname1 type[, ...])\n clause.\n\n\n\n\n\n\nrbind(array, array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function combines the supplied two matrices vertically. Both matrices needs to have same element data type. If width of matrices are not equivalent, it fills up the padding area by zero.\n\n\n\n\n\n\nrbind(array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function is similar to \nrbind(array, array)\n, but performs as an aggregate function, then combines all the input matrices into one result vertically.\n\n\n\n\n\n\ncbind(array, array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function combines the supplied two matrices horizontally. Both matrices needs to have same element data type. If height of matrices are not equivalent, it fills up the padding area by zero.\n\n\n\n\n\n\ncbind(array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function is similar to cbind(array, array), but performs as an aggregate function, then combines all the input matrices into one result horizontally.\n\n\n\n\n\n\ntranspose(array)\n\n\narray\n\n\narray\n is an array of \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function makes a transposed matrix that swaps height and width of the supplied matrix.\n\n\n\n\n\n\narray_matrix_validation(anyarray)\n\n\nbool\n\n\nIt validates whether the supplied array (\nanyarray\n) is adequate for the array-based matrix. It is intended to use for sanity check prior to invocation of PL/CUDA function, or check constraint on domain type definition.\n\n\n\n\n\n\narray_matrix_height(array)\n\n\nint\n\n\narray\n is an array of either \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function returns the height of the supplied matrix.\n\n\n\n\n\n\narray_matrix_width(array)\n\n\nint\n\n\narray\n is an array of either \nsmallint\n, \nint\n, \nbigint\n, \nreal\n or \nfloat\n data. This function returns the width of the supplied matrix.", 
            "title": "PL/CUDA"
        }, 
        {
            "location": "/plcuda/#plcuda-overview", 
            "text": "PG-Strom internally constructs GPU programs by CUDA language, according to the supplied SQL, then generates GPU's native binary using just-in-time compile. CUDA is a programming environment provided by NVIDIA. It allows implementing parallel program which is executable on GPU device, using C-like statement. This transformation process from SQL statement to CUDA program is an internal process, thus, no need to pay attention what GPU programs are generated and executed from the standpoint of users.  On the other hands, PostgreSQL supports to add programming language to implement SQL functions by  CREATE LANGUAGE  statement. PL/CUDA is a language handler to supports  CREATE LANGUAGE  command. It also allows users to run arbitrary GPU programs manually implemented as SQL functions, but not only GPU programs automatically generated by PG-Strom based on SQL.  Its argument can take the data types supported by PG-Strom, like numeric, text, or array-matrix data type. These arguments are implicitly loaded onto GPU device memory by the PL/CUDA infrastructure, so users don't need to pay attention for data loading between the database and GPU devices. In a similar fashion, the return value of PL/CUDA function (including the case of variable length data type) will be written back to CPU from GPU, then decode to the result of SQL function.  You can also use foreign tables defined with  gstore_fdw  as arguments of PL/CUDA function. In this case, no need to load the data onto GPU for each invocation because foreign table already keeps the data, and available to use larger data than 1GB which is a restriction of variable length data in PostgreSQL.  Therefore, users can focus on productive tasks like implementation of statistical analysis, code optimization and so on, without routine process like data input/output between GPU and databases.   Once a PL/CUDA function is declared using  CREATE FUNCTION , it generates a CUDA program source code that embeds the definition of this function, then build it for the target GPU device.\nThis CUDA program is almost identical to usual GPU software based on CUDA runtime, except for the auxiliary code to receive arguments of SQL function and to write back its results. It also allows to include/link some libraries for CUDA device runtime.  Native CUDA programs implemented by PL/CUDA are executed as child-processes of PostgreSQL backend.\nTherefore, it has independent address space and OS/GPU resources from PostgreSQL.\nCUDA program contains host code for the host system and device code to be executed on GPU devices.\nThe host code can execute any logic we can program using C-language, so we restrict only database superuser can define PL/CUDA function from the standpoint of security.  Below is an example of simple PL/CUDA function. This function takes two same length  real[]  array as arguments, then returns its dot product in  float  data type.  CREATE OR REPLACE FUNCTION\ngpu_dot_product(real[], real[])\nRETURNS float\nAS $$\n#plcuda_decl\n#include  cuda_matrix.h \n\nKERNEL_FUNCTION_MAXTHREADS(void)\ngpu_dot_product(double *p_dot,\n                VectorTypeFloat *X,\n                VectorTypeFloat *Y)\n{\n    size_t      index = get_global_id();\n    size_t      nitems = X- height;\n    float       v[MAXTHREADS_PER_BLOCK];\n    float       sum;\n\n    if (index   nitems)\n        v[get_local_id()] = X- values[index] * Y- values[index];\n    else\n        v[get_local_id()] = 0.0;\n\n    sum = pgstromTotalSum(v, MAXTHREADS_PER_BLOCK);\n    if (get_local_id() == 0)\n        atomicAdd(p_dot, (double)sum);\n    __syncthreads();\n}\n#plcuda_begin\n{\n    size_t      nitems;\n    int         blockSz;\n    int         gridSz;\n    double     *dot;\n    cudaError_t rc;\n\n    if (!VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg1, PG_FLOAT4OID) ||\n        !VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg2, PG_FLOAT4OID))\n        EEXIT( arguments are not vector like array );\n    nitems = ARRAY_VECTOR_HEIGHT(arg1);\n    if (nitems != ARRAY_VECTOR_HEIGHT(arg2))\n        EEXIT( length of arguments mismatch );\n\n    rc = cudaMallocManaged( dot, sizeof(double));\n    if (rc != cudaSuccess)\n        CUEXIT(rc,  failed on cudaMallocManaged );\n    memset(dot, 0, sizeof(double));\n\n    blockSz = MAXTHREADS_PER_BLOCK;\n    gridSz = (nitems + MAXTHREADS_PER_BLOCK - 1) / MAXTHREADS_PER_BLOCK;\n    gpu_dot_product gridSz,blockSz (dot,\n                                        (VectorTypeFloat *)arg1,\n                                        (VectorTypeFloat *)arg2);\n    rc = cudaStreamSynchronize(NULL);\n    if (rc != cudaSuccess)\n        CUEXIT(rc,  failed on cudaStreamSynchronize );\n\n    return *dot;\n}\n#plcuda_end\n$$ LANGUAGE 'plcuda';  PL/CUDA infrastructure makes entrypoint function of CUDA program by the block between  #plcuda_begin  and  #plcuda_end  with extra code to exchange arguments of SQL function.\nThe portion enclosed by  #plcuda_decl  and  #plcuda_begin  is a block for declaration of GPU device functions and other host functions. It is placed prior to the entrypoint above.  At the entrypoint of the CUDA program, you can refer the arguments of SQL function using  arg1 , `arg2, and so on.  In the above example, the  arg1  and  arg2 ,  real[]  array type, are passed to the entrypoint, then  VALIDATE_ARRAY_VECTOR_TYPE_STRICT  macro checks whether it is 1-dimensional array of 32bit floating-point values without NULL.  Ditto with return value, the entrypoint returns a value in CUDA C representation corresponding to the SQL data type.\nIf entrypoint does not return any value (or, it exits the program with status code 1 by  exit() ), it is considered PL/CUDA function returns  NULL .   The above sample program validates the array of  real  values passed from SQL function, then it allocates the result buffer by  cudaMallocManaged , and invokes  gpu_dot_product , a GPU kernel function, to compute dot product with two vectors.  The result of this function is below. It computes the dot product of two vectors which contain 10,000 items randomly generated.  postgres=# SELECT gpu_dot_product(array_matrix(random()::real),\n                                  array_matrix(random()::real))\n             FROM generate_series(1,10000);\n gpu_dot_product\n------------------\n 3.71461999509484\n(1 row)", 
            "title": "PL/CUDA Overview"
        }, 
        {
            "location": "/plcuda/#plcuda-structure", 
            "text": "Function declaration of PL/CUDA is consists of two code blocks split by the directives of  #plcuda_decl ,  #plcuda_begin  and  #plcuda_end . Users can put their custom code on the code blocks according to the purpose, then PL/CUDA language handler reconstruct them into single source file with extra logic to exchange function arguments and results.  #plcuda_decl\n  [...any declarations...]\n#plcuda_begin\n  [...host code in the entrypoint...]\n#plcuda_end  The code block, begins from  #plcuda_decl , can have declaration of  __host__  and  __device__  functions and variables for CUDA C.\nThis code block locates in front of the entrypoint function which contains the code block between  #plcuda_begin  and  #plcuda_end  at the source file eventually constructed.  If external header files are included using  #include  statement of CUDA C, put the statement on this code block.  The code block between  #plcuda_begin  and  #plcuda_end  is embedded to a part of entrypoint function. Therefore, it does not describe function name, arguments definition and so on.\nPrior to execution of the code block, the entrypoint function receives arguments of the SQL function from PostgreSQL backend, and set up  arg1 ,  arg2 , ... variables for further references.  These variables have the following CUDA C representation according to SQL data types.     SQL data type  CUDA C data type  Examples      reggstore  void *  OID of Gstore_fdw foreign table    real  float  32bit floating point    float  double  64bit floating point    Other inline data types  Datum  int ,  date , ...    Fixed-length value by reference  void *  uuid , ...    Variable-length value (varlena)  varlena *  text ,  real[] , ...     PL/CUDA language handler constructs a single CUDA C source file from the code blocks above, then builds it once by  nvcc  compiler at declaration or execution time.\nIf it contains any  #plcuda_include  directive, its source code is not fixed until execution time, so built at the execution time only. In case when identical CUDA program is already pre-built, we can reuse it without rebuild.   When SQL command invokes PL/CUDA function, PL/CUDA language handler launch the pre-built CUDA program, then copies the arguments of SQL function over pipe. These are stored in the argument buffer of the CUDA program, so custom logic can refer them using  arg1  or  arg2  variables.  The data types by reference at CUDA C program, like variable-length datum, are initialized as pointers to the argument buffer. It is a managed memory region allocated by  cudaMallocManaged() , these pointers are available without explicit DMA between host system and GPU devices.  Here is a special case if argument has  reggstore  type. It is actually an OID (32bit integer) of Gstore_Fdw foreign table, however, it is replaced to the reference of GPU device memory acquired by the Gstore_Fdw foreign table if it is supplied as PL/CUDA argument.  The argument is setup to the pointer for  GstoreIpcMapping  object.  GstoreIpcMapping::map  holds the mapped address of the GPU device memory acquired by the Gstore_Fdw foreign table. GstoreIpcHandle::device_id  indicates the device-id of GPU which physically holds the region, and GstoreIpcHandle::rawsize` is raw length of the region.  typedef struct\n{\n    cl_uint     __vl_len;       /* 4B varlena header */\n    cl_short    device_id;      /* GPU device where pinning on */\n    cl_char     format;         /* one of GSTORE_FDW_FORMAT__* */\n    cl_char     __padding__;    /* reserved */\n    cl_long     rawsize;        /* length in bytes */\n    union {\n#ifdef CU_IPC_HANDLE_SIZE\n        CUipcMemHandle      d;  /* CUDA driver API */\n#endif\n#ifdef CUDA_IPC_HANDLE_SIZE\n        cudaIpcMemHandle_t  r;  /* CUDA runtime API */\n#endif\n        char                data[64];\n    } ipc_mhandle;\n} GstoreIpcHandle;\n\ntypedef struct\n{\n    GstoreIpcHandle h; /* IPChandle of Gstore_Fdw */\n    void       *map;    /* mapped device pointer */\n} GstoreIpcMapping;  PL/CUDA function can return its result using  return  of a CUDA C datum relevant to the SQL data type.\nIn case when no  return  clause is executed,  NULL  pointer is returned if CUDA C data type is pointer, or CUDA program is terminated with status code = 1 by  exit(1) , PL/CUDA function returns  null  to SQL.", 
            "title": "PL/CUDA Structure"
        }, 
        {
            "location": "/plcuda/#plcuda-references", 
            "text": "This section is a reference for PL/CUDA function's directives and related SQL functions.", 
            "title": "PL/CUDA References"
        }, 
        {
            "location": "/plcuda/#advantage-and-disadvantage-of-plcuda", 
            "text": "On invocation of PL/CUDA function, it launches the relevant CUDA program on behalf of the invocation, then CUDA program initialize per process context of GPU device. The series of operations are never lightweight, so we don't recommend to implement a simple comparison of scalar values using PL/CUDA, and use for full table scan on billion rows.  On the other hands, once GPU device is correctly initialized, it allows to process massive amount of data using several thousands of processor cores on GPU device. Especially, it is suitable for computing intensive workloads, like machine-learning or advanced analytics that approach to the optimal values by repeated calculation for example.  According to the growth of data size, we need to pay attention how to exchange data with CUDA program.\nPostgreSQL supports array types, and it is easy and simple way to exchange several millions of integer or real values at most.  However, variable-length datum of PostgreSQL, including the array-types, is restricted to 1GB at a maximum.\nWe need to take a little idea to handle larger data, like separation of data-set. In addition, PostgreSQL backend process set up the argument of SQL functions in single thread, so it takes a certain amount of time to manipulate gigabytes-class memory object.  Please consider usage of Gstore_Fdw foreign-table when data size grows more than several hundreds megabytes.\nOnce you preload the large data-set onto GPU device memory through Gstore_Fdw, no need to set up large arguments on invocation of PL/CUDA function. It also allows to keep larger data than gigabytes, as lond as GPU device memory capacity allows.", 
            "title": "Advantage and disadvantage of PL/CUDA"
        }, 
        {
            "location": "/plcuda/#plcuda-directives", 
            "text": "", 
            "title": "PL/CUDA Directives"
        }, 
        {
            "location": "/plcuda/#plcuda_decl", 
            "text": "This directive begins a code block which contains CUDA C functions and variables with both of  __host__  and  __device__  attributes. PL/CUDA language handler copies this code block in front of the program entrypoint as is.  Use of this directive is optional, however, it makes no sense if here is no declaration of GPU kernel functions to be called from the entrypoint. So, we usually have more than one GPU kernel function.", 
            "title": "#plcuda_decl"
        }, 
        {
            "location": "/plcuda/#plcuda_begin", 
            "text": "This directive begins a code block which consists a part of the entrypoint of CUDA program.\nThe CUDA program setup the referable  arg1 ,  arg2 , ... variables according to the arguments of PL/CUDA function, then switch control to the user defined portion. This code block is a host code; we can implement own control logic working on CPU or heavy calculation by GPU kernel invocation.  Result of PL/CUDA function can be returned using  return  statement of CUDA C, according to the function definition.", 
            "title": "#plcuda_begin"
        }, 
        {
            "location": "/plcuda/#plcuda_end", 
            "text": "It marks end of the kernel function code block. By the way, if a directive to start code block was put inside of the different code block, the current code block is implicitly closed by the  #plcuda_end  directive.", 
            "title": "#plcuda_end"
        }, 
        {
            "location": "/plcuda/#plcuda_include-function-name", 
            "text": "This directive is similar to  #include  of CUDA C, however, it injects result of the specified SQL function onto the location where the directive was written.\nThe SQL function should have identical arguments and return  text  data.  For example, when we calculate similarity of massive items, we can generate multiple variant of the algorithm on the fly that is almost equivalent but only distance definitions are different. It makes maintenance of PL/CUDA function simplified.", 
            "title": "#plcuda_include &lt;function name&gt;"
        }, 
        {
            "location": "/plcuda/#plcuda_library-library-name", 
            "text": "It specifies the library name to be linked when CUDA program is built by  nvcc .\nThe  library name  portion is supplied to  nvcc  command as  -l  option.\nFor example, if  libcublas.co  library is linked, you need to describe  cublas  without prefix ( lib ) and suffix ( .so ).\nRight now, we can specify the libraries only installed on the standard library path of CUDA Toolkit (`/usr/local/cuda/lib64).", 
            "title": "#plcuda_library &lt;library name&gt;"
        }, 
        {
            "location": "/plcuda/#plcuda_sanity_check-function", 
            "text": "It allows to specify the sanity check function that preliminary checks adequacy of the supplied arguments, prior to GPU kernel launch.\nNo sanity check function is configured on the default.\nUsually, launch of GPU kernel function is heavier task than call of another function on CPU, because it also involves initialization of GPU devices. If supplied arguments have unacceptable values from the specification of the PL/CUDA function, a few thousands or millions (or more in some cases) of GPU kernel threads shall be launched just to check the arguments and return an error status. If sanity check can be applied prior to the launch of GPU kernel function with enough small cost, it is a valuable idea to raise an error using sanity check function prior to the GPU kernel function. The sanity check function takes identical arguments with PL/CUDA function, and returns  bool  data type.", 
            "title": "#plcuda_sanity_check &lt;function&gt;"
        }, 
        {
            "location": "/plcuda/#plcuda-related-functions", 
            "text": "Definition  Result  Description      plcuda_function_source(regproc)  text  It returns source code of the GPU kernel generated from the PL/CUDA function, towards the OID input of PL/CUDA function as argument.", 
            "title": "PL/CUDA Related Functions"
        }, 
        {
            "location": "/plcuda/#support-functions-for-plcuda-invocations", 
            "text": "The functions below are provided to simplify invocation of PL/CUDA functions.     Definition  Result  Description      attnums_of(regclass,text[])  smallint[]  It returns attribute numbers for the column names (may be multiple) of the 2nd argument on the table of the 1st argument.    attnum_of(regclass,text)  smallint  It returns attribute number for the column name of the 2nd argument on the table of the 1st argument.    atttypes_of(regclass,text[])  regtype[]  It returns data types for the column names (may be multiple) of the 2nd argument on the table of the 1st argument.    atttype_of(regclass,text)  regtype  It returns data type for the column name of the 2nd argument on the table of the 1st argument.    attrs_types_check(regclass,text[],regtype[])  bool  It checks whether the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data types of the 3rd argument for each.    attrs_type_check(regclass,text[],regtype)  bool  It checks whether all the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data type of the 3rd argument.", 
            "title": "Support functions for PL/CUDA invocations"
        }, 
        {
            "location": "/plcuda/#array-matrix-functions", 
            "text": "This section introduces the SQL functions that supports array-based matrix types provided by PG-Strom.   2-dimensional Array  Element of array begins from 1 for each dimension  No NULL value is contained  Length of the array is less than 1GB, due to the restriction of variable length datum in PostgreSQL  Array with  smallint ,  int ,  bigint ,  real  or  float  data type   If and when the array satisfies the above terms, we can determine the location of (i,j) element of the array by the index uniquely, and it enables GPU thread to fetch the datum to be processed very efficiently. Also, array-based matrix packs only the data to be used for calculation, unlike usual row-based format, so it has advantaged on memory consumption and data transfer.     Definition  Result  Description      array_matrix(variadic arg, ...)  array  It is an aggregate function that combines all the rows supplied. For example, when 3  float  arguments were supplied by 1000 rows, it returns an array-based matrix of 3 columns X 1000 rows, with  float  data type. This function is declared to take variable length arguments. The  arg  takes one or more scalar values of either  smallint ,  int ,  bigint ,  real  or  float . All the arg must have same data types.    matrix_unnest(array)  record  It is a set function that extracts the array-based matrix to set of records.  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. It returns  record  type which consists of more than one columns according to the width of matrix. For example, in case of a matrix of 10 columns X 500 rows, each records contains 10 columns with element type of the matrix, then it generates 500 of the records.  It is similar to the standard  unnest  function, but generates  record  type, thus, it requires to specify the record type to be returned using  AS (colname1 type[, ...])  clause.    rbind(array, array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function combines the supplied two matrices vertically. Both matrices needs to have same element data type. If width of matrices are not equivalent, it fills up the padding area by zero.    rbind(array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function is similar to  rbind(array, array) , but performs as an aggregate function, then combines all the input matrices into one result vertically.    cbind(array, array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function combines the supplied two matrices horizontally. Both matrices needs to have same element data type. If height of matrices are not equivalent, it fills up the padding area by zero.    cbind(array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function is similar to cbind(array, array), but performs as an aggregate function, then combines all the input matrices into one result horizontally.    transpose(array)  array  array  is an array of  smallint ,  int ,  bigint ,  real  or  float  data. This function makes a transposed matrix that swaps height and width of the supplied matrix.    array_matrix_validation(anyarray)  bool  It validates whether the supplied array ( anyarray ) is adequate for the array-based matrix. It is intended to use for sanity check prior to invocation of PL/CUDA function, or check constraint on domain type definition.    array_matrix_height(array)  int  array  is an array of either  smallint ,  int ,  bigint ,  real  or  float  data. This function returns the height of the supplied matrix.    array_matrix_width(array)  int  array  is an array of either  smallint ,  int ,  bigint ,  real  or  float  data. This function returns the width of the supplied matrix.", 
            "title": "Array-Matrix Functions"
        }, 
        {
            "location": "/ref_types/", 
            "text": "Data Types\n\n\n\nPG-Strom support the following data types for use on GPU device.\n\n\nBuilt-in numeric types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nsmallint\n\n\ncl_short\n\n\n2 bytes\n\n\n\n\n\n\n\n\ninteger\n\n\ncl_int\n\n\n4 bytes\n\n\n\n\n\n\n\n\nbigint\n\n\ncl_long\n\n\n8 bytes\n\n\n\n\n\n\n\n\nreal\n\n\ncl_float\n\n\n4 bytes\n\n\n\n\n\n\n\n\nfloat\n\n\ncl_double\n\n\n8 bytes\n\n\n\n\n\n\n\n\nnumeric\n\n\ncl_ulong\n\n\nvariable length\n\n\nmapped to 64bit internal format\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhen GPU processes values in \nnumeric\n data type, it is converted to an internal 64bit format because of implementation reason.\nIt is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert \nnumaric\n datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if \nnumeric\n data with large number of digits are supplied to GPU device.\nTo avoid the problem, turn off the GUC option \npg_strom.enable_numeric_type\n not to run operational expression including \nnumeric\n data types on GPU devices.\n\n\n\n\nBuilt-in date and time types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\ndate\n\n\nDateADT\n\n\n4 bytes\n\n\n\n\n\n\n\n\ntime\n\n\nTimeADT\n\n\n8 bytes\n\n\n\n\n\n\n\n\ntimetz\n\n\nTimeTzADT\n\n\n12 bytes\n\n\n\n\n\n\n\n\ntimestamp\n\n\nTimestamp\n\n\n8 bytes\n\n\n\n\n\n\n\n\ntimestamptz\n\n\nTimestampTz\n\n\n8 bytes\n\n\n\n\n\n\n\n\ninterval\n\n\nInterval\n\n\n16 bytes\n\n\n\n\n\n\n\n\n\n\nBuilt-in variable length types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nbpchar\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\nvarchar\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\nbytea\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\ntext\n\n\nvarlena *\n\n\nvariable length\n\n\n\n\n\n\n\n\n\n\nBuilt-in miscellaneous types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nboolean\n\n\ncl_bool\n\n\n1 byte\n\n\n\n\n\n\n\n\nmoney\n\n\ncl_long\n\n\n8 bytes\n\n\n\n\n\n\n\n\nuuid\n\n\npg_uuid\n\n\n16 bytes\n\n\n\n\n\n\n\n\nmacaddr\n\n\nmacaddr\n\n\n6 bytes\n\n\n\n\n\n\n\n\ninet\n\n\ninet_struct\n\n\n7 bytes or 19 bytes\n\n\n\n\n\n\n\n\ncidr\n\n\ninet_struct\n\n\n7 bytes or 19 bytes\n\n\n\n\n\n\n\n\n\n\nBuilt-in range data types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nint4range\n\n\n__int4range\n\n\n14 bytes\n\n\n\n\n\n\n\n\nint8range\n\n\n__int8range\n\n\n22 bytes\n\n\n\n\n\n\n\n\ntsrange\n\n\n__tsrange\n\n\n22 bytes\n\n\n\n\n\n\n\n\ntstzrange\n\n\n__tstzrange\n\n\n22 bytes\n\n\n\n\n\n\n\n\ndaterange\n\n\n__daterange\n\n\n14 bytes\n\n\n\n\n\n\n\n\n\n\nExtra Types\n\n\n\n\n\n\n\n\nSQL data types\n\n\nInternal format\n\n\nLength\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nfloat2\n\n\nhalf_t\n\n\n2 bytes\n\n\nHalf precision data type\n\n\n\n\n\n\nreggstore\n\n\ncl_uint\n\n\n4 bytes\n\n\nSpecific version of regclass for gstore_fdw. Special handling at PL/CUDA function invocation.", 
            "title": "Data Types"
        }, 
        {
            "location": "/ref_types/#built-in-numeric-types", 
            "text": "SQL data types  Internal format  Length  Memo      smallint  cl_short  2 bytes     integer  cl_int  4 bytes     bigint  cl_long  8 bytes     real  cl_float  4 bytes     float  cl_double  8 bytes     numeric  cl_ulong  variable length  mapped to 64bit internal format      Note  When GPU processes values in  numeric  data type, it is converted to an internal 64bit format because of implementation reason.\nIt is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert  numaric  datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if  numeric  data with large number of digits are supplied to GPU device.\nTo avoid the problem, turn off the GUC option  pg_strom.enable_numeric_type  not to run operational expression including  numeric  data types on GPU devices.", 
            "title": "Built-in numeric types"
        }, 
        {
            "location": "/ref_types/#built-in-date-and-time-types", 
            "text": "SQL data types  Internal format  Length  Memo      date  DateADT  4 bytes     time  TimeADT  8 bytes     timetz  TimeTzADT  12 bytes     timestamp  Timestamp  8 bytes     timestamptz  TimestampTz  8 bytes     interval  Interval  16 bytes", 
            "title": "Built-in date and time types"
        }, 
        {
            "location": "/ref_types/#built-in-variable-length-types", 
            "text": "SQL data types  Internal format  Length  Memo      bpchar  varlena *  variable length     varchar  varlena *  variable length     bytea  varlena *  variable length     text  varlena *  variable length", 
            "title": "Built-in variable length types"
        }, 
        {
            "location": "/ref_types/#built-in-miscellaneous-types", 
            "text": "SQL data types  Internal format  Length  Memo      boolean  cl_bool  1 byte     money  cl_long  8 bytes     uuid  pg_uuid  16 bytes     macaddr  macaddr  6 bytes     inet  inet_struct  7 bytes or 19 bytes     cidr  inet_struct  7 bytes or 19 bytes", 
            "title": "Built-in miscellaneous types"
        }, 
        {
            "location": "/ref_types/#built-in-range-data-types", 
            "text": "SQL data types  Internal format  Length  Memo      int4range  __int4range  14 bytes     int8range  __int8range  22 bytes     tsrange  __tsrange  22 bytes     tstzrange  __tstzrange  22 bytes     daterange  __daterange  14 bytes", 
            "title": "Built-in range data types"
        }, 
        {
            "location": "/ref_types/#extra-types", 
            "text": "SQL data types  Internal format  Length  Memo      float2  half_t  2 bytes  Half precision data type    reggstore  cl_uint  4 bytes  Specific version of regclass for gstore_fdw. Special handling at PL/CUDA function invocation.", 
            "title": "Extra Types"
        }, 
        {
            "location": "/ref_devfuncs/", 
            "text": "Functions and operators\n\n\n\nThis chapter introduces the functions and operators executable on GPU devices.\n\n\nType cast\n\n\n\n\n\n\n\n\ndestination type\n\n\nsource type\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nbool\n\n\nint4\n\n\n\n\n\n\n\n\nint2\n\n\nint4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\n\n\nint4\n\n\nint2,int8,float2,float4,float8,numeric\n\n\n\n\n\n\n\n\nint8\n\n\nint2,int4,float2,float4,float8,numeric\n\n\n\n\n\n\n\n\nfloat2\n\n\nint2,int4,int8,float4,float8,numeric\n\n\n\n\n\n\n\n\nfloat4\n\n\nint2,int4,int8,float2,float8,numeric\n\n\n\n\n\n\n\n\nfloat8\n\n\nint2,int4,int8,float2,float4,numeric\n\n\n\n\n\n\n\n\nnumeric\n\n\nint2,int4,int8,float2,float4,float8\n\n\n\n\n\n\n\n\nmoney\n\n\nint4,int8,numeric\n\n\n\n\n\n\n\n\ninet\n\n\ncidr\n\n\n\n\n\n\n\n\ndate\n\n\ntimestamp,timestamptz\n\n\n\n\n\n\n\n\ntime\n\n\ntimetz,timestamp,timestamptz\n\n\n\n\n\n\n\n\ntimetz\n\n\ntime,timestamptz\n\n\n\n\n\n\n\n\ntimestamp\n\n\ndate,timestamptz\n\n\n\n\n\n\n\n\ntimestamptz\n\n\ndate,timestamp\n\n\n\n\n\n\n\n\n\n\nNumeric functions/operators\n\n\n\n\n\n\n\n\nfunction/operator\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nTYPE COMP TYPE\n\n\nComparison of two values\nTYPE\n is any of \nint2,int4,int8\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\nTYPE COMP TYPE\n\n\nComparison of two values\nTYPE\n is any of \nfloat2,float4,float8\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\nnumeric COMP numeric\n\n\nComparison of two values\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\nTYPE + TYPE\n\n\nArithemetic addition\nTYPE\n is any of \nint2,int4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\nTYPE - TYPE\n\n\nArithemetic substract\nTYPE\n is any of \nint2,int4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\nTYPE * TYPE\n\n\nArithemetic multiplication\nTYPE\n is any of \nint2,int4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\nTYPE / TYPE\n\n\nArithemetic division\nTYPE\n is any of \nint2,int4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\nTYPE % TYPE\n\n\nReminer operator\nTYPE\n is any of \nint2,int4,int8\n\n\n\n\n\n\nTYPE \n TYPE\n\n\nBitwise AND\nTYPE\n is any of \nint2,int4,int8\n\n\n\n\n\n\nTYPE \n TYPE\n\n\nBitwise OR\nTYPE\n is any of \nint2,int4,int8\n\n\n\n\n\n\nTYPE # TYPE\n\n\nBitwise XOR\nTYPE\n is any of \nint2,int4,int8\n\n\n\n\n\n\n~ TYPE\n\n\nBitwise NOT\nTYPE\n is any if \nint2,int4,int8\n\n\n\n\n\n\nTYPE \n int4\n\n\nRight shift\nTYPE\n is any of \nint2,int4,int8\n\n\n\n\n\n\nTYPE \n int4\n\n\nLeft shift\nTYPE\n is any of \nint2,int4,int8\n\n\n\n\n\n\n+ TYPE\n\n\nUnary plus\nTYPE\n is any of \nint2,int4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\n- TYPE\n\n\nUnary minus\nTYPE\n is any of \nint2,int4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\n@TYPE\n\n\nAbsolute value\nTYPE\n is any of \nint2,int4,int8,float2,float4,float8,numeric\n\n\n\n\n\n\n\n\nMathematical functions\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ncbrt(float8)\n\n\ncube root\n\n\n\n\n\n\ndcbrt(float8)\n\n\ncube root\n\n\n\n\n\n\nceil(float8)\n\n\nnearest integer greater than or equal to argument\n\n\n\n\n\n\nceiling(float8)\n\n\nnearest integer greater than or equal to argument\n\n\n\n\n\n\nexp(float8)\n\n\nexponential\n\n\n\n\n\n\ndexp(float8)\n\n\nexponential\n\n\n\n\n\n\nfloor(float8)\n\n\nnearest integer less than or equal to argument\n\n\n\n\n\n\nln(float8)\n\n\nnatural logarithm\n\n\n\n\n\n\ndlog1(float8)\n\n\nnatural logarithm\n\n\n\n\n\n\nlog(float8)\n\n\nbase 10 logarithm\n\n\n\n\n\n\ndlog10(float8)\n\n\nbase 10 logarithm\n\n\n\n\n\n\npi()\n\n\ncircumference ratio\n\n\n\n\n\n\npower(float8,float8)\n\n\npower\n\n\n\n\n\n\npow(float8,float8)\n\n\npower\n\n\n\n\n\n\ndpow(float8,float8)\n\n\npower\n\n\n\n\n\n\nround(float8)\n\n\nround to the nearest integer\n\n\n\n\n\n\ndround(float8)\n\n\nround to the nearest integer\n\n\n\n\n\n\nsign(float8)\n\n\nsign of the argument\n\n\n\n\n\n\nsqrt(float8)\n\n\nsquare root\n\n\n\n\n\n\ndsqrt(float8)\n\n\nsquare root\n\n\n\n\n\n\ntrunc(float8)\n\n\ntruncate toward zero\n\n\n\n\n\n\ndtrunc(float8)\n\n\ntruncate toward zero\n\n\n\n\n\n\n\n\nTrigonometric functions\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ndegrees(float8)\n\n\nradians to degrees\n\n\n\n\n\n\nradians(float8)\n\n\ndegrees to radians\n\n\n\n\n\n\nacos(float8)\n\n\ninverse cosine\n\n\n\n\n\n\nasin(float8)\n\n\ninverse sine\n\n\n\n\n\n\natan(float8)\n\n\ninverse tangent\n\n\n\n\n\n\natan2(float8,float8)\n\n\ninverse tangent of \narg1 / arg2\n\n\n\n\n\n\ncos(float8)\n\n\ncosine\n\n\n\n\n\n\ncot(float8)\n\n\ncotangent\n\n\n\n\n\n\nsin(float8)\n\n\nsine\n\n\n\n\n\n\ntan(float8)\n\n\ntangent\n\n\n\n\n\n\n\n\nDate and time operators\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ndate COMP date\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ndate COMP timestamp\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ndate COMP timestamptz\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntime COMP time\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntimetz COMP timetz\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntimestamp COMP timestamp\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntimestamp COMP date\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntimestamptz COMP timestamptz\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntimestamptz COMP date\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntimestamp COMP timestamptz\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ntimestamptz COMP timestamp\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ninterval COMP interval\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ndate OP int4\n\n\nOP\n is either of \n+,-\n\n\n\n\n\n\nint4 + date\n\n\n\n\n\n\n\n\ndate - date\n\n\n\n\n\n\n\n\ndate + time\n\n\n\n\n\n\n\n\ndate + timetz\n\n\n\n\n\n\n\n\ntime + date\n\n\n\n\n\n\n\n\ntime - time\n\n\n\n\n\n\n\n\ntimestamp - timestamp\n\n\n\n\n\n\n\n\ntimetz OP interval\n\n\nOP\n is either of \n+,-\n\n\n\n\n\n\ntimestamptz OP interval\n\n\nOP\n is either of \n+,-\n\n\n\n\n\n\noverlaps(TYPE,TYPE,TYPE,TYPE)\n\n\nTYPE\n is any of \ntime,timetz,timestamp,timestamptz\n\n\n\n\n\n\nextract(text FROM TYPE)\n\n\nTYPE\n is any of \ntime,timetz,timestamp,timestamptz,interval\n\n\n\n\n\n\nnow()\n\n\n\n\n\n\n\n\n- interval\n\n\nunary minus operator\n\n\n\n\n\n\ninterval OP interval\n\n\nOP\n is either of \n+,-\n\n\n\n\n\n\n\n\nText functions/operators\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n{text,bpchar} COMP {text,bpchar}\n\n\nCOMP\n is either of \n=,\n\n\n\n\n\n\n{text,bpchar} COMP {text,bpchar}\n\n\nCOMP\n is either of \n,\n=,\n=,\nOnly available on no-locale or UTF-8\n\n\n\n\n\n\nlength(TYPE)\n\n\nlength of the string\nTYPE\n is either of \ntext,bpchar\n\n\n\n\n\n\nTYPE LIKE text\n\n\nTYPE\n is either of \ntext,bpchar\n\n\n\n\n\n\nTYPE NOT LIKE text\n\n\nTYPE\n is either of \ntext,bpchar\n\n\n\n\n\n\nTYPE ILIKE text\n\n\nTYPE\n is either of \ntext,bpchar\nOnly available on no-locale or UTF-8\n\n\n\n\n\n\nTYPE NOT ILIKE text\n\n\nTYPE\n is either of \ntext,bpchar\nOnly available on no-locale or UTF-8\n\n\n\n\n\n\n\n\nNetwork functions/operators\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nmacaddr COMP macaddr\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\nmacaddr \n macaddr\n\n\nBitwise AND operator\n\n\n\n\n\n\nmacaddr \n macaddr\n\n\nBitwise OR operator\n\n\n\n\n\n\n~ macaddr\n\n\nBitwise NOT operator\n\n\n\n\n\n\ntrunc(macaddr)\n\n\nSet last 3 bytes to zero\n\n\n\n\n\n\ninet COMP inet\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\ninet INCL inet\n\n\nINCL\n is any of \n,\n=,\n,\n=,\n\n\n\n\n\n\n~ inet\n\n\n\n\n\n\n\n\ninet \n inet\n\n\n\n\n\n\n\n\ninet \n inet\n\n\n\n\n\n\n\n\ninet + int8\n\n\n\n\n\n\n\n\ninet - int8\n\n\n\n\n\n\n\n\ninet - inet\n\n\n\n\n\n\n\n\nbroadcast(inet)\n\n\n\n\n\n\n\n\nfamily(inet)\n\n\n\n\n\n\n\n\nhostmask(inet)\n\n\n\n\n\n\n\n\nmasklen(inet)\n\n\n\n\n\n\n\n\nnetmask(inet)\n\n\n\n\n\n\n\n\nnetwork(inet)\n\n\n\n\n\n\n\n\nset_masklen(cidr,int)\n\n\n\n\n\n\n\n\nset_masklen(inet,int)\n\n\n\n\n\n\n\n\ninet_same_family(inet, inet)\n\n\n\n\n\n\n\n\ninet_merge(inet,inet)\n\n\n\n\n\n\n\n\n\n\nCurrency operators\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nmoney COMP money\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\nmoney OP money\n\n\nOP\n is any of \n+,-,/\n\n\n\n\n\n\nmoney * TYPE\n\n\nTYPE\n is any of \nint2,int4,float2,float4,float8\n\n\n\n\n\n\nTYPE * money\n\n\nTYPE\n is any of \nint2,int4,float2,float4,float8\n\n\n\n\n\n\nmoney / TYPE\n\n\nTYPE\n is any of \nint2,int4,float2,float4,float8\n\n\n\n\n\n\n\n\nUUID operators\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nuuid COMP uuid\n\n\nCOMP\n is any of \n=,\n,\n,\n=,\n=,\n\n\n\n\n\n\n\n\nRange type functions/operators\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nRANGE = RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n= RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n= RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE @ RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE @ TYPE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\nTYPE\n is element type of \nRANGE\n.\n\n\n\n\n\n\nRANGE \n@RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nTYPE \n@RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\nTYPE\n is element type of \nRANGE\n.\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE \n RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE -\n- RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE + RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE * RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nRANGE - RANGE\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nlower(RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nupper(RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nisempty(RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nlower_inc(RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nupper_inc(RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nlower_inf(RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nupper_inf(RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\nrange_merge(RANGE,RANGE)\n\n\nRANGE\n is any of \nint4range,int8range,tsrange,tstzrange,daterange\n\n\n\n\n\n\n\n\nMiscellaneous device functions\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\nresult\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nas_int8(float8)\n\n\nint8\n\n\nRe-interpret double-precision floating point bit-pattern as 64bit integer value\n\n\n\n\n\n\nas_int4(float4)\n\n\nint4\n\n\nRe-interpret single-precision floating point bit-pattern as 32bit integer value\n\n\n\n\n\n\nas_int2(float2)\n\n\nint2\n\n\nRe-interpret half-precision floating point bit-pattern as 16bit integer value\n\n\n\n\n\n\nas_float8(int8)\n\n\nfloat8\n\n\nRe-interpret 64bit integer bit-pattern as double-precision floating point value\n\n\n\n\n\n\nas_float4(int4)\n\n\nfloat4\n\n\nRe-interpret 32bit integer bit-pattern as single-precision floating point value\n\n\n\n\n\n\nas_float2(int2)\n\n\nfloat2\n\n\nRe-interpret 16bit integer bit-pattern as half-precision floating point value", 
            "title": "Functions and Operators"
        }, 
        {
            "location": "/ref_devfuncs/#type-cast", 
            "text": "destination type  source type  description      bool  int4     int2  int4,int8,float2,float4,float8,numeric     int4  int2,int8,float2,float4,float8,numeric     int8  int2,int4,float2,float4,float8,numeric     float2  int2,int4,int8,float4,float8,numeric     float4  int2,int4,int8,float2,float8,numeric     float8  int2,int4,int8,float2,float4,numeric     numeric  int2,int4,int8,float2,float4,float8     money  int4,int8,numeric     inet  cidr     date  timestamp,timestamptz     time  timetz,timestamp,timestamptz     timetz  time,timestamptz     timestamp  date,timestamptz     timestamptz  date,timestamp", 
            "title": "Type cast"
        }, 
        {
            "location": "/ref_devfuncs/#numeric-functionsoperators", 
            "text": "function/operator  description      TYPE COMP TYPE  Comparison of two values TYPE  is any of  int2,int4,int8 COMP  is any of  =, , , =, =,    TYPE COMP TYPE  Comparison of two values TYPE  is any of  float2,float4,float8 COMP  is any of  =, , , =, =,    numeric COMP numeric  Comparison of two values COMP  is any of  =, , , =, =,    TYPE + TYPE  Arithemetic addition TYPE  is any of  int2,int4,int8,float2,float4,float8,numeric    TYPE - TYPE  Arithemetic substract TYPE  is any of  int2,int4,int8,float2,float4,float8,numeric    TYPE * TYPE  Arithemetic multiplication TYPE  is any of  int2,int4,int8,float2,float4,float8,numeric    TYPE / TYPE  Arithemetic division TYPE  is any of  int2,int4,int8,float2,float4,float8,numeric    TYPE % TYPE  Reminer operator TYPE  is any of  int2,int4,int8    TYPE   TYPE  Bitwise AND TYPE  is any of  int2,int4,int8    TYPE   TYPE  Bitwise OR TYPE  is any of  int2,int4,int8    TYPE # TYPE  Bitwise XOR TYPE  is any of  int2,int4,int8    ~ TYPE  Bitwise NOT TYPE  is any if  int2,int4,int8    TYPE   int4  Right shift TYPE  is any of  int2,int4,int8    TYPE   int4  Left shift TYPE  is any of  int2,int4,int8    + TYPE  Unary plus TYPE  is any of  int2,int4,int8,float2,float4,float8,numeric    - TYPE  Unary minus TYPE  is any of  int2,int4,int8,float2,float4,float8,numeric    @TYPE  Absolute value TYPE  is any of  int2,int4,int8,float2,float4,float8,numeric", 
            "title": "Numeric functions/operators"
        }, 
        {
            "location": "/ref_devfuncs/#mathematical-functions", 
            "text": "functions/operators  description      cbrt(float8)  cube root    dcbrt(float8)  cube root    ceil(float8)  nearest integer greater than or equal to argument    ceiling(float8)  nearest integer greater than or equal to argument    exp(float8)  exponential    dexp(float8)  exponential    floor(float8)  nearest integer less than or equal to argument    ln(float8)  natural logarithm    dlog1(float8)  natural logarithm    log(float8)  base 10 logarithm    dlog10(float8)  base 10 logarithm    pi()  circumference ratio    power(float8,float8)  power    pow(float8,float8)  power    dpow(float8,float8)  power    round(float8)  round to the nearest integer    dround(float8)  round to the nearest integer    sign(float8)  sign of the argument    sqrt(float8)  square root    dsqrt(float8)  square root    trunc(float8)  truncate toward zero    dtrunc(float8)  truncate toward zero", 
            "title": "Mathematical functions"
        }, 
        {
            "location": "/ref_devfuncs/#trigonometric-functions", 
            "text": "functions/operators  description      degrees(float8)  radians to degrees    radians(float8)  degrees to radians    acos(float8)  inverse cosine    asin(float8)  inverse sine    atan(float8)  inverse tangent    atan2(float8,float8)  inverse tangent of  arg1 / arg2    cos(float8)  cosine    cot(float8)  cotangent    sin(float8)  sine    tan(float8)  tangent", 
            "title": "Trigonometric functions"
        }, 
        {
            "location": "/ref_devfuncs/#date-and-time-operators", 
            "text": "functions/operators  description      date COMP date  COMP  is any of  =, , , =, =,    date COMP timestamp  COMP  is any of  =, , , =, =,    date COMP timestamptz  COMP  is any of  =, , , =, =,    time COMP time  COMP  is any of  =, , , =, =,    timetz COMP timetz  COMP  is any of  =, , , =, =,    timestamp COMP timestamp  COMP  is any of  =, , , =, =,    timestamp COMP date  COMP  is any of  =, , , =, =,    timestamptz COMP timestamptz  COMP  is any of  =, , , =, =,    timestamptz COMP date  COMP  is any of  =, , , =, =,    timestamp COMP timestamptz  COMP  is any of  =, , , =, =,    timestamptz COMP timestamp  COMP  is any of  =, , , =, =,    interval COMP interval  COMP  is any of  =, , , =, =,    date OP int4  OP  is either of  +,-    int4 + date     date - date     date + time     date + timetz     time + date     time - time     timestamp - timestamp     timetz OP interval  OP  is either of  +,-    timestamptz OP interval  OP  is either of  +,-    overlaps(TYPE,TYPE,TYPE,TYPE)  TYPE  is any of  time,timetz,timestamp,timestamptz    extract(text FROM TYPE)  TYPE  is any of  time,timetz,timestamp,timestamptz,interval    now()     - interval  unary minus operator    interval OP interval  OP  is either of  +,-", 
            "title": "Date and time operators"
        }, 
        {
            "location": "/ref_devfuncs/#text-functionsoperators", 
            "text": "functions/operators  description      {text,bpchar} COMP {text,bpchar}  COMP  is either of  =,    {text,bpchar} COMP {text,bpchar}  COMP  is either of  , =, =, Only available on no-locale or UTF-8    length(TYPE)  length of the string TYPE  is either of  text,bpchar    TYPE LIKE text  TYPE  is either of  text,bpchar    TYPE NOT LIKE text  TYPE  is either of  text,bpchar    TYPE ILIKE text  TYPE  is either of  text,bpchar Only available on no-locale or UTF-8    TYPE NOT ILIKE text  TYPE  is either of  text,bpchar Only available on no-locale or UTF-8", 
            "title": "Text functions/operators"
        }, 
        {
            "location": "/ref_devfuncs/#network-functionsoperators", 
            "text": "functions/operators  description      macaddr COMP macaddr  COMP  is any of  =, , , =, =,    macaddr   macaddr  Bitwise AND operator    macaddr   macaddr  Bitwise OR operator    ~ macaddr  Bitwise NOT operator    trunc(macaddr)  Set last 3 bytes to zero    inet COMP inet  COMP  is any of  =, , , =, =,    inet INCL inet  INCL  is any of  , =, , =,    ~ inet     inet   inet     inet   inet     inet + int8     inet - int8     inet - inet     broadcast(inet)     family(inet)     hostmask(inet)     masklen(inet)     netmask(inet)     network(inet)     set_masklen(cidr,int)     set_masklen(inet,int)     inet_same_family(inet, inet)     inet_merge(inet,inet)", 
            "title": "Network functions/operators"
        }, 
        {
            "location": "/ref_devfuncs/#currency-operators", 
            "text": "functions/operators  description      money COMP money  COMP  is any of  =, , , =, =,    money OP money  OP  is any of  +,-,/    money * TYPE  TYPE  is any of  int2,int4,float2,float4,float8    TYPE * money  TYPE  is any of  int2,int4,float2,float4,float8    money / TYPE  TYPE  is any of  int2,int4,float2,float4,float8", 
            "title": "Currency operators"
        }, 
        {
            "location": "/ref_devfuncs/#uuid-operators", 
            "text": "functions/operators  description      uuid COMP uuid  COMP  is any of  =, , , =, =,", 
            "title": "UUID operators"
        }, 
        {
            "location": "/ref_devfuncs/#range-type-functionsoperators", 
            "text": "functions/operators  description      RANGE = RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE  = RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE  = RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE @ RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE @ TYPE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange TYPE  is element type of  RANGE .    RANGE  @RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    TYPE  @RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange TYPE  is element type of  RANGE .    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE   RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE - - RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE + RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE * RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    RANGE - RANGE  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    lower(RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    upper(RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    isempty(RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    lower_inc(RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    upper_inc(RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    lower_inf(RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    upper_inf(RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange    range_merge(RANGE,RANGE)  RANGE  is any of  int4range,int8range,tsrange,tstzrange,daterange", 
            "title": "Range type functions/operators"
        }, 
        {
            "location": "/ref_devfuncs/#miscellaneous-device-functions", 
            "text": "functions/operators  result  description      as_int8(float8)  int8  Re-interpret double-precision floating point bit-pattern as 64bit integer value    as_int4(float4)  int4  Re-interpret single-precision floating point bit-pattern as 32bit integer value    as_int2(float2)  int2  Re-interpret half-precision floating point bit-pattern as 16bit integer value    as_float8(int8)  float8  Re-interpret 64bit integer bit-pattern as double-precision floating point value    as_float4(int4)  float4  Re-interpret 32bit integer bit-pattern as single-precision floating point value    as_float2(int2)  float2  Re-interpret 16bit integer bit-pattern as half-precision floating point value", 
            "title": "Miscellaneous device functions"
        }, 
        {
            "location": "/ref_sqlfuncs/", 
            "text": "SQL Objects\n\n\n\nThis chapter introduces SQL objects additionally provided by PG-Strom.\n\n\nDevice Information\n\n\n\n\n\n\n\n\nFunction\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngpu_device_name(int = 0)\n\n\ntext\n\n\nIt tells name of the specified GPU device.\n\n\n\n\n\n\ngpu_global_memsize(int = 0)\n\n\nbigint\n\n\nIt tells amount of the specified GPU device in bytes.\n\n\n\n\n\n\ngpu_max_blocksize(int = 0)\n\n\nint\n\n\nIt tells maximum block-size on the specified GPU device. 1024, in the currently supported GPU models.\n\n\n\n\n\n\ngpu_warp_size(int = 0)\n\n\nint\n\n\nIt tells warp-size on the specified GPU device. 32, in the currently supported GPU models.\n\n\n\n\n\n\ngpu_max_shared_memory_perblock(int = 0)\n\n\nint\n\n\nIt tells maximum shared memory size per block on the specified GPU device.\n\n\n\n\n\n\ngpu_num_registers_perblock(int = 0)\n\n\nint\n\n\nIt tells total number of registers per block on the specified GPU device.\n\n\n\n\n\n\ngpu_num_multiptocessors(int = 0)\n\n\nint\n\n\nIt tells number of SM(Streaming Multiprocessor) units on the specified GPU device.\n\n\n\n\n\n\ngpu_num_cuda_cores(int = 0)\n\n\nint\n\n\nIt tells number of CUDA cores on the specified GPU device.\n\n\n\n\n\n\ngpu_cc_major(int = 0)\n\n\nint\n\n\nIt tells major CC(Compute Capability) version of the specified GPU device.\n\n\n\n\n\n\ngpu_cc_minor(int = 0)\n\n\nint\n\n\nIt tells minor CC(Compute Capability) version of the specified GPU device.\n\n\n\n\n\n\ngpu_pci_id(int = 0)\n\n\nint\n\n\nIt tells PCI bus-id of the specified GPU device.\n\n\n\n\n\n\n\n\nArray-based matrix support\n\n\nYou can use array data type of PostgreSQL to deliver matrix-data for PL/CUDA functions.\nA two-dimensional array of fixed-length boolean/numeric values without NULL has flat data structure (expect for the array header). It allows to identify the address of elements by indexes of the matrix uniquely.\nPG-Strom provides several SQL functions to handle array-based matrix.\n\n\nType cast\n\n\n\n\n\n\n\n\ndestination type\n\n\nsource type\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nint[]\n\n\nbit\n\n\nconvert bit-string to 32bit integer array. Unaligned bits are filled up by 0.\n\n\n\n\n\n\nbit\n\n\nint[]\n\n\nconvert 32bit integer to bit-string\n\n\n\n\n\n\n\n\nArray-based matrix functions\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\nresult\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\narray_matrix_validation(anyarray)\n\n\nbool\n\n\nIt checks whether the supplied array satisfies the requirement of array-based matrix.\n\n\n\n\n\n\narray_matrix_height(anyarray)\n\n\nint\n\n\nIt tells height of the array-based matrix.\n\n\n\n\n\n\narray_matrix_width(anyarray)\n\n\nint\n\n\nIt tells width of the array-based matrix.\n\n\n\n\n\n\narray_vector_rawsize(regtype,int)\n\n\nbigint\n\n\nIt tells expected size if N-items vector is created with the specified type.\n\n\n\n\n\n\narray_matrix_rawsize(regtype,int,int)\n\n\nbigint\n\n\nIt tells expected size if HxW matrix is created with the specified type.\n\n\n\n\n\n\narray_cube_rawsize(regtype,int,int,int)\n\n\nbigint\n\n\nIt tells expected size if HxWxD cube is created with the specified type.\n\n\n\n\n\n\ntype_len(regtype)\n\n\nbigint\n\n\nIt tells unit length of the specified type.\n\n\n\n\n\n\ncomposite_type_rawsize(LEN,...)\n\n\nbigint\n\n\nIt tells expected size of the composite type if constructed with the specified data-length order. We expect to use the function with \ntype_len()\nLEN\n is either of \nint,bigint\n\n\n\n\n\n\nmatrix_unnest(anyarray)\n\n\nrecord\n\n\nIt is a function to return set, to fetch rows from top of the supplied array-based matrix. PostgreSQL has no type information of the record, so needs to give type information using \nROW()\n clause.\n\n\n\n\n\n\nrbind(MATRIX,MATRIX)\n\n\nMATRIX\n\n\nIt combines two array-based matrix vertically.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\n\n\n\n\n\n\nrbind(TYPE,MATRIX)\n\n\nMATRIX\n\n\nIt adds a scalar value on head of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the head row.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\n.\nTYPE\n is element of \nMATRIX\n\n\n\n\n\n\nrbind(MATRIX,TYPE)\n\n\nMATRIX\n\n\nIt adds a scalar value on bottom of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the bottom row.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\nTYPE\n is element type of \nMATRIX\n\n\n\n\n\n\ncbind(MATRIX,MATRIX)\n\n\nMATRIX\n\n\nIt combines two array-based matrix horizontally.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\n\n\n\n\n\n\ncbind(TYPE,MATRIX)\n\n\nMATRIX\n\n\nIt adds a scalar value on left of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the left column.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\nTYPE\n is element type of \nMATRIX\n\n\n\n\n\n\ncbind(MATRIX,TYPE)\n\n\nMATRIX\n\n\nIt adds a scalar value on right of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the right column.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\n.\nTYPE\n is element type of \nMATRIX\n\n\n\n\n\n\ntranspose(MATRIX)\n\n\nMATRIX\n\n\nIt transposes the array-based matrix.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\n\n\n\n\n\n\n\n\nAggregate functions\n\n\n\n\n\n\n\n\nfunctions/operators\n\n\nresult\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\narray_matrix(TYPE,...)\n\n\nTYPE[]\n\n\nAn aggregate function with varidic arguments. It produces M-cols x N-rows array-based matrix if N-rows were supplied with M-columns.\nTYPE\n is any of \nbool,int2,int4,int8,float4,float8\n\n\n\n\n\n\narray_matrix(bit)\n\n\nbit[]\n\n\nAn aggregate function to produce \nint4[]\n array-based matrix. It considers bit-string as a set of 32bits integer values.\n\n\n\n\n\n\nrbind(MATRIX)\n\n\nMATRIX\n\n\nAn aggregate function to combine the supplied array-based matrix vertically.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\n\n\n\n\n\n\ncbind(MATRIX)\n\n\nMATRIX\n\n\nAn aggregate function to combine the supplied array-based matrix horizontally.\nMATRIX\n is array type of any of \nbool,int2,int4,int8,float4,float8\n\n\n\n\n\n\n\n\nMiscellaneous functions\n\n\n\n\n\n\n\n\nFunction\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npgstrom_ccache_enabled(regclass)\n\n\ntext\n\n\nEnables in-memory columnar cache on the specified table.\n\n\n\n\n\n\npgstrom_ccache_disabled(regclass)\n\n\ntext\n\n\nDisables in-memory columnar cache on the specified table.\n\n\n\n\n\n\npgstrom_ccache_prewarm(regclass)\n\n\nint\n\n\nBuild in-memory columnar cache on the specified table synchronously, until cache usage is less than the threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngstore_fdw_format(reggstore)\n\n\ntext\n\n\nIt tells internal format of the specified gstore_fdw foreign table.\n\n\n\n\n\n\ngstore_fdw_nitems(reggstore)\n\n\nbigint\n\n\nIt tells number of rows of the specified gstore_fdw foreign table.\n\n\n\n\n\n\ngstore_fdw_nattrs(reggstore)\n\n\nbigint\n\n\nIt tells number of columns of the specified gstore_fdw foreign table.\n\n\n\n\n\n\ngstore_fdw_rawsize(reggstore)\n\n\nbigint\n\n\nIt tells raw size of the specified gstore_fdw foreign table in bytes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngstore_export_ipchandle(reggstore)\n\n\nbytea\n\n\nIt tells IPC-handle of the GPU device memory region of the specified gstore_fdw foreign table.\n\n\n\n\n\n\nlo_import_gpu(int, bytea, bigint, bigint, oid=0)\n\n\noid\n\n\nIt maps GPU device memory region acquired by external application, then import its contents into a largeobject.\n\n\n\n\n\n\nlo_export_gpu(oid, int, bytea, bigint, bigint)\n\n\nbigint\n\n\nIt maps GPU device memory region acquired by external application, then export contents of the specified largeobject into the region.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nplcuda_kernel_max_blocksz\n\n\nint\n\n\nIt tells maximum block size of the GPU kernel of PL/CUDA function when it is called as its helper.\n\n\n\n\n\n\nplcuda_kernel_static_shmsz()\n\n\nint\n\n\nIt tells size of the statically acquired shared memory per block by the GPU kernel of PL/CUDA function when it is called as its helper.\n\n\n\n\n\n\nplcuda_kernel_dynamic_shmsz()\n\n\nint\n\n\nIt tells size of the dynamic shared memory per block, which GPU kernel of the PL/CUDA function can allocate, when it is called as its helper.\n\n\n\n\n\n\nplcuda_kernel_const_memsz()\n\n\nint\n\n\nIt tells size of the constant memory acquired by the GPU kernel of PL/CUDA function, when it is called as its helper.\n\n\n\n\n\n\nplcuda_kernel_local_memsz()\n\n\nint\n\n\nIt tells size of the local memory per thread acquired by the GPU kernel of PL/CUDA function, when it is called as its helper.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\n\n\nResult\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npgstrom.license_validation()\n\n\ntext\n\n\nIt validates commercial subscription.\n\n\n\n\n\n\npgstrom.license_query()\n\n\ntext\n\n\nIt shows the active commercial subscription.\n\n\n\n\n\n\n\n\nSystem View\n\n\nPG-Strom provides several system view to export its internal state for users or applications.\nThe future version may add extra fields here. So, it is not recommended to reference these information schemas using \nSELECT * FROM ...\n.\n\n\npgstrom.device_info\n\n\npgstrom.device_into\n system view exports device attributes of the GPUs recognized by PG-Strom.\nGPU has different specification for each model, like number of cores, capacity of global memory, maximum number of threads and etc, user's software should be optimized according to the information if you try raw GPU programming with PL/CUDA functions.\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndevice_nr\n\n\nint\n\n\nGPU device number\n\n\n\n\n\n\naindex\n\n\nint\n\n\nAttribute index\n\n\n\n\n\n\nattribute\n\n\ntext\n\n\nAttribute name\n\n\n\n\n\n\nvalue\n\n\ntext\n\n\nValue of the attribute\n\n\n\n\n\n\n\n\npgstrom.device_preserved_meminfo\n\n\npgstrom.device_preserved_meminfo\n system view exports information of the preserved device memory; which can be shared multiple PostgreSQL backend.\nRight now, only gstore_fdw uses this feature.\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndevice_nr\n\n\nint\n\n\nGPU device number\n\n\n\n\n\n\nhandle\n\n\nbytea\n\n\nIPC handle of the preserved device memory\n\n\n\n\n\n\nowner\n\n\nregrole\n\n\nOwner of the preserved device memory\n\n\n\n\n\n\nlength\n\n\nbigint\n\n\nLength of the preserved device memory in bytes\n\n\n\n\n\n\nctime\n\n\ntimestamp with time zone\n\n\nTimestamp when the preserved device memory is created\n\n\n\n\n\n\n\n\npgstrom.ccache_info\n\n\npgstrom.ccache_info\n system view exports attribute of the columnar-cache chunks (128MB unit for each).\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndatabase_id\n\n\noid\n\n\nDatabase Id\n\n\n\n\n\n\ntable_id\n\n\nregclass\n\n\nTable Id\n\n\n\n\n\n\nblock_nr\n\n\nint\n\n\nHead block-number of the chunk\n\n\n\n\n\n\nnitems\n\n\nbigint\n\n\nNumber of rows in the chunk\n\n\n\n\n\n\nlength\n\n\nbigint\n\n\nRaw size of the cached chunk\n\n\n\n\n\n\nctime\n\n\ntimestamp with time zone\n\n\nTimestamp of the chunk creation\n\n\n\n\n\n\natime\n\n\ntimestamp with time zone\n\n\nTimestamp of the least access to the chunk\n\n\n\n\n\n\n\n\npgstrom.ccache_builder_info\n\n\npgstrom.ccache_builder_info\n system view exports information of asynchronous builder process of columnar cache.\n\n\n\n\n\n\n\n\nName\n\n\nData Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbuilder_id\n\n\nint\n\n\nAsynchronous builder Id of columnar cache\n\n\n\n\n\n\nstate\n\n\ntext\n\n\nState of the builder process (\nshutdown\n, \nstartup\n, \nloading\n or \nsleep\n)\n\n\n\n\n\n\ndatabase_id\n\n\noid\n\n\nDatabase Id where builder process is assigned on\n\n\n\n\n\n\ntable_id\n\n\nregclass\n\n\nTable Id where the builder process is scanning on, if \nstate\n is \nloading\n.\n\n\n\n\n\n\nblock_nr\n\n\nint\n\n\nBlock number where the builder process is scanning on, if \nstate\n is \nloading\n.", 
            "title": "SQL Objects"
        }, 
        {
            "location": "/ref_sqlfuncs/#device-information", 
            "text": "Function  Result  Description      gpu_device_name(int = 0)  text  It tells name of the specified GPU device.    gpu_global_memsize(int = 0)  bigint  It tells amount of the specified GPU device in bytes.    gpu_max_blocksize(int = 0)  int  It tells maximum block-size on the specified GPU device. 1024, in the currently supported GPU models.    gpu_warp_size(int = 0)  int  It tells warp-size on the specified GPU device. 32, in the currently supported GPU models.    gpu_max_shared_memory_perblock(int = 0)  int  It tells maximum shared memory size per block on the specified GPU device.    gpu_num_registers_perblock(int = 0)  int  It tells total number of registers per block on the specified GPU device.    gpu_num_multiptocessors(int = 0)  int  It tells number of SM(Streaming Multiprocessor) units on the specified GPU device.    gpu_num_cuda_cores(int = 0)  int  It tells number of CUDA cores on the specified GPU device.    gpu_cc_major(int = 0)  int  It tells major CC(Compute Capability) version of the specified GPU device.    gpu_cc_minor(int = 0)  int  It tells minor CC(Compute Capability) version of the specified GPU device.    gpu_pci_id(int = 0)  int  It tells PCI bus-id of the specified GPU device.", 
            "title": "Device Information"
        }, 
        {
            "location": "/ref_sqlfuncs/#array-based-matrix-support", 
            "text": "You can use array data type of PostgreSQL to deliver matrix-data for PL/CUDA functions.\nA two-dimensional array of fixed-length boolean/numeric values without NULL has flat data structure (expect for the array header). It allows to identify the address of elements by indexes of the matrix uniquely.\nPG-Strom provides several SQL functions to handle array-based matrix.", 
            "title": "Array-based matrix support"
        }, 
        {
            "location": "/ref_sqlfuncs/#type-cast", 
            "text": "destination type  source type  description      int[]  bit  convert bit-string to 32bit integer array. Unaligned bits are filled up by 0.    bit  int[]  convert 32bit integer to bit-string", 
            "title": "Type cast"
        }, 
        {
            "location": "/ref_sqlfuncs/#array-based-matrix-functions", 
            "text": "functions/operators  result  description      array_matrix_validation(anyarray)  bool  It checks whether the supplied array satisfies the requirement of array-based matrix.    array_matrix_height(anyarray)  int  It tells height of the array-based matrix.    array_matrix_width(anyarray)  int  It tells width of the array-based matrix.    array_vector_rawsize(regtype,int)  bigint  It tells expected size if N-items vector is created with the specified type.    array_matrix_rawsize(regtype,int,int)  bigint  It tells expected size if HxW matrix is created with the specified type.    array_cube_rawsize(regtype,int,int,int)  bigint  It tells expected size if HxWxD cube is created with the specified type.    type_len(regtype)  bigint  It tells unit length of the specified type.    composite_type_rawsize(LEN,...)  bigint  It tells expected size of the composite type if constructed with the specified data-length order. We expect to use the function with  type_len() LEN  is either of  int,bigint    matrix_unnest(anyarray)  record  It is a function to return set, to fetch rows from top of the supplied array-based matrix. PostgreSQL has no type information of the record, so needs to give type information using  ROW()  clause.    rbind(MATRIX,MATRIX)  MATRIX  It combines two array-based matrix vertically. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8    rbind(TYPE,MATRIX)  MATRIX  It adds a scalar value on head of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the head row. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8 . TYPE  is element of  MATRIX    rbind(MATRIX,TYPE)  MATRIX  It adds a scalar value on bottom of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the bottom row. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8 TYPE  is element type of  MATRIX    cbind(MATRIX,MATRIX)  MATRIX  It combines two array-based matrix horizontally. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8    cbind(TYPE,MATRIX)  MATRIX  It adds a scalar value on left of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the left column. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8 TYPE  is element type of  MATRIX    cbind(MATRIX,TYPE)  MATRIX  It adds a scalar value on right of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the right column. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8 . TYPE  is element type of  MATRIX    transpose(MATRIX)  MATRIX  It transposes the array-based matrix. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8", 
            "title": "Array-based matrix functions"
        }, 
        {
            "location": "/ref_sqlfuncs/#aggregate-functions", 
            "text": "functions/operators  result  description      array_matrix(TYPE,...)  TYPE[]  An aggregate function with varidic arguments. It produces M-cols x N-rows array-based matrix if N-rows were supplied with M-columns. TYPE  is any of  bool,int2,int4,int8,float4,float8    array_matrix(bit)  bit[]  An aggregate function to produce  int4[]  array-based matrix. It considers bit-string as a set of 32bits integer values.    rbind(MATRIX)  MATRIX  An aggregate function to combine the supplied array-based matrix vertically. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8    cbind(MATRIX)  MATRIX  An aggregate function to combine the supplied array-based matrix horizontally. MATRIX  is array type of any of  bool,int2,int4,int8,float4,float8", 
            "title": "Aggregate functions"
        }, 
        {
            "location": "/ref_sqlfuncs/#miscellaneous-functions", 
            "text": "Function  Result  Description      pgstrom_ccache_enabled(regclass)  text  Enables in-memory columnar cache on the specified table.    pgstrom_ccache_disabled(regclass)  text  Disables in-memory columnar cache on the specified table.    pgstrom_ccache_prewarm(regclass)  int  Build in-memory columnar cache on the specified table synchronously, until cache usage is less than the threshold.        Function  Result  Description      gstore_fdw_format(reggstore)  text  It tells internal format of the specified gstore_fdw foreign table.    gstore_fdw_nitems(reggstore)  bigint  It tells number of rows of the specified gstore_fdw foreign table.    gstore_fdw_nattrs(reggstore)  bigint  It tells number of columns of the specified gstore_fdw foreign table.    gstore_fdw_rawsize(reggstore)  bigint  It tells raw size of the specified gstore_fdw foreign table in bytes.        Function  Result  Description      gstore_export_ipchandle(reggstore)  bytea  It tells IPC-handle of the GPU device memory region of the specified gstore_fdw foreign table.    lo_import_gpu(int, bytea, bigint, bigint, oid=0)  oid  It maps GPU device memory region acquired by external application, then import its contents into a largeobject.    lo_export_gpu(oid, int, bytea, bigint, bigint)  bigint  It maps GPU device memory region acquired by external application, then export contents of the specified largeobject into the region.        Function  Result  Description      plcuda_kernel_max_blocksz  int  It tells maximum block size of the GPU kernel of PL/CUDA function when it is called as its helper.    plcuda_kernel_static_shmsz()  int  It tells size of the statically acquired shared memory per block by the GPU kernel of PL/CUDA function when it is called as its helper.    plcuda_kernel_dynamic_shmsz()  int  It tells size of the dynamic shared memory per block, which GPU kernel of the PL/CUDA function can allocate, when it is called as its helper.    plcuda_kernel_const_memsz()  int  It tells size of the constant memory acquired by the GPU kernel of PL/CUDA function, when it is called as its helper.    plcuda_kernel_local_memsz()  int  It tells size of the local memory per thread acquired by the GPU kernel of PL/CUDA function, when it is called as its helper.        Function  Result  Description      pgstrom.license_validation()  text  It validates commercial subscription.    pgstrom.license_query()  text  It shows the active commercial subscription.", 
            "title": "Miscellaneous functions"
        }, 
        {
            "location": "/ref_sqlfuncs/#system-view", 
            "text": "PG-Strom provides several system view to export its internal state for users or applications.\nThe future version may add extra fields here. So, it is not recommended to reference these information schemas using  SELECT * FROM ... .  pgstrom.device_info  pgstrom.device_into  system view exports device attributes of the GPUs recognized by PG-Strom.\nGPU has different specification for each model, like number of cores, capacity of global memory, maximum number of threads and etc, user's software should be optimized according to the information if you try raw GPU programming with PL/CUDA functions.     Name  Data Type  Description      device_nr  int  GPU device number    aindex  int  Attribute index    attribute  text  Attribute name    value  text  Value of the attribute     pgstrom.device_preserved_meminfo  pgstrom.device_preserved_meminfo  system view exports information of the preserved device memory; which can be shared multiple PostgreSQL backend.\nRight now, only gstore_fdw uses this feature.     Name  Data Type  Description      device_nr  int  GPU device number    handle  bytea  IPC handle of the preserved device memory    owner  regrole  Owner of the preserved device memory    length  bigint  Length of the preserved device memory in bytes    ctime  timestamp with time zone  Timestamp when the preserved device memory is created     pgstrom.ccache_info  pgstrom.ccache_info  system view exports attribute of the columnar-cache chunks (128MB unit for each).     Name  Data Type  Description      database_id  oid  Database Id    table_id  regclass  Table Id    block_nr  int  Head block-number of the chunk    nitems  bigint  Number of rows in the chunk    length  bigint  Raw size of the cached chunk    ctime  timestamp with time zone  Timestamp of the chunk creation    atime  timestamp with time zone  Timestamp of the least access to the chunk     pgstrom.ccache_builder_info  pgstrom.ccache_builder_info  system view exports information of asynchronous builder process of columnar cache.     Name  Data Type  Description      builder_id  int  Asynchronous builder Id of columnar cache    state  text  State of the builder process ( shutdown ,  startup ,  loading  or  sleep )    database_id  oid  Database Id where builder process is assigned on    table_id  regclass  Table Id where the builder process is scanning on, if  state  is  loading .    block_nr  int  Block number where the builder process is scanning on, if  state  is  loading .", 
            "title": "System View"
        }, 
        {
            "location": "/ref_params/", 
            "text": "GUC Parameters\n\n\n\nThis session introduces PG-Strom's configuration parameters.\n\n\nEnables/disables a particular feature\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.enabled\n\n\nbool\n\n\non\n\n\nEnables/disables entire PG-Strom features at once\n\n\n\n\n\n\npg_strom.enable_gpuscan\n\n\nbool\n\n\non\n\n\nEnables/disables GpuScan\n\n\n\n\n\n\npg_strom.enable_gpuhashjoin\n\n\nbool\n\n\non\n\n\nEnables/disables GpuJoin by HashJoin\n\n\n\n\n\n\npg_strom.enable_gpunestloop\n\n\nbool\n\n\non\n\n\nEnables/disables GpuJoin by NestLoop\n\n\n\n\n\n\npg_strom.enable_gpupreagg\n\n\nbool\n\n\non\n\n\nEnables/disables GpuPreAgg\n\n\n\n\n\n\npg_strom.enable_partitionwise_gpujoin\n\n\nbool\n\n\non\n\n\nEnables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later.\n\n\n\n\n\n\npg_strom.enable_partitionwise_gpupreagg\n\n\nbool\n\n\non\n\n\nEnables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later.\n\n\n\n\n\n\npg_strom.pullup_outer_scan\n\n\nbool\n\n\non\n\n\nEnables/disables to pull up full-table scan if it is just below GpuPreAgg/GpuJoin, to reduce data transfer between CPU/RAM and GPU.\n\n\n\n\n\n\npg_strom.pullup_outer_join\n\n\nbool\n\n\non\n\n\nEnables/disables to pull up tables-join if GpuJoin is just below GpuPreAgg, to reduce data transfer between CPU/RAM and GPU.\n\n\n\n\n\n\npg_strom.enable_numeric_type\n\n\nbool\n\n\non\n\n\nEnables/disables support of \nnumeric\n data type in arithmetic expression on GPU device\n\n\n\n\n\n\npg_strom.cpu_fallback\n\n\nbool\n\n\noff\n\n\nControls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\"\n\n\n\n\n\n\npg_strom.nvme_strom_enabled\n\n\nbool\n\n\non\n\n\nEnables/disables the feature of SSD-to-GPU Direct SQL Execution\n\n\n\n\n\n\npg_strom.nvme_strom_threshold\n\n\nint\n\n\n\u81ea\u52d5\n\n\nControls the table-size threshold to invoke the feature of SSD-to-GPU Direct SQL Execution\n\n\n\n\n\n\n\n\nOptimizer Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.chunk_size\n\n\nint\n\n\n65534kB\n\n\nSize of the data blocks processed by a single GPU kernel invocation. It was configurable, but makes less sense, so fixed to about 64MB in the current version.\n\n\n\n\n\n\npg_strom.gpu_setup_cost\n\n\nreal\n\n\n4000\n\n\nCost value for initialization of GPU device\n\n\n\n\n\n\npg_strom.gpu_dma_cost\n\n\nreal\n\n\n10\n\n\nCost value for DMA transfer over PCIe bus per data-chunk (64MB)\n\n\n\n\n\n\npg_strom.gpu_operator_cost\n\n\nreal\n\n\n0.00015\n\n\nCost value to process an expression formula on GPU. If larger value than \ncpu_operator_cost\n is configured, no chance to choose PG-Strom towards any size of tables\n\n\n\n\n\n\n\n\nExecutor Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.global_max_async_tasks\n\n\nint\n\n\n160\n\n\nNumber of asynchronous taks PG-Strom can throw into GPU's execution queue in the whole system.\n\n\n\n\n\n\npg_strom.local_max_async_tasks\n\n\nint\n\n\n8\n\n\nNumber of asynchronous taks PG-Strom can throw into GPU's execution queue per process. If CPU parallel is used in combination, this limitation shall be applied for each background worker. So, more than \npg_strom.local_max_async_tasks\n asynchronous tasks are executed in parallel on the entire batch job.\n\n\n\n\n\n\npg_strom.max_number_of_gpucontext\n\n\nint\n\n\nauto\n\n\nSpecifies the number of internal data structure \nGpuContext\n to abstract GPU device. Usually, no need to expand the initial value.\n\n\n\n\n\n\n\n\nColumnar Cache Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.ccache_base_dir\n\n\nstring\n\n\n'/dev/shm'\n\n\nSpecifies the directory path to store columnar cache data files. Usually, no need to change from \n/dev/shm\n where \ntmpfs\n is mounted at.\n\n\n\n\n\n\npg_strom.ccache_total_size\n\n\nint\n\n\nauto\n\n\nUpper limit of the columnar cache in kB. Default is the smaller in 75% of volume size or 66% of system physical memory.\n\n\n\n\n\n\n\n\ngstore_fdw Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.gstore_max_relations\n\n\nint\n\n\n100\n\n\nUpper limit of the number of foreign tables with gstore_fdw. It needs restart to update the parameter.\n\n\n\n\n\n\n\n\nConfiguration of GPU code generation and build\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.program_cache_size\n\n\nint\n\n\n256MB\n\n\nAmount of the shared memory size to cache GPU programs already built. It needs restart to update the parameter.\n\n\n\n\n\n\npg_strom.num_program_builders\n\n\nint\n\n\n2\n\n\nNumber of background workers to build GPU programs asynchronously. It needs restart to update the parameter.\n\n\n\n\n\n\npg_strom.debug_jit_compile_options\n\n\nbool\n\n\noff\n\n\nControls to include debug option (line-numbers and symbol information) on JIT compile of GPU programs. It is valuable for complicated bug analysis using GPU core dump, however, should not be enabled on daily use because of performance degradation.\n\n\n\n\n\n\npg_strom.debug_kernel_source\n\n\nbool\n\n\noff\n\n\nIf enables, \nEXPLAIN VERBOSE\n command also prints out file paths of GPU programs written out.\n\n\n\n\n\n\n\n\nGPU Device Configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nType\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npg_strom.cuda_visible_devices\n\n\nstring\n\n\n''\n\n\nList of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable \nCUDAVISIBLE_DEVICES\n\n\n\n\n\n\npg_strom.gpu_memory_segment_size\n\n\nint\n\n\n512MB\n\n\nSpecifies the amount of device memory to be allocated per CUDA API call. Larger configuration will reduce the overhead of API calls, but not efficient usage of device memory.\n\n\n\n\n\n\npg_strom.max_num_preserved_gpu_memory\n\n\nint\n\n\n2048\n\n\nUpper limit of the number of preserved GPU device memory segment. Usually, don't need to change from the default value.", 
            "title": "GUC Parameters"
        }, 
        {
            "location": "/ref_params/#enablesdisables-a-particular-feature", 
            "text": "Parameter  Type  Default  Description      pg_strom.enabled  bool  on  Enables/disables entire PG-Strom features at once    pg_strom.enable_gpuscan  bool  on  Enables/disables GpuScan    pg_strom.enable_gpuhashjoin  bool  on  Enables/disables GpuJoin by HashJoin    pg_strom.enable_gpunestloop  bool  on  Enables/disables GpuJoin by NestLoop    pg_strom.enable_gpupreagg  bool  on  Enables/disables GpuPreAgg    pg_strom.enable_partitionwise_gpujoin  bool  on  Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later.    pg_strom.enable_partitionwise_gpupreagg  bool  on  Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later.    pg_strom.pullup_outer_scan  bool  on  Enables/disables to pull up full-table scan if it is just below GpuPreAgg/GpuJoin, to reduce data transfer between CPU/RAM and GPU.    pg_strom.pullup_outer_join  bool  on  Enables/disables to pull up tables-join if GpuJoin is just below GpuPreAgg, to reduce data transfer between CPU/RAM and GPU.    pg_strom.enable_numeric_type  bool  on  Enables/disables support of  numeric  data type in arithmetic expression on GPU device    pg_strom.cpu_fallback  bool  off  Controls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\"    pg_strom.nvme_strom_enabled  bool  on  Enables/disables the feature of SSD-to-GPU Direct SQL Execution    pg_strom.nvme_strom_threshold  int  \u81ea\u52d5  Controls the table-size threshold to invoke the feature of SSD-to-GPU Direct SQL Execution", 
            "title": "Enables/disables a particular feature"
        }, 
        {
            "location": "/ref_params/#optimizer-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.chunk_size  int  65534kB  Size of the data blocks processed by a single GPU kernel invocation. It was configurable, but makes less sense, so fixed to about 64MB in the current version.    pg_strom.gpu_setup_cost  real  4000  Cost value for initialization of GPU device    pg_strom.gpu_dma_cost  real  10  Cost value for DMA transfer over PCIe bus per data-chunk (64MB)    pg_strom.gpu_operator_cost  real  0.00015  Cost value to process an expression formula on GPU. If larger value than  cpu_operator_cost  is configured, no chance to choose PG-Strom towards any size of tables", 
            "title": "Optimizer Configuration"
        }, 
        {
            "location": "/ref_params/#executor-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.global_max_async_tasks  int  160  Number of asynchronous taks PG-Strom can throw into GPU's execution queue in the whole system.    pg_strom.local_max_async_tasks  int  8  Number of asynchronous taks PG-Strom can throw into GPU's execution queue per process. If CPU parallel is used in combination, this limitation shall be applied for each background worker. So, more than  pg_strom.local_max_async_tasks  asynchronous tasks are executed in parallel on the entire batch job.    pg_strom.max_number_of_gpucontext  int  auto  Specifies the number of internal data structure  GpuContext  to abstract GPU device. Usually, no need to expand the initial value.", 
            "title": "Executor Configuration"
        }, 
        {
            "location": "/ref_params/#columnar-cache-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.ccache_base_dir  string  '/dev/shm'  Specifies the directory path to store columnar cache data files. Usually, no need to change from  /dev/shm  where  tmpfs  is mounted at.    pg_strom.ccache_total_size  int  auto  Upper limit of the columnar cache in kB. Default is the smaller in 75% of volume size or 66% of system physical memory.", 
            "title": "Columnar Cache Configuration"
        }, 
        {
            "location": "/ref_params/#gstore_fdw-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.gstore_max_relations  int  100  Upper limit of the number of foreign tables with gstore_fdw. It needs restart to update the parameter.", 
            "title": "gstore_fdw Configuration"
        }, 
        {
            "location": "/ref_params/#configuration-of-gpu-code-generation-and-build", 
            "text": "Parameter  Type  Default  Description      pg_strom.program_cache_size  int  256MB  Amount of the shared memory size to cache GPU programs already built. It needs restart to update the parameter.    pg_strom.num_program_builders  int  2  Number of background workers to build GPU programs asynchronously. It needs restart to update the parameter.    pg_strom.debug_jit_compile_options  bool  off  Controls to include debug option (line-numbers and symbol information) on JIT compile of GPU programs. It is valuable for complicated bug analysis using GPU core dump, however, should not be enabled on daily use because of performance degradation.    pg_strom.debug_kernel_source  bool  off  If enables,  EXPLAIN VERBOSE  command also prints out file paths of GPU programs written out.", 
            "title": "Configuration of GPU code generation and build"
        }, 
        {
            "location": "/ref_params/#gpu-device-configuration", 
            "text": "Parameter  Type  Default  Description      pg_strom.cuda_visible_devices  string  ''  List of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable  CUDAVISIBLE_DEVICES    pg_strom.gpu_memory_segment_size  int  512MB  Specifies the amount of device memory to be allocated per CUDA API call. Larger configuration will reduce the overhead of API calls, but not efficient usage of device memory.    pg_strom.max_num_preserved_gpu_memory  int  2048  Upper limit of the number of preserved GPU device memory segment. Usually, don't need to change from the default value.", 
            "title": "GPU Device Configuration"
        }, 
        {
            "location": "/release_note/", 
            "text": "PG-Strom v2.0 Release\n\n\nPG-Strom Development Team (17-Apr-2018)\n\n\n\nOverview\n\n\nMajor enhancement in PG-Strom v2.0 includes:\n\n\n\n\nOverall redesign of the internal infrastructure to manage GPU and stabilization\n\n\nCPU+GPU hybrid parallel execution\n\n\nSSD-to-GPU Direct SQL Execution\n\n\nIn-memory columnar cache\n\n\nGPU memory store (gstore_fdw)\n\n\nRedesign of GpuJoin and GpuPreAgg and speed-up\n\n\nGpuPreAgg + GpuJoin + GpuScan combined GPU kernel\n\n\n\n\nYou can download the summary of new features from: \nPG-Strom v2.0 Technical Brief\n.\n\n\nPrerequisites\n\n\n\n\nPostgreSQL v9.6, v10\n\n\nCUDA Toolkit 9.1\n\n\nLinux distributions supported by CUDA Toolkit\n\n\nIntel x86 64bit architecture (x86_64)\n\n\nNVIDIA GPU CC 6.0 or later (Pascal or Volta)\n\n\n\n\nNew Features\n\n\n\n\n\n\nEntire re-design and stabilization of the internal infrastructure to manage GPU device.\n\n\n\n\nPostgreSQL backend process simultaneously uses only one GPU at most. In case of multi-GPUs installation, it assumes combination use with CPU parallel execution of PostgreSQL. Usually, it is not a matter because throughput of CPU to provide data to GPU is much narrower than capability of GPU processors. We prioritized simpleness of the software architecture.\n\n\nWe began to utilize the demand paging feature of GPU device memory supported at the GPU models since Pascal generation. In most of SQL workloads, we cannot know exact size of the required result buffer prior to its execution, therefore, we had allocated more buffer than estimated buffer length, and retried piece of the workloads if estimated buffer size is not sufficient actually. This design restricts available resources of GPU which can be potentially used for other concurrent processes, and complicated error-retry logic was a nightmare for software quality. The demand paging feature allows to eliminate and simplify these stuffs.\n\n\nWe stop to use CUDA asynchronous interface. Use of the demand paging feature on GPU device memory makes asynchronous APIs for DMA (like \ncuMemCpyHtoD\n) perform synchronously, then it reduces concurrency and usage ratio of GPU kernels. Instead of the CUDA asynchronous APIs, PG-Strom manages its own worker threads which call synchronous APIs for each. As a by-product, we also could eliminate asynchronous callbacks (\ncuStreamAddCallback\n), it allows to use MPS daemon which has a restriction at this API.\n\n\n\n\n\n\n\n\nCPU+GPU Hybrid Parallel Execution\n\n\n\n\nCPU parallel execution at PostgreSQL v9.6 is newly supported.\n\n\nCustomScan logic of GpuScan, GpuJoin and GpuPreAgg provided by PG-Strom are executable on multiple background worker processes of PostgreSQL in parallel.\n\n\nLimitation: PG-Strom's own statistics displayed at \nEXPLAIN ANALYZE\n if CPU parallel execution. Because PostgreSQL v9.6 does not provide \nShutdownCustomScan\n callback of the CustomScan interface, coordinator process has no way to reclaim information of worker processes prior to the release of DSM (Dynamic Shared Memory) segment.\n\n\n\n\n\n\n\n\nSSD-to-GPU Direct SQL Execution\n\n\n\n\nBy cooperation with the \nnvme_strom\n Linux kernel module, it enables to load PostgreSQL's data blocks on NVMe-SSD to GPU device memory directly, bypassing the CPU and host buffer. This feature enables to apply PG-Strom on the area which have to process large data set more than system RAM size.\n\n\nIt allows to pull out pretty high throughput close to the hardware limitation because its data stream skips block-device or filesystem layer. Then, GPU runs SQL workloads that usually reduce the amount of data to be processed by CPU. The chemical reaction of these characteristics enables to redefine GPU's role as accelerator of I/O workloads also, not only computing intensive workloads.\n\n\n\n\n\n\n\n\nIn-memory Columnar Cache\n\n\n\n\nFor middle size data-set loadable onto the system RAM, it allows to cache data-blocks in column format which is more suitable for GPU computing. If cached data-blocks are found during table scan, PG-Strom prefers to reference the columnar cache more than shared buffer of PostgreSQL.\n\n\nIn-memory columnar cache can be built synchronously, or asynchronously by the background workers.\n\n\nYou may remember very early revision of PG-Strom had similar feature. In case when a cached tuple gets updated, the latest in-memory columnar cache which we newly implemented in v2.0 invalidates the cache block which includes the updated tuples. It never updates the columnar cache according to the updates of row-store, so performance degradation is quite limited.\n\n\n\n\n\n\n\n\nGPU Memory Store (gstore_fdw)\n\n\n\n\nIt enables to write to / read from preserved GPU device memory region by SELECT/INSERT/UPDATE/DELETE in SQL-level, using foreign table interface.\n\n\nIn v2.0, only \npgstrom\n internal data format is supported. It saves written data using PG-Strom's buffer format of \nKDS_FORMAT_COLUMN\n. It can compress variable length data using LZ algorithm.\n\n\nIn v2.0, GPU memory store can be used as data source of PL/CUDA user defined function.\n\n\n\n\n\n\n\n\nRedesign and performance improvement of GpuJoin and GpuPreAgg\n\n\n\n\nStop using Dynamic Parallelism which we internally used in GpuJoin and GpuPreAgg, and revised entire logic of these operations. Old design had a problem of less GPU usage ratio because a GPU kernel which launches GPU sub-kernel and just waits for its completion occupied GPU's execution slot.\n\n\nA coproduct of this redesign is suspend/resume of GpuJoin. In principle, JOIN operation of SQL may generate larger number of rows than number of input rows, but preliminary not predictive. The new design allows to suspend GPU kernel once buffer available space gets lacked, then resume with new result buffer. It simplifies size estimation logic of the result buffer, and eliminates GPU kernel retry by lack of buffer on run-time.\n\n\n\n\n\n\n\n\nGpuPreAgg+GpuJoin+GpuScan combined GPU kernel\n\n\n\n\nIn case when GPU executable SCAN, JOIN and GROUP BY are serially cascaded, a single GPU kernel invocation runs a series of tasks equivalent to the GpuScan, GpuJoin and GpuPreAgg. This is an approach to minimize data exchange between CPU and GPU. For example, result buffer of GpuJoin is used as input buffer of GpuPreAgg.\n\n\nThis feature is especially valuable if combined with SSD-to-GPU Direct SQL Execution.\n\n\n\n\n\n\n\n\nPL/CUDA Enhancement\n\n\n\n\n#plcuda_include\n is enhanced to specify SQL function which returns \ntext\n type. It can change the code block to inject according to the argument, so it also allows to generate multiple GPU kernel variations, not only inclusion of externally defined functions.\n\n\nIf PL/CUDA takes \nreggstore\n type argument, GPU kernel function receives pointer of the GPU memory store. Note that it does not pass the OID value.\n\n\n\n\n\n\n\n\nOther Enhancement\n\n\n\n\nlo_import_gpu\n and \nlo_export_gpu\n functions allows to import contents of the GPU device memory acquired by external applications directly, or export contents of the largeobject to the GPU device memory.\n\n\n\n\n\n\n\n\nPackaging\n\n\n\n\nAdd RPM packages to follow the PostgreSQL packages distributed by PostgreSQL Global Development Group.\n\n\nAll the software packages are available at HeteroDB SWDC(Software Distribution Center) and downloadable.\n\n\n\n\n\n\n\n\nDocument\n\n\n\n\nPG-Strom documentation was entirely rewritten using markdown and mkdocs. It makes documentation maintenance easier than the previous HTML based approach, so expects timely updates according to the development of new features.\n\n\n\n\n\n\n\n\nTest\n\n\n\n\nRegression test for PG-Strom was built on top of the regression test framework of PostgreSQL.\n\n\n\n\n\n\n\n\nDropped features\n\n\n\n\n\n\nPostgreSQL v9.5 Support\n\n\n\n\nPostgreSQL v9.6 had big changes in both of the optimizer and executor to support CPU parallel query execution. The biggest change for extension modules that interact them is an enhancement of the interface called \"upper planner path-ification\". It allows to choose an optimal execution-plan from the multiple candidates based on the estimated cost, even if it is aggregation or sorting.\n\n\nIt is fundamentally different from the older way where we rewrote query execution plan to inject GpuPreAgg using the hooks. It allows to inject GpuPreAgg node in more reasonable and reliable way, and we could drop complicated (and buggy) logic to rewrite query execution plan once constructed.\n\n\nCustomScan interface is also enhanced to support CPU parallel execution. Due to the reason, we dropped PostgreSQL v9.5 support to follow these new enhancement.\n\n\n\n\n\n\n\n\nGpuSort feature\n\n\n\n\nWe dropped GpuSort because we have little advantages in the performance.\n\n\nSorting is one of the GPU suitable workloads. However, in case when we try to sort data blocks larger than GPU device memory, we have to split the data blocks into multiple chunks, then partially sort them and merge them by CPU to generate final results.\n\n\nLarger chunk size is better to reduce the load to merge multiple chunks by CPU, on the other hands, larger chunk size takes larger lead time to launch GPU kernel to sort. It means here is a trade-off; which disallows asynchronous processing by PG-Strom to make data transfer latency invisible.\n\n\nIt is hard to solve the problem, or too early to solve the problem, we dropped GpuSort feature once.", 
            "title": "Release Note"
        }, 
        {
            "location": "/release_note/#pg-strom-v20-release", 
            "text": "PG-Strom Development Team (17-Apr-2018)", 
            "title": "PG-Strom v2.0 Release"
        }, 
        {
            "location": "/release_note/#overview", 
            "text": "Major enhancement in PG-Strom v2.0 includes:   Overall redesign of the internal infrastructure to manage GPU and stabilization  CPU+GPU hybrid parallel execution  SSD-to-GPU Direct SQL Execution  In-memory columnar cache  GPU memory store (gstore_fdw)  Redesign of GpuJoin and GpuPreAgg and speed-up  GpuPreAgg + GpuJoin + GpuScan combined GPU kernel   You can download the summary of new features from:  PG-Strom v2.0 Technical Brief .", 
            "title": "Overview"
        }, 
        {
            "location": "/release_note/#prerequisites", 
            "text": "PostgreSQL v9.6, v10  CUDA Toolkit 9.1  Linux distributions supported by CUDA Toolkit  Intel x86 64bit architecture (x86_64)  NVIDIA GPU CC 6.0 or later (Pascal or Volta)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/release_note/#new-features", 
            "text": "Entire re-design and stabilization of the internal infrastructure to manage GPU device.   PostgreSQL backend process simultaneously uses only one GPU at most. In case of multi-GPUs installation, it assumes combination use with CPU parallel execution of PostgreSQL. Usually, it is not a matter because throughput of CPU to provide data to GPU is much narrower than capability of GPU processors. We prioritized simpleness of the software architecture.  We began to utilize the demand paging feature of GPU device memory supported at the GPU models since Pascal generation. In most of SQL workloads, we cannot know exact size of the required result buffer prior to its execution, therefore, we had allocated more buffer than estimated buffer length, and retried piece of the workloads if estimated buffer size is not sufficient actually. This design restricts available resources of GPU which can be potentially used for other concurrent processes, and complicated error-retry logic was a nightmare for software quality. The demand paging feature allows to eliminate and simplify these stuffs.  We stop to use CUDA asynchronous interface. Use of the demand paging feature on GPU device memory makes asynchronous APIs for DMA (like  cuMemCpyHtoD ) perform synchronously, then it reduces concurrency and usage ratio of GPU kernels. Instead of the CUDA asynchronous APIs, PG-Strom manages its own worker threads which call synchronous APIs for each. As a by-product, we also could eliminate asynchronous callbacks ( cuStreamAddCallback ), it allows to use MPS daemon which has a restriction at this API.     CPU+GPU Hybrid Parallel Execution   CPU parallel execution at PostgreSQL v9.6 is newly supported.  CustomScan logic of GpuScan, GpuJoin and GpuPreAgg provided by PG-Strom are executable on multiple background worker processes of PostgreSQL in parallel.  Limitation: PG-Strom's own statistics displayed at  EXPLAIN ANALYZE  if CPU parallel execution. Because PostgreSQL v9.6 does not provide  ShutdownCustomScan  callback of the CustomScan interface, coordinator process has no way to reclaim information of worker processes prior to the release of DSM (Dynamic Shared Memory) segment.     SSD-to-GPU Direct SQL Execution   By cooperation with the  nvme_strom  Linux kernel module, it enables to load PostgreSQL's data blocks on NVMe-SSD to GPU device memory directly, bypassing the CPU and host buffer. This feature enables to apply PG-Strom on the area which have to process large data set more than system RAM size.  It allows to pull out pretty high throughput close to the hardware limitation because its data stream skips block-device or filesystem layer. Then, GPU runs SQL workloads that usually reduce the amount of data to be processed by CPU. The chemical reaction of these characteristics enables to redefine GPU's role as accelerator of I/O workloads also, not only computing intensive workloads.     In-memory Columnar Cache   For middle size data-set loadable onto the system RAM, it allows to cache data-blocks in column format which is more suitable for GPU computing. If cached data-blocks are found during table scan, PG-Strom prefers to reference the columnar cache more than shared buffer of PostgreSQL.  In-memory columnar cache can be built synchronously, or asynchronously by the background workers.  You may remember very early revision of PG-Strom had similar feature. In case when a cached tuple gets updated, the latest in-memory columnar cache which we newly implemented in v2.0 invalidates the cache block which includes the updated tuples. It never updates the columnar cache according to the updates of row-store, so performance degradation is quite limited.     GPU Memory Store (gstore_fdw)   It enables to write to / read from preserved GPU device memory region by SELECT/INSERT/UPDATE/DELETE in SQL-level, using foreign table interface.  In v2.0, only  pgstrom  internal data format is supported. It saves written data using PG-Strom's buffer format of  KDS_FORMAT_COLUMN . It can compress variable length data using LZ algorithm.  In v2.0, GPU memory store can be used as data source of PL/CUDA user defined function.     Redesign and performance improvement of GpuJoin and GpuPreAgg   Stop using Dynamic Parallelism which we internally used in GpuJoin and GpuPreAgg, and revised entire logic of these operations. Old design had a problem of less GPU usage ratio because a GPU kernel which launches GPU sub-kernel and just waits for its completion occupied GPU's execution slot.  A coproduct of this redesign is suspend/resume of GpuJoin. In principle, JOIN operation of SQL may generate larger number of rows than number of input rows, but preliminary not predictive. The new design allows to suspend GPU kernel once buffer available space gets lacked, then resume with new result buffer. It simplifies size estimation logic of the result buffer, and eliminates GPU kernel retry by lack of buffer on run-time.     GpuPreAgg+GpuJoin+GpuScan combined GPU kernel   In case when GPU executable SCAN, JOIN and GROUP BY are serially cascaded, a single GPU kernel invocation runs a series of tasks equivalent to the GpuScan, GpuJoin and GpuPreAgg. This is an approach to minimize data exchange between CPU and GPU. For example, result buffer of GpuJoin is used as input buffer of GpuPreAgg.  This feature is especially valuable if combined with SSD-to-GPU Direct SQL Execution.     PL/CUDA Enhancement   #plcuda_include  is enhanced to specify SQL function which returns  text  type. It can change the code block to inject according to the argument, so it also allows to generate multiple GPU kernel variations, not only inclusion of externally defined functions.  If PL/CUDA takes  reggstore  type argument, GPU kernel function receives pointer of the GPU memory store. Note that it does not pass the OID value.     Other Enhancement   lo_import_gpu  and  lo_export_gpu  functions allows to import contents of the GPU device memory acquired by external applications directly, or export contents of the largeobject to the GPU device memory.     Packaging   Add RPM packages to follow the PostgreSQL packages distributed by PostgreSQL Global Development Group.  All the software packages are available at HeteroDB SWDC(Software Distribution Center) and downloadable.     Document   PG-Strom documentation was entirely rewritten using markdown and mkdocs. It makes documentation maintenance easier than the previous HTML based approach, so expects timely updates according to the development of new features.     Test   Regression test for PG-Strom was built on top of the regression test framework of PostgreSQL.", 
            "title": "New Features"
        }, 
        {
            "location": "/release_note/#dropped-features", 
            "text": "PostgreSQL v9.5 Support   PostgreSQL v9.6 had big changes in both of the optimizer and executor to support CPU parallel query execution. The biggest change for extension modules that interact them is an enhancement of the interface called \"upper planner path-ification\". It allows to choose an optimal execution-plan from the multiple candidates based on the estimated cost, even if it is aggregation or sorting.  It is fundamentally different from the older way where we rewrote query execution plan to inject GpuPreAgg using the hooks. It allows to inject GpuPreAgg node in more reasonable and reliable way, and we could drop complicated (and buggy) logic to rewrite query execution plan once constructed.  CustomScan interface is also enhanced to support CPU parallel execution. Due to the reason, we dropped PostgreSQL v9.5 support to follow these new enhancement.     GpuSort feature   We dropped GpuSort because we have little advantages in the performance.  Sorting is one of the GPU suitable workloads. However, in case when we try to sort data blocks larger than GPU device memory, we have to split the data blocks into multiple chunks, then partially sort them and merge them by CPU to generate final results.  Larger chunk size is better to reduce the load to merge multiple chunks by CPU, on the other hands, larger chunk size takes larger lead time to launch GPU kernel to sort. It means here is a trade-off; which disallows asynchronous processing by PG-Strom to make data transfer latency invisible.  It is hard to solve the problem, or too early to solve the problem, we dropped GpuSort feature once.", 
            "title": "Dropped features"
        }
    ]
}