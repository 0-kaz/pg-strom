{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This chapter introduces the overview of PG-Strom, and developer's community. What is PG-Strom? PG-Strom is an extension module of PostgreSQL designed for version 9.6 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set. Its core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications. Unlike some DWH systems, PG-Strom shares the storage system of PostgreSQL which saves data in row-format. It is not always best choice for summary or analytics workloads, however, it is also an advantage as well. Users don't need to export and transform the data from transactional database for processing. PG-Strom v2.0 enhanced the capability of reading from storage. SSD-to-GPU Direct SQL mechanism allows to load from storage (NVME-SSD) to GPU directly, and supply data to GPU that runs SQL workloads. On the other hands, the feature of PL/CUDA and gstore_fdw allows to run highly computing density problems, like advanced statistical analytics or machine learning, on the database management system, and to return only results to users. License and Copyright PG-Strom is an open source software distributed under the GPL(GNU Public License) v2. See LICENSE for the license details. PG-Strom Development Team reserves the copyright of the software. PG-Strom Development Team is an international, unincorporated association of individuals and companies who have contributed to the PG-Strom project, but not a legal entity. Community We have a community mailing-list at: PG-Strom community ML It is a right place to post questions, requests, troubles and etc, related to PG-Strom project. Please pay attention it is a public list for world wide. So, it is your own responsibility not to disclose confidential information. The primary language of the mailing-list is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach (JP) prefix on the subject like, for non-Japanese speakers to skip messages. Bug or troubles report If you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue with bug tag at the PG-Strom Issue Tracker . Please ensure the items below on bug reports. Whether you can reproduce the same problem on the latest revision? Hopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software. Whether you can reproduce the same problem if PG-Strom is disabled? GUC option pg_strom.enabled can turn on/off PG-Strom. Is there any known issues on the issue tracker of GitHub? Please don't forget to search closed issues The information below are helpful for bug-reports. Output of EXPLAIN VERBOSE for the queries in trouble. Data structure of the tables involved with \\d+ <table name> on psql command. Log messages (verbose messages are more helpful) Status of GUC options you modified from the default configurations. Hardware configuration - GPU model and host RAM size especially. If you are not certain whether the strange behavior on your site is bug or not, please report it to the mailing-list prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information. New features proposition If you have any ideas of new features, please open a new issue with feature tag at the PG-Strom Issue Tracker , then have a discussion with other developers. A preferable design proposal will contain the items below. What is your problem to solve / improve? How much serious is it on your workloads / user case? Way to implement your idea? Expected downside, if any. Once we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed. Once a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again. On the development stage, please attach patch file on the issue ticket. We don't use pull request. Support Policy The PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release. Please note that it is volunteer based community support policy, so our support is best effort and no SLA definition. If you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom). Versioning Policy PG-Strom's version number is consists of two portion; major and minor version. <major>.<minor> Its minor version shall be incremented for each release; including bug fixes and new features. Its major version shall be incremented in the following situation. Some of supported PostgreSQL version gets deprecated. Some of supported GPU devices gets deprecated. New version adds epoch making features.","title":"Home"},{"location":"#what-is-pg-strom","text":"PG-Strom is an extension module of PostgreSQL designed for version 9.6 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set. Its core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications. Unlike some DWH systems, PG-Strom shares the storage system of PostgreSQL which saves data in row-format. It is not always best choice for summary or analytics workloads, however, it is also an advantage as well. Users don't need to export and transform the data from transactional database for processing. PG-Strom v2.0 enhanced the capability of reading from storage. SSD-to-GPU Direct SQL mechanism allows to load from storage (NVME-SSD) to GPU directly, and supply data to GPU that runs SQL workloads. On the other hands, the feature of PL/CUDA and gstore_fdw allows to run highly computing density problems, like advanced statistical analytics or machine learning, on the database management system, and to return only results to users.","title":"What is PG-Strom?"},{"location":"#license-and-copyright","text":"PG-Strom is an open source software distributed under the GPL(GNU Public License) v2. See LICENSE for the license details. PG-Strom Development Team reserves the copyright of the software. PG-Strom Development Team is an international, unincorporated association of individuals and companies who have contributed to the PG-Strom project, but not a legal entity.","title":"License and Copyright"},{"location":"#community","text":"We have a community mailing-list at: PG-Strom community ML It is a right place to post questions, requests, troubles and etc, related to PG-Strom project. Please pay attention it is a public list for world wide. So, it is your own responsibility not to disclose confidential information. The primary language of the mailing-list is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach (JP) prefix on the subject like, for non-Japanese speakers to skip messages.","title":"Community"},{"location":"#bug-or-troubles-report","text":"If you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue with bug tag at the PG-Strom Issue Tracker . Please ensure the items below on bug reports. Whether you can reproduce the same problem on the latest revision? Hopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software. Whether you can reproduce the same problem if PG-Strom is disabled? GUC option pg_strom.enabled can turn on/off PG-Strom. Is there any known issues on the issue tracker of GitHub? Please don't forget to search closed issues The information below are helpful for bug-reports. Output of EXPLAIN VERBOSE for the queries in trouble. Data structure of the tables involved with \\d+ <table name> on psql command. Log messages (verbose messages are more helpful) Status of GUC options you modified from the default configurations. Hardware configuration - GPU model and host RAM size especially. If you are not certain whether the strange behavior on your site is bug or not, please report it to the mailing-list prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information.","title":"Bug or troubles report"},{"location":"#new-features-proposition","text":"If you have any ideas of new features, please open a new issue with feature tag at the PG-Strom Issue Tracker , then have a discussion with other developers. A preferable design proposal will contain the items below. What is your problem to solve / improve? How much serious is it on your workloads / user case? Way to implement your idea? Expected downside, if any. Once we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed. Once a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again. On the development stage, please attach patch file on the issue ticket. We don't use pull request.","title":"New features proposition"},{"location":"#support-policy","text":"The PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release. Please note that it is volunteer based community support policy, so our support is best effort and no SLA definition. If you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom).","title":"Support Policy"},{"location":"#versioning-policy","text":"PG-Strom's version number is consists of two portion; major and minor version. <major>.<minor> Its minor version shall be incremented for each release; including bug fixes and new features. Its major version shall be incremented in the following situation. Some of supported PostgreSQL version gets deprecated. Some of supported GPU devices gets deprecated. New version adds epoch making features.","title":"Versioning Policy"},{"location":"arrow_fdw/","text":"Columnar data store (Arrow_Fdw) Overview PostgreSQL tables internally consist of 8KB blocks 1 , and block contains tuples which is a data structure of all the attributes and metadata per row. It collocates date of a row closely, so it works effectively for INSERT/UPDATE-major workloads, but not suitable for summarizing or analytics of mass-data. It is not usual to reference all the columns in a table on mass-data processing, and we tend to reference a part of columns in most cases. In this case, the storage I/O bandwidth consumed by unreferenced columns are waste, however, we have no easy way to fetch only particular columns referenced from the row-oriented data structure. In case of column oriented data structure, in an opposite manner, it has extreme disadvantage on INSERT/UPDATE-major workloads, however, it can pull out maximum performance of storage I/O on mass-data processing workloads because it can loads only referenced columns. From the standpoint of processor efficiency also, column-oriented data structure looks like a flat array that pulls out maximum bandwidth of memory subsystem for GPU, by special memory access pattern called Coalesced Memory Access. What is Apache Arrow? Apache Arrow is a data format of structured data to save in columnar-form and to exchange other applications. Some applications for big-data processing support the format, and it is easy for self-developed applications to use Apache Arrow format since they provides libraries for major programming languages like C,C++ or Python. Apache Arrow format file internally contains Schema portion to define data structure, and one or more RecordBatch to save columnar-data based on the schema definition. For data types, it supports integers, strint (variable-length), date/time types and so on. Indivisual columnar data has its internal representation according to the data types. Data representation in Apache Arrow is not identical with the representation in PostgreSQL. For example, epoch of timestamp in Arrow is 1970-01-01 and it supports multiple precision. On the other hands, epoch of timestamp in PostgreSQL is 2001-01-01 and it has microseconds accuracy. Arrow_Fdw allows to read Apache Arrow files on PostgreSQL using foreign table mechanism. If an Arrow file contains 8 of record batches that has million items for each column data, for example, we can access 8 million rows on the Arrow files through the foreign table. Operations Creation of foreign tables Usually it takes the 3 steps below to create a foreign table. Define a foreign-data-wrapper using CREATE FOREIGN DATA WRAPPER command Define a foreign server using CREATE SERVER command Define a foreign table using CREATE FOREIGN TABLE command The first 2 steps above are included in the CREATE EXTENSION pg_strom command. All you need to run individually is CREATE FOREIGN TABLE command last. CREATE FOREIGN TABLE flogdata ( ts timestamp, sensor_id int, signal1 smallint, signal2 smallint, signal3 smallint, signal4 smallint, ) SERVER arrow_fdw OPTIONS (file '/path/to/logdata.arrow'); Data type of columns specified by the CREATE FOREIGN TABLE command must be matched to schema definition of the Arrow files to be mapped. Arrow_Fdw also supports a useful manner using IMPORT FOREIGN SCHEMA statement. It automatically generates a foreign table definition using schema definition of the Arrow files. It specifies the foreign table name, schema name to import, and path name of the Arrow files using OPTION-clause. Schema definition of Arrow files contains data types and optional column name for each column. It declares a new foreign table using these information. IMPORT FOREIGN SCHEMA flogdata FROM SERVER arrow_fdw INTO public OPTIONS (file '/path/to/logdata.arrow'); Foreign table options Arrow_Fdw supports the options below. Right now, all the options are for foreign tables. Target Option Description foreign table file It maps an Arrow file specified on the foreign table. foreign table files It maps multiple Arrow files specified by comma (,) separated files list on the foreign table. foreign table dir It maps all the Arrow files in the directory specified on the foreign table. foreign table suffix When dir option is given, it maps only files with the specified suffix, like .arrow for example. Data type mapping Arrow data types are mapped on PostgreSQL data types as follows. Arrow data types PostgreSQL data types Remarks Int int2,int4,int8 is_signed attribute is ignored. bitWidth attribute supports only 16,32 or 64. FloatingPoint float2,float4,float8 float2 is enhanced by PG-Strom. Binary bytea Utf8 text Decimal numeric Date date Adjusted as if unitsz=Day Time time Adjusted as if unitsz=MicroSecond Timestamp timestamp Adjusted as if unitsz=MicroSecond Interval interval List array of base type It supports only 1-dimensional List(WIP). Struct composite type PG composite type must be preliminary defined. Union -------- FixedSizeBinary char(n) FixedSizeList -------- Map -------- How to read EXPLAIN EXPLAIN command show us information about Arrow files reading. The example below is an output of query execution plan that includes flineorder foreign table that mapps an Arrow file of 309GB. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM flineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN ----------------------------------------------------------------------------------------------------- Aggregate (cost=12632759.02..12632759.03 rows=1 width=32) -> Custom Scan (GpuPreAgg) (cost=12632754.43..12632757.49 rows=204 width=8) Reduction: NoGroup Combined GpuJoin: enabled GPU Preference: GPU0 (Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on flineorder (cost=9952.15..12638126.98 rows=572635 width=12) Outer Scan: flineorder (cost=9877.70..12649677.69 rows=4010017 width=16) Outer Scan Filter: ((lo_discount >= 1) AND (lo_discount <= 3) AND (lo_quantity < 25)) Depth 1: GpuHashJoin (nrows 4010017...572635) HashKeys: flineorder.lo_orderdate JoinQuals: (flineorder.lo_orderdate = date1.d_datekey) KDS-Hash (size: 66.06KB) GPU Preference: GPU0 (Tesla V100-PCIE-16GB) NVMe-Strom: enabled referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/nvme/lineorder_s401.arrow (size: 309.23GB) -> Seq Scan on date1 (cost=0.00..78.95 rows=365 width=4) Filter: (d_year = 1993) (18 rows) According to the EXPLAIN output, we can see Custom Scan (GpuJoin) scans flineorder foreign table. file0 item shows the filename ( /opt/nvme/lineorder_s401.arrow ) on behalf of the foreign table and its size. If multiple files are mapped, any files are individually shown, like file1 , file2 , ... The referenced item shows the list of referenced columns. We can see this query touches lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns. In addition, GPU Preference: GPU0 (Tesla V100-PCIE-16GB) and NVMe-Strom: enabled shows us the scan on flineorder uses SSD-to-GPU Direct SQL mechanism. VERBOSE option outputs more detailed information. =# EXPLAIN VERBOSE SELECT sum(lo_extendedprice*lo_discount) as revenue FROM flineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=12632759.02..12632759.03 rows=1 width=32) Output: sum((pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount)))) -> Custom Scan (GpuPreAgg) (cost=12632754.43..12632757.49 rows=204 width=8) Output: (pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount))) Reduction: NoGroup GPU Projection: flineorder.lo_extendedprice, flineorder.lo_discount, pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount)) Combined GpuJoin: enabled GPU Preference: GPU0 (Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on public.flineorder (cost=9952.15..12638126.98 rows=572635 width=12) Output: flineorder.lo_extendedprice, flineorder.lo_discount GPU Projection: flineorder.lo_extendedprice::bigint, flineorder.lo_discount::integer Outer Scan: public.flineorder (cost=9877.70..12649677.69 rows=4010017 width=16) Outer Scan Filter: ((flineorder.lo_discount >= 1) AND (flineorder.lo_discount <= 3) AND (flineorder.lo_quantity < 25)) Depth 1: GpuHashJoin (nrows 4010017...572635) HashKeys: flineorder.lo_orderdate JoinQuals: (flineorder.lo_orderdate = date1.d_datekey) KDS-Hash (size: 66.06KB) GPU Preference: GPU0 (Tesla V100-PCIE-16GB) NVMe-Strom: enabled referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/nvme/lineorder_s401.arrow (size: 309.23GB) lo_orderpriority: 33.61GB lo_extendedprice: 17.93GB lo_ordertotalprice: 17.93GB lo_revenue: 17.93GB -> Seq Scan on public.date1 (cost=0.00..78.95 rows=365 width=4) Output: date1.d_datekey Filter: (date1.d_year = 1993) (28 rows) The verbose output additionally displays amount of column-data to be loaded on reference of columns. The load of lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns needs to read 87.4GB in total. It is 28.3% towards the filesize (309.2GB). How to make Arrow files This section introduces the way to transform dataset already stored in PostgreSQL database system into Apache Arrow file. Using PyArrow+Pandas A pair of PyArrow module, developed by Arrow developers community, and Pandas data frame can dump PostgreSQL database into an Arrow file. The example below reads all the data in table t0 , then write out them into /tmp/t0.arrow . import pyarrow as pa import pandas as pd X = pd.read_sql(sql=\"SELECT * FROM t0\", con=\"postgresql://localhost/postgres\") Y = pa.Table.from_pandas(X) f = pa.RecordBatchFileWriter('/tmp/t0.arrow', Y.schema) f.write_table(Y,1000000) # RecordBatch for each million rows f.close() Please note that the above operation once keeps query result of the SQL on memory, so should pay attention on memory consumption if you want to transfer massive rows at once. Using Pg2Arrow On the other hand, pg2arrow command, developed by PG-Strom Development Team, enables us to write out query result into Arrow file. This tool is designed to write out massive amount of data into storage device like NVME-SSD. It fetch query results from PostgreSQL database system, and write out Record Batches of Arrow format for each data size specified by the -s|--segment-size option. Thus, its memory consumption is relatively reasonable. pg2arrow command is distributed with PG-Strom. It shall be installed on the bin directory of PostgreSQL related utilities. $ pg2arrow --help Usage: pg2arrow [OPTION]... [DBNAME [USERNAME]] General options: -d, --dbname=DBNAME database name to connect to -c, --command=COMMAND SQL command to run -f, --file=FILENAME SQL command from file (-c and -f are exclusive, either of them must be specified) -o, --output=FILENAME result file in Apache Arrow format (default creates a temporary file) Arrow format options: -s, --segment-size=SIZE size of record batch for each (default: 256MB) Connection options: -h, --host=HOSTNAME database server host -p, --port=PORT database server port -U, --username=USERNAME database user name -w, --no-password never prompt for password -W, --password force password prompt Debug options: --dump=FILENAME dump information of arrow file --progress shows progress of the job. Report bugs to <pgstrom@heterodbcom. The -h or -U option specifies the connection parameters of PostgreSQL, like psql or pg_dump . The simplest usage of this command is running a SQL command specified by -c|--command option on PostgreSQL server, then write out results into the file specified by -o|--output option in Arrow format. The example below reads all the data in table t0 , then write out them into the file /tmp/t0.arrow . $ pg2arrow -U kaigai -d postgres -c \"SELECT * FROM t0\" -o /tmp/t0.arrow Although it is an option for developers, --dump <filename> prints schema definition and record-batch location and size of Arrow file in human readable form. Advanced Usage SSDtoGPU Direct SQL In case when all the Arrow files mapped on the Arrow_Fdw foreign table satisfies the terms below, PG-Strom enables SSD-to-GPU Direct SQL to load columnar data. Arrow files are on NVME-SSD volume. NVME-SSD volume is managed by Ext4 filesystem. Total size of Arrow files exceeds the pg_strom.nvme_strom_threshold configuration. Partition configuration Arrow_Fdw foreign tables can be used as a part of partition leafs. Usual PostgreSQL tables can be mixtured with Arrow_Fdw foreign tables. So, pay attention Arrow_Fdw foreign table does not support any writer operations. And, make boundary condition of the partition consistent to the contents of the mapped Arrow file. It is a responsibility of the database administrators. A typical usage scenario is processing of long-standing accumulated log-data. Unlike transactional data, log-data is mostly write-once and will never be updated / deleted. Thus, by migration of the log-data after a lapse of certain period into Arrow_Fdw foreign table that is read-only but rapid processing, we can accelerate summarizing and analytics workloads. In addition, log-data likely have timestamp, so it is quite easy design to add partition leafs periodically, like monthly, weekly or others. The example below defines a partitioned table that mixes a normal PostgreSQL table and Arrow_Fdw foreign tables. The normal PostgreSQL table, is read-writable, is specified as default partition 2 , so DBA can migrate only past log-data into Arrow_Fdw foreign table under the database system operations. CREATE TABLE lineorder ( lo_orderkey numeric, lo_linenumber integer, lo_custkey numeric, lo_partkey integer, lo_suppkey numeric, lo_orderdate integer, lo_orderpriority character(15), lo_shippriority character(1), lo_quantity numeric, lo_extendedprice numeric, lo_ordertotalprice numeric, lo_discount numeric, lo_revenue numeric, lo_supplycost numeric, lo_tax numeric, lo_commit_date character(8), lo_shipmode character(10) ) PARTITION BY RANGE (lo_orderdate); CREATE TABLE lineorder__now PARTITION OF lineorder default; CREATE FOREIGN TABLE lineorder__1993 PARTITION OF lineorder FOR VALUES FROM (19930101) TO (19940101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1993.arrow'); CREATE FOREIGN TABLE lineorder__1994 PARTITION OF lineorder FOR VALUES FROM (19940101) TO (19950101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1994.arrow'); CREATE FOREIGN TABLE lineorder__1995 PARTITION OF lineorder FOR VALUES FROM (19950101) TO (19960101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1995.arrow'); CREATE FOREIGN TABLE lineorder__1996 PARTITION OF lineorder FOR VALUES FROM (19960101) TO (19970101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1996.arrow'); Below is the query execution plan towards the table. By the query condition lo_orderdate between 19950701 and 19960630 that touches boundary condition of the partition, the partition leaf lineorder__1993 and lineorder__1994 are pruned, so it makes a query execution plan to read other (foreign) tables only. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM lineorder,date1 WHERE lo_orderdate = d_datekey AND lo_orderdate between 19950701 and 19960630 AND lo_discount between 1 and 3 ABD lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=172088.90..172088.91 rows=1 width=32) -> Hash Join (cost=10548.86..172088.51 rows=77 width=64) Hash Cond: (lineorder__1995.lo_orderdate = date1.d_datekey) -> Append (cost=10444.35..171983.80 rows=77 width=67) -> Custom Scan (GpuScan) on lineorder__1995 (cost=10444.35..33671.87 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1995.arrow (size: 892.57MB) -> Custom Scan (GpuScan) on lineorder__1996 (cost=10444.62..33849.21 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1996.arrow (size: 897.87MB) -> Custom Scan (GpuScan) on lineorder__now (cost=11561.33..104462.33 rows=1 width=18) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) -> Hash (cost=72.56..72.56 rows=2556 width=4) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=4) (16 rows) The operation below extracts the data in 1997 from lineorder__now table, then move to a new Arrow_Fdw foreign table. $ pg2arrow -d sample -o /opt/tmp/lineorder_1997.arrow \\ -c \"SELECT * FROM lineorder WHERE lo_orderdate between 19970101 and 19971231\" pg2arrow command extracts the data in 1997 from the lineorder table into a new Arrow file. BEGIN; -- -- remove rows in 1997 from the read-writable table -- DELETE FROM lineorder WHERE lo_orderdate BETWEEN 19970101 AND 19971231; -- -- define a new partition leaf which maps log-data in 1997 -- CREATE FOREIGN TABLE lineorder__1997 PARTITION OF lineorder FOR VALUES FROM (19970101) TO (19980101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1997.arrow'); COMMIT; A series of operations above delete the data in 1997 from lineorder__new that is a PostgreSQL table, then maps an Arrow file ( /opt/tmp/lineorder_1997.arrow ) which contains an identical contents as a foreign table lineorder__1997 . For correctness, block size is configurable on build from 4KB to 32KB. \u21a9 Supported at PostgreSQL v11 or later. \u21a9","title":"Arrow_fdw"},{"location":"arrow_fdw/#overview","text":"PostgreSQL tables internally consist of 8KB blocks 1 , and block contains tuples which is a data structure of all the attributes and metadata per row. It collocates date of a row closely, so it works effectively for INSERT/UPDATE-major workloads, but not suitable for summarizing or analytics of mass-data. It is not usual to reference all the columns in a table on mass-data processing, and we tend to reference a part of columns in most cases. In this case, the storage I/O bandwidth consumed by unreferenced columns are waste, however, we have no easy way to fetch only particular columns referenced from the row-oriented data structure. In case of column oriented data structure, in an opposite manner, it has extreme disadvantage on INSERT/UPDATE-major workloads, however, it can pull out maximum performance of storage I/O on mass-data processing workloads because it can loads only referenced columns. From the standpoint of processor efficiency also, column-oriented data structure looks like a flat array that pulls out maximum bandwidth of memory subsystem for GPU, by special memory access pattern called Coalesced Memory Access.","title":"Overview"},{"location":"arrow_fdw/#what-is-apache-arrow","text":"Apache Arrow is a data format of structured data to save in columnar-form and to exchange other applications. Some applications for big-data processing support the format, and it is easy for self-developed applications to use Apache Arrow format since they provides libraries for major programming languages like C,C++ or Python. Apache Arrow format file internally contains Schema portion to define data structure, and one or more RecordBatch to save columnar-data based on the schema definition. For data types, it supports integers, strint (variable-length), date/time types and so on. Indivisual columnar data has its internal representation according to the data types. Data representation in Apache Arrow is not identical with the representation in PostgreSQL. For example, epoch of timestamp in Arrow is 1970-01-01 and it supports multiple precision. On the other hands, epoch of timestamp in PostgreSQL is 2001-01-01 and it has microseconds accuracy. Arrow_Fdw allows to read Apache Arrow files on PostgreSQL using foreign table mechanism. If an Arrow file contains 8 of record batches that has million items for each column data, for example, we can access 8 million rows on the Arrow files through the foreign table.","title":"What is Apache Arrow?"},{"location":"arrow_fdw/#operations","text":"","title":"Operations"},{"location":"arrow_fdw/#creation-of-foreign-tables","text":"Usually it takes the 3 steps below to create a foreign table. Define a foreign-data-wrapper using CREATE FOREIGN DATA WRAPPER command Define a foreign server using CREATE SERVER command Define a foreign table using CREATE FOREIGN TABLE command The first 2 steps above are included in the CREATE EXTENSION pg_strom command. All you need to run individually is CREATE FOREIGN TABLE command last. CREATE FOREIGN TABLE flogdata ( ts timestamp, sensor_id int, signal1 smallint, signal2 smallint, signal3 smallint, signal4 smallint, ) SERVER arrow_fdw OPTIONS (file '/path/to/logdata.arrow'); Data type of columns specified by the CREATE FOREIGN TABLE command must be matched to schema definition of the Arrow files to be mapped. Arrow_Fdw also supports a useful manner using IMPORT FOREIGN SCHEMA statement. It automatically generates a foreign table definition using schema definition of the Arrow files. It specifies the foreign table name, schema name to import, and path name of the Arrow files using OPTION-clause. Schema definition of Arrow files contains data types and optional column name for each column. It declares a new foreign table using these information. IMPORT FOREIGN SCHEMA flogdata FROM SERVER arrow_fdw INTO public OPTIONS (file '/path/to/logdata.arrow');","title":"Creation of foreign tables"},{"location":"arrow_fdw/#foreign-table-options","text":"Arrow_Fdw supports the options below. Right now, all the options are for foreign tables. Target Option Description foreign table file It maps an Arrow file specified on the foreign table. foreign table files It maps multiple Arrow files specified by comma (,) separated files list on the foreign table. foreign table dir It maps all the Arrow files in the directory specified on the foreign table. foreign table suffix When dir option is given, it maps only files with the specified suffix, like .arrow for example.","title":"Foreign table options"},{"location":"arrow_fdw/#data-type-mapping","text":"Arrow data types are mapped on PostgreSQL data types as follows. Arrow data types PostgreSQL data types Remarks Int int2,int4,int8 is_signed attribute is ignored. bitWidth attribute supports only 16,32 or 64. FloatingPoint float2,float4,float8 float2 is enhanced by PG-Strom. Binary bytea Utf8 text Decimal numeric Date date Adjusted as if unitsz=Day Time time Adjusted as if unitsz=MicroSecond Timestamp timestamp Adjusted as if unitsz=MicroSecond Interval interval List array of base type It supports only 1-dimensional List(WIP). Struct composite type PG composite type must be preliminary defined. Union -------- FixedSizeBinary char(n) FixedSizeList -------- Map --------","title":"Data type mapping"},{"location":"arrow_fdw/#how-to-read-explain","text":"EXPLAIN command show us information about Arrow files reading. The example below is an output of query execution plan that includes flineorder foreign table that mapps an Arrow file of 309GB. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM flineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN ----------------------------------------------------------------------------------------------------- Aggregate (cost=12632759.02..12632759.03 rows=1 width=32) -> Custom Scan (GpuPreAgg) (cost=12632754.43..12632757.49 rows=204 width=8) Reduction: NoGroup Combined GpuJoin: enabled GPU Preference: GPU0 (Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on flineorder (cost=9952.15..12638126.98 rows=572635 width=12) Outer Scan: flineorder (cost=9877.70..12649677.69 rows=4010017 width=16) Outer Scan Filter: ((lo_discount >= 1) AND (lo_discount <= 3) AND (lo_quantity < 25)) Depth 1: GpuHashJoin (nrows 4010017...572635) HashKeys: flineorder.lo_orderdate JoinQuals: (flineorder.lo_orderdate = date1.d_datekey) KDS-Hash (size: 66.06KB) GPU Preference: GPU0 (Tesla V100-PCIE-16GB) NVMe-Strom: enabled referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/nvme/lineorder_s401.arrow (size: 309.23GB) -> Seq Scan on date1 (cost=0.00..78.95 rows=365 width=4) Filter: (d_year = 1993) (18 rows) According to the EXPLAIN output, we can see Custom Scan (GpuJoin) scans flineorder foreign table. file0 item shows the filename ( /opt/nvme/lineorder_s401.arrow ) on behalf of the foreign table and its size. If multiple files are mapped, any files are individually shown, like file1 , file2 , ... The referenced item shows the list of referenced columns. We can see this query touches lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns. In addition, GPU Preference: GPU0 (Tesla V100-PCIE-16GB) and NVMe-Strom: enabled shows us the scan on flineorder uses SSD-to-GPU Direct SQL mechanism. VERBOSE option outputs more detailed information. =# EXPLAIN VERBOSE SELECT sum(lo_extendedprice*lo_discount) as revenue FROM flineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=12632759.02..12632759.03 rows=1 width=32) Output: sum((pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount)))) -> Custom Scan (GpuPreAgg) (cost=12632754.43..12632757.49 rows=204 width=8) Output: (pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount))) Reduction: NoGroup GPU Projection: flineorder.lo_extendedprice, flineorder.lo_discount, pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount)) Combined GpuJoin: enabled GPU Preference: GPU0 (Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on public.flineorder (cost=9952.15..12638126.98 rows=572635 width=12) Output: flineorder.lo_extendedprice, flineorder.lo_discount GPU Projection: flineorder.lo_extendedprice::bigint, flineorder.lo_discount::integer Outer Scan: public.flineorder (cost=9877.70..12649677.69 rows=4010017 width=16) Outer Scan Filter: ((flineorder.lo_discount >= 1) AND (flineorder.lo_discount <= 3) AND (flineorder.lo_quantity < 25)) Depth 1: GpuHashJoin (nrows 4010017...572635) HashKeys: flineorder.lo_orderdate JoinQuals: (flineorder.lo_orderdate = date1.d_datekey) KDS-Hash (size: 66.06KB) GPU Preference: GPU0 (Tesla V100-PCIE-16GB) NVMe-Strom: enabled referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/nvme/lineorder_s401.arrow (size: 309.23GB) lo_orderpriority: 33.61GB lo_extendedprice: 17.93GB lo_ordertotalprice: 17.93GB lo_revenue: 17.93GB -> Seq Scan on public.date1 (cost=0.00..78.95 rows=365 width=4) Output: date1.d_datekey Filter: (date1.d_year = 1993) (28 rows) The verbose output additionally displays amount of column-data to be loaded on reference of columns. The load of lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns needs to read 87.4GB in total. It is 28.3% towards the filesize (309.2GB).","title":"How to read EXPLAIN"},{"location":"arrow_fdw/#how-to-make-arrow-files","text":"This section introduces the way to transform dataset already stored in PostgreSQL database system into Apache Arrow file.","title":"How to make Arrow files"},{"location":"arrow_fdw/#using-pyarrowpandas","text":"A pair of PyArrow module, developed by Arrow developers community, and Pandas data frame can dump PostgreSQL database into an Arrow file. The example below reads all the data in table t0 , then write out them into /tmp/t0.arrow . import pyarrow as pa import pandas as pd X = pd.read_sql(sql=\"SELECT * FROM t0\", con=\"postgresql://localhost/postgres\") Y = pa.Table.from_pandas(X) f = pa.RecordBatchFileWriter('/tmp/t0.arrow', Y.schema) f.write_table(Y,1000000) # RecordBatch for each million rows f.close() Please note that the above operation once keeps query result of the SQL on memory, so should pay attention on memory consumption if you want to transfer massive rows at once.","title":"Using PyArrow+Pandas"},{"location":"arrow_fdw/#using-pg2arrow","text":"On the other hand, pg2arrow command, developed by PG-Strom Development Team, enables us to write out query result into Arrow file. This tool is designed to write out massive amount of data into storage device like NVME-SSD. It fetch query results from PostgreSQL database system, and write out Record Batches of Arrow format for each data size specified by the -s|--segment-size option. Thus, its memory consumption is relatively reasonable. pg2arrow command is distributed with PG-Strom. It shall be installed on the bin directory of PostgreSQL related utilities. $ pg2arrow --help Usage: pg2arrow [OPTION]... [DBNAME [USERNAME]] General options: -d, --dbname=DBNAME database name to connect to -c, --command=COMMAND SQL command to run -f, --file=FILENAME SQL command from file (-c and -f are exclusive, either of them must be specified) -o, --output=FILENAME result file in Apache Arrow format (default creates a temporary file) Arrow format options: -s, --segment-size=SIZE size of record batch for each (default: 256MB) Connection options: -h, --host=HOSTNAME database server host -p, --port=PORT database server port -U, --username=USERNAME database user name -w, --no-password never prompt for password -W, --password force password prompt Debug options: --dump=FILENAME dump information of arrow file --progress shows progress of the job. Report bugs to <pgstrom@heterodbcom. The -h or -U option specifies the connection parameters of PostgreSQL, like psql or pg_dump . The simplest usage of this command is running a SQL command specified by -c|--command option on PostgreSQL server, then write out results into the file specified by -o|--output option in Arrow format. The example below reads all the data in table t0 , then write out them into the file /tmp/t0.arrow . $ pg2arrow -U kaigai -d postgres -c \"SELECT * FROM t0\" -o /tmp/t0.arrow Although it is an option for developers, --dump <filename> prints schema definition and record-batch location and size of Arrow file in human readable form.","title":"Using Pg2Arrow"},{"location":"arrow_fdw/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"arrow_fdw/#ssdtogpu-direct-sql","text":"In case when all the Arrow files mapped on the Arrow_Fdw foreign table satisfies the terms below, PG-Strom enables SSD-to-GPU Direct SQL to load columnar data. Arrow files are on NVME-SSD volume. NVME-SSD volume is managed by Ext4 filesystem. Total size of Arrow files exceeds the pg_strom.nvme_strom_threshold configuration.","title":"SSDtoGPU Direct SQL"},{"location":"arrow_fdw/#partition-configuration","text":"Arrow_Fdw foreign tables can be used as a part of partition leafs. Usual PostgreSQL tables can be mixtured with Arrow_Fdw foreign tables. So, pay attention Arrow_Fdw foreign table does not support any writer operations. And, make boundary condition of the partition consistent to the contents of the mapped Arrow file. It is a responsibility of the database administrators. A typical usage scenario is processing of long-standing accumulated log-data. Unlike transactional data, log-data is mostly write-once and will never be updated / deleted. Thus, by migration of the log-data after a lapse of certain period into Arrow_Fdw foreign table that is read-only but rapid processing, we can accelerate summarizing and analytics workloads. In addition, log-data likely have timestamp, so it is quite easy design to add partition leafs periodically, like monthly, weekly or others. The example below defines a partitioned table that mixes a normal PostgreSQL table and Arrow_Fdw foreign tables. The normal PostgreSQL table, is read-writable, is specified as default partition 2 , so DBA can migrate only past log-data into Arrow_Fdw foreign table under the database system operations. CREATE TABLE lineorder ( lo_orderkey numeric, lo_linenumber integer, lo_custkey numeric, lo_partkey integer, lo_suppkey numeric, lo_orderdate integer, lo_orderpriority character(15), lo_shippriority character(1), lo_quantity numeric, lo_extendedprice numeric, lo_ordertotalprice numeric, lo_discount numeric, lo_revenue numeric, lo_supplycost numeric, lo_tax numeric, lo_commit_date character(8), lo_shipmode character(10) ) PARTITION BY RANGE (lo_orderdate); CREATE TABLE lineorder__now PARTITION OF lineorder default; CREATE FOREIGN TABLE lineorder__1993 PARTITION OF lineorder FOR VALUES FROM (19930101) TO (19940101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1993.arrow'); CREATE FOREIGN TABLE lineorder__1994 PARTITION OF lineorder FOR VALUES FROM (19940101) TO (19950101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1994.arrow'); CREATE FOREIGN TABLE lineorder__1995 PARTITION OF lineorder FOR VALUES FROM (19950101) TO (19960101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1995.arrow'); CREATE FOREIGN TABLE lineorder__1996 PARTITION OF lineorder FOR VALUES FROM (19960101) TO (19970101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1996.arrow'); Below is the query execution plan towards the table. By the query condition lo_orderdate between 19950701 and 19960630 that touches boundary condition of the partition, the partition leaf lineorder__1993 and lineorder__1994 are pruned, so it makes a query execution plan to read other (foreign) tables only. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM lineorder,date1 WHERE lo_orderdate = d_datekey AND lo_orderdate between 19950701 and 19960630 AND lo_discount between 1 and 3 ABD lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=172088.90..172088.91 rows=1 width=32) -> Hash Join (cost=10548.86..172088.51 rows=77 width=64) Hash Cond: (lineorder__1995.lo_orderdate = date1.d_datekey) -> Append (cost=10444.35..171983.80 rows=77 width=67) -> Custom Scan (GpuScan) on lineorder__1995 (cost=10444.35..33671.87 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1995.arrow (size: 892.57MB) -> Custom Scan (GpuScan) on lineorder__1996 (cost=10444.62..33849.21 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1996.arrow (size: 897.87MB) -> Custom Scan (GpuScan) on lineorder__now (cost=11561.33..104462.33 rows=1 width=18) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) -> Hash (cost=72.56..72.56 rows=2556 width=4) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=4) (16 rows) The operation below extracts the data in 1997 from lineorder__now table, then move to a new Arrow_Fdw foreign table. $ pg2arrow -d sample -o /opt/tmp/lineorder_1997.arrow \\ -c \"SELECT * FROM lineorder WHERE lo_orderdate between 19970101 and 19971231\" pg2arrow command extracts the data in 1997 from the lineorder table into a new Arrow file. BEGIN; -- -- remove rows in 1997 from the read-writable table -- DELETE FROM lineorder WHERE lo_orderdate BETWEEN 19970101 AND 19971231; -- -- define a new partition leaf which maps log-data in 1997 -- CREATE FOREIGN TABLE lineorder__1997 PARTITION OF lineorder FOR VALUES FROM (19970101) TO (19980101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1997.arrow'); COMMIT; A series of operations above delete the data in 1997 from lineorder__new that is a PostgreSQL table, then maps an Arrow file ( /opt/tmp/lineorder_1997.arrow ) which contains an identical contents as a foreign table lineorder__1997 . For correctness, block size is configurable on build from 4KB to 32KB. \u21a9 Supported at PostgreSQL v11 or later. \u21a9","title":"Partition configuration"},{"location":"brin/","text":"Index Support Overview PostgreSQL supports several index strategies. The default is B-tree that can rapidly fetch records with a particular value. Elsewhere, it also supports Hash, BRIN, GiST, GIN and others that have own characteristics for each. PG-Strom supports only BRIN-index right now. BRIN-index works efficiently on the dataset we can expect physically neighbor records have similar key values, like timestamp values of time-series data. It allows to skip blocks-range if any records in the range obviously don't match to the scan qualifiers, then, also enables to reduce the amount of I/O due to full table scan. PG-Strom also utilizes the feature of BRIN-index, to skip obviously unnecessary blocks from the ones to be loaded to GPU. Configuration No special configurations are needed to use BRIN-index. PG-Strom automatically applies BRIN-index based scan if BRIN-index is configured on the referenced columns and scan qualifiers are suitable to the index. Also see the PostgreSQL Documentation for the BRIN-index feature. By the GUC parameters below, PG-Strom enables/disables usage of BRIN-index. It usually don't need to change from the default configuration, except for debugging or trouble shooting. Parameter Type Default Description pg_strom.enable_brin bool on enables/disables usage of BRIN-index Operations By EXPLAIN , we can check whether BRIN-index is in use. postgres=# EXPLAIN ANALYZE SELECT * FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' AND cat LIKE '%aaa%'; QUERY PLAN -------------------------------------------------------------------------------- Custom Scan (GpuScan) on dt (cost=94810.93..176275.00 rows=169992 width=44) (actual time=1777.819..1901.537 rows=175277 loops=1) GPU Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date) AND (cat ~~ '%aaa%'::text)) Rows Removed by GPU Filter: 4385491 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 0.529 ms Execution time: 2323.063 ms (7 rows) In the example above, BRIN-index is configured on the ymd column. BRIN cond shows the qualifier of BRIN-index for concentration. BRIN skipped shows the number of skipped blocks actually. In this case, 424704 blocks are skipped, then, it filters out 4385491 rows in the loaded blocks by the scan qualifiers. GpuJoin and GpuPreAgg often pulls up its underlying table scan and runs the scan by itself, to reduce inefficient data transfer. In this case, it also uses the BRIN-index to concentrate the scan. The example below shows a usage of BRIN-index in a query which includes GROUP BY. postgres=# EXPLAIN ANALYZE SELECT cat,count(*) FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=6149.78..6151.86 rows=26 width=12) (actual time=427.482..427.499 rows=26 loops=1) Group Key: cat -> Sort (cost=6149.78..6150.24 rows=182 width=12) (actual time=427.465..427.467 rows=26 loops=1) Sort Key: cat Sort Method: quicksort Memory: 26kB -> Custom Scan (GpuPreAgg) on dt (cost=6140.68..6142.95 rows=182 width=12) (actual time=427.331..427.339 rows=26 loops=1) Reduction: Local Outer Scan: dt (cost=4000.00..4011.99 rows=4541187 width=4) (actual time=78.573..415.961 rows=4560768 loops=1) Outer Scan Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) Rows Removed by Outer Scan Filter: 15564 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 30.992 ms Execution time: 818.994 ms (14 rows)","title":"Index Support"},{"location":"brin/#overview","text":"PostgreSQL supports several index strategies. The default is B-tree that can rapidly fetch records with a particular value. Elsewhere, it also supports Hash, BRIN, GiST, GIN and others that have own characteristics for each. PG-Strom supports only BRIN-index right now. BRIN-index works efficiently on the dataset we can expect physically neighbor records have similar key values, like timestamp values of time-series data. It allows to skip blocks-range if any records in the range obviously don't match to the scan qualifiers, then, also enables to reduce the amount of I/O due to full table scan. PG-Strom also utilizes the feature of BRIN-index, to skip obviously unnecessary blocks from the ones to be loaded to GPU.","title":"Overview"},{"location":"brin/#configuration","text":"No special configurations are needed to use BRIN-index. PG-Strom automatically applies BRIN-index based scan if BRIN-index is configured on the referenced columns and scan qualifiers are suitable to the index. Also see the PostgreSQL Documentation for the BRIN-index feature. By the GUC parameters below, PG-Strom enables/disables usage of BRIN-index. It usually don't need to change from the default configuration, except for debugging or trouble shooting. Parameter Type Default Description pg_strom.enable_brin bool on enables/disables usage of BRIN-index","title":"Configuration"},{"location":"brin/#operations","text":"By EXPLAIN , we can check whether BRIN-index is in use. postgres=# EXPLAIN ANALYZE SELECT * FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' AND cat LIKE '%aaa%'; QUERY PLAN -------------------------------------------------------------------------------- Custom Scan (GpuScan) on dt (cost=94810.93..176275.00 rows=169992 width=44) (actual time=1777.819..1901.537 rows=175277 loops=1) GPU Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date) AND (cat ~~ '%aaa%'::text)) Rows Removed by GPU Filter: 4385491 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 0.529 ms Execution time: 2323.063 ms (7 rows) In the example above, BRIN-index is configured on the ymd column. BRIN cond shows the qualifier of BRIN-index for concentration. BRIN skipped shows the number of skipped blocks actually. In this case, 424704 blocks are skipped, then, it filters out 4385491 rows in the loaded blocks by the scan qualifiers. GpuJoin and GpuPreAgg often pulls up its underlying table scan and runs the scan by itself, to reduce inefficient data transfer. In this case, it also uses the BRIN-index to concentrate the scan. The example below shows a usage of BRIN-index in a query which includes GROUP BY. postgres=# EXPLAIN ANALYZE SELECT cat,count(*) FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=6149.78..6151.86 rows=26 width=12) (actual time=427.482..427.499 rows=26 loops=1) Group Key: cat -> Sort (cost=6149.78..6150.24 rows=182 width=12) (actual time=427.465..427.467 rows=26 loops=1) Sort Key: cat Sort Method: quicksort Memory: 26kB -> Custom Scan (GpuPreAgg) on dt (cost=6140.68..6142.95 rows=182 width=12) (actual time=427.331..427.339 rows=26 loops=1) Reduction: Local Outer Scan: dt (cost=4000.00..4011.99 rows=4541187 width=4) (actual time=78.573..415.961 rows=4560768 loops=1) Outer Scan Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) Rows Removed by Outer Scan Filter: 15564 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 30.992 ms Execution time: 818.994 ms (14 rows)","title":"Operations"},{"location":"gstore_fdw/","text":"GPU Memory Store(gstore_fdw) Overview Usually, PG-Strom uses GPU device memory for temporary purpose only. It allocates a certain amount of device memory needed for query execution, then transfers data blocks and launch GPU kernel to process SQL workloads. Once GPU kernel gets finished, these device memory regison shall be released soon, to re-allocate unused device memory for other workloads. This design allows concurrent multiple session or scan workloads on the tables larger than GPU device memory. It may not be optimal depending on circumstances. A typical example is, repeated calculation under various conditions for data with a scale large enough to fit in the GPU device memory, not so large. This applies to workloads such as machine-learning, pattern matching or similarity search. For modern GPUs, it is not so difficult to process a few gigabytes data on memory at most, but it is a costly process to setup data to be loaded onto GPU device memory and transfer them. In addition, since variable length data in PostgreSQL has size limitation up to 1GB, it restricts the data format when it is givrn as an argument of PL/CUDA function, even if the data size itself is sufficient in the GPU device memory. GPU memory store (gstore_fdw) is a feature to preserve GPU device memory and to load data to the memory preliminary. It makes unnecessary to setup arguments and load for each invocation of PL/CUDA function, and eliminates 1GB limitation of variable length data because it allows GPU device memory allocation up to the capacity. As literal, gstore_fdw is implemented using foreign-data-wrapper of PostgreSQL. You can modify the data structure on GPU device memory using INSERT , UPDATE or DELETE commands on the foreign table managed by gstore_fdw. In the similar way, you can also read the data using SELECT command. PL/CUDA function can reference the data stored onto GPU device memory through the foreign table. Right now, GPU programs which is transparently generated from SQL statement cannot reference this device memory region, however, we plan to enhance the feature in the future release. Setup Usually it takes the 3 steps below to create a foreign table. Define a foreign-data-wrapper using CREATE FOREIGN DATA WRAPPER command Define a foreign server using CREATE SERVER command Define a foreign table using CREATE FOREIGN TABLE command The first 2 steps above are included in the CREATE EXTENSION pg_strom command. All you need to run individually is CREATE FOREIGN TABLE command last. CREATE FOREIGN TABLE ft ( id int, signature smallint[] OPTIONS (compression 'pglz') ) SERVER gstore_fdw OPTIONS(pinning '0', format 'pgstrom'); You can specify some options on creation of foreign table using CREATE FOREIGN TABLE command. SERVER gstore_fdw is a mandatory option. It indicates the new foreign table is managed by gstore_fdw. The options below are supported in the OPTIONS clause. name target description pinning table Specifies device number of the GPU where device memory is preserved. format table Specifies the internal data format on GPU device memory. Default is pgstrom compression column Specifies whether variable length data is compressed, or not. Default is uncompressed. Right now, only pgstrom is supported for format option. It is an original data format of PG-Strom to store structured data of PostgreSQL in columnar format. In most cases, no need to pay attention to internal data format on writing / reading GPU data store using SQL. On the other hands, you need to consider when you program PL/CUDA function or share the GPU device memory with external applications using IPC handle. Right now, only pglz is supported for compression option. This compression logic adopts an identical data format and algorithm used by PostgreSQL to compress variable length data larger than its threshold. It can be decompressed by GPU internal function pglz_decompress() from PL/CUDA function. Due to the characteristics of the compression algorithm, it is valuable to represent sparse matrix that is mostly zero. Operations Loading data Like normal tables, you can write GPU device memory on behalf of the foreign table using INSERT , UPDATE and DELETE command. Note that gstore_fdw acquires SHARE UPDATE EXCLUSIVE lock on the beginning of these commands. It means only single transaction can update the gstore_fdw foreign table at a certain point. It is a trade-off. We don't need to check visibility per record when PL/CUDA function references gstore_fdw foreign table. Any contents written to the gstore_fdw foreign table is not visible to other sessions until transaction getting committed, like regular tables. This is a significant feature to ensure atomicity of transaction, however, it also means the older revision of gstore_fdw foreign table contents must be kept on the GPU device memory until any concurrent transaction which may reference the older revision gets committed or aborted. So, even though you can run INSERT , UPDATE or DELETE commands as if it is regular tables, you should avoidto update several rows then commit transaction many times. Basically, INSERT of massive rows at once (bulk loading) is recommended. Unlike regular tables, contents of the gstore_fdw foreign table is vollatile. So, it is very easy to loose contents of the gstore_fdw foreign table by power-down or PostgreSQL restart. So, what we load onto gstore_fdw foreign table should be reconstructable by other data source. Checking the memory consumption See pgstrom.gstore_fdw_chunk_info system view to see amount of the device memory consumed by gstore_fdw. postgres=# select * from pgstrom.gstore_fdw_chunk_info ; database_oid | table_oid | revision | xmin | xmax | pinning | format | rawsize | nitems --------------+-----------+----------+------+------+---------+---------+-----------+---------- 13806 | 26800 | 3 | 2 | 0 | 0 | pgstrom | 660000496 | 15000000 13806 | 26797 | 2 | 2 | 0 | 0 | pgstrom | 440000496 | 10000000 (2 rows) By nvidia-smi command, you can check how much device memory is consumed for each GPU device. \"PG-Strom GPU memory keeper\" process actually keeps and manages the device memory area acquired by Gstore_fdw. In this example, 1211MB is preliminary allocated for total of the above rawsize (about 1100MB) and CUDA internal usage. $ nvidia-smi Wed Apr 4 15:11:50 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.30 Driver Version: 390.30 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P40 Off | 00000000:02:00.0 Off | 0 | | N/A 39C P0 52W / 250W | 1221MiB / 22919MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 6885 C ...bgworker: PG-Strom GPU memory keeper 1211MiB | +-----------------------------------------------------------------------------+ Internal Data Format See the notes for details of the internal data format when gstore_fdw write on GPU device memory. Detail of the pgstrom format Inter-process Data Collaboration CUDA provides special APIs cuIpcGetMemHandle() and cuIpcOpenMemHandle() . The first allows to get a unique identifier of GPU device memory allocated by applications. The other one allows to reference a shared GPU device memory region from other applications. In the other words, it supports something like a shared memory on the host system. This unique identifier is CUipcMemHandle object; which is simple binary data in 64bytes. This session introduces SQL functions which exchange GPU device memory with other applications using CUipcMemHandle identifier. SQL Functions to gstore_export_ipchandle(reggstore) This function gets CUipcMemHandle identifier of the GPU device memory which is preserved by gstore_fdw foreign table, then returns as a binary data in bytea type. If foreign table is empty and has no GPU device memory, it returns NULL. 1st arg( ftable_oid ): OID of the foreign table. Because it is reggstore type, you can specify the foreign table by name string. result: CUipcMemHandle identifier in the bytea type. # select gstore_export_ipchandle('ft'); gstore_export_ipchandle ------------------------------------------------------------------------------------------------------------------------------------ \\xe057880100000000de3a000000000000904e7909000000000000800900000000000000000000000000020000000000005c000000000000001200d0c10101005c (1 row) lo_import_gpu(int, bytea, bigint, bigint, oid=0) This function temporary opens the GPU device memory region acquired by external applications, then read this region and writes out as a largeobject of PostgreSQL. If largeobject already exists, its contents is replaced by the data read from the GPU device memory. It keeps owner and permission configuration. Elsewhere, it creates a new largeobject, then write out the data which is read from GPU device memory. 1st arg( device_nr ): GPU device number where device memory is acquired 2nd arg( ipc_mhandle ): CUipcMemHandle identifier in bytea type 3rd( offset ): offset of the head position to read, from the GPU device memory region. 4th( length ): size to read in bytes 5th( loid ): OID of the largeobject to be written. 0 is assumed, if no valid value is supplied. result: OID of the written largeobject lo_export_gpu(oid, int, bytea, bigint, bigint) 1st arg( loid ): OID of the largeobject to be read 2nd arg( device_nr ): GPU device number where device memory is acquired 3rd arg( ipc_mhandle ): CUipcMemHandle identifier in bytea type 4th arg( offset ): offset of the head position to write, from the GPU device memory region. 5th arg( length ): size to write in bytes result: Length of bytes actually written. If length of the largeobject is less then length , it may return the value less than length .","title":"Gstore_fdw"},{"location":"gstore_fdw/#overview","text":"Usually, PG-Strom uses GPU device memory for temporary purpose only. It allocates a certain amount of device memory needed for query execution, then transfers data blocks and launch GPU kernel to process SQL workloads. Once GPU kernel gets finished, these device memory regison shall be released soon, to re-allocate unused device memory for other workloads. This design allows concurrent multiple session or scan workloads on the tables larger than GPU device memory. It may not be optimal depending on circumstances. A typical example is, repeated calculation under various conditions for data with a scale large enough to fit in the GPU device memory, not so large. This applies to workloads such as machine-learning, pattern matching or similarity search. For modern GPUs, it is not so difficult to process a few gigabytes data on memory at most, but it is a costly process to setup data to be loaded onto GPU device memory and transfer them. In addition, since variable length data in PostgreSQL has size limitation up to 1GB, it restricts the data format when it is givrn as an argument of PL/CUDA function, even if the data size itself is sufficient in the GPU device memory. GPU memory store (gstore_fdw) is a feature to preserve GPU device memory and to load data to the memory preliminary. It makes unnecessary to setup arguments and load for each invocation of PL/CUDA function, and eliminates 1GB limitation of variable length data because it allows GPU device memory allocation up to the capacity. As literal, gstore_fdw is implemented using foreign-data-wrapper of PostgreSQL. You can modify the data structure on GPU device memory using INSERT , UPDATE or DELETE commands on the foreign table managed by gstore_fdw. In the similar way, you can also read the data using SELECT command. PL/CUDA function can reference the data stored onto GPU device memory through the foreign table. Right now, GPU programs which is transparently generated from SQL statement cannot reference this device memory region, however, we plan to enhance the feature in the future release.","title":"Overview"},{"location":"gstore_fdw/#setup","text":"Usually it takes the 3 steps below to create a foreign table. Define a foreign-data-wrapper using CREATE FOREIGN DATA WRAPPER command Define a foreign server using CREATE SERVER command Define a foreign table using CREATE FOREIGN TABLE command The first 2 steps above are included in the CREATE EXTENSION pg_strom command. All you need to run individually is CREATE FOREIGN TABLE command last. CREATE FOREIGN TABLE ft ( id int, signature smallint[] OPTIONS (compression 'pglz') ) SERVER gstore_fdw OPTIONS(pinning '0', format 'pgstrom'); You can specify some options on creation of foreign table using CREATE FOREIGN TABLE command. SERVER gstore_fdw is a mandatory option. It indicates the new foreign table is managed by gstore_fdw. The options below are supported in the OPTIONS clause. name target description pinning table Specifies device number of the GPU where device memory is preserved. format table Specifies the internal data format on GPU device memory. Default is pgstrom compression column Specifies whether variable length data is compressed, or not. Default is uncompressed. Right now, only pgstrom is supported for format option. It is an original data format of PG-Strom to store structured data of PostgreSQL in columnar format. In most cases, no need to pay attention to internal data format on writing / reading GPU data store using SQL. On the other hands, you need to consider when you program PL/CUDA function or share the GPU device memory with external applications using IPC handle. Right now, only pglz is supported for compression option. This compression logic adopts an identical data format and algorithm used by PostgreSQL to compress variable length data larger than its threshold. It can be decompressed by GPU internal function pglz_decompress() from PL/CUDA function. Due to the characteristics of the compression algorithm, it is valuable to represent sparse matrix that is mostly zero.","title":"Setup"},{"location":"gstore_fdw/#operations","text":"","title":"Operations"},{"location":"gstore_fdw/#loading-data","text":"Like normal tables, you can write GPU device memory on behalf of the foreign table using INSERT , UPDATE and DELETE command. Note that gstore_fdw acquires SHARE UPDATE EXCLUSIVE lock on the beginning of these commands. It means only single transaction can update the gstore_fdw foreign table at a certain point. It is a trade-off. We don't need to check visibility per record when PL/CUDA function references gstore_fdw foreign table. Any contents written to the gstore_fdw foreign table is not visible to other sessions until transaction getting committed, like regular tables. This is a significant feature to ensure atomicity of transaction, however, it also means the older revision of gstore_fdw foreign table contents must be kept on the GPU device memory until any concurrent transaction which may reference the older revision gets committed or aborted. So, even though you can run INSERT , UPDATE or DELETE commands as if it is regular tables, you should avoidto update several rows then commit transaction many times. Basically, INSERT of massive rows at once (bulk loading) is recommended. Unlike regular tables, contents of the gstore_fdw foreign table is vollatile. So, it is very easy to loose contents of the gstore_fdw foreign table by power-down or PostgreSQL restart. So, what we load onto gstore_fdw foreign table should be reconstructable by other data source.","title":"Loading data"},{"location":"gstore_fdw/#checking-the-memory-consumption","text":"See pgstrom.gstore_fdw_chunk_info system view to see amount of the device memory consumed by gstore_fdw. postgres=# select * from pgstrom.gstore_fdw_chunk_info ; database_oid | table_oid | revision | xmin | xmax | pinning | format | rawsize | nitems --------------+-----------+----------+------+------+---------+---------+-----------+---------- 13806 | 26800 | 3 | 2 | 0 | 0 | pgstrom | 660000496 | 15000000 13806 | 26797 | 2 | 2 | 0 | 0 | pgstrom | 440000496 | 10000000 (2 rows) By nvidia-smi command, you can check how much device memory is consumed for each GPU device. \"PG-Strom GPU memory keeper\" process actually keeps and manages the device memory area acquired by Gstore_fdw. In this example, 1211MB is preliminary allocated for total of the above rawsize (about 1100MB) and CUDA internal usage. $ nvidia-smi Wed Apr 4 15:11:50 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.30 Driver Version: 390.30 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P40 Off | 00000000:02:00.0 Off | 0 | | N/A 39C P0 52W / 250W | 1221MiB / 22919MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 6885 C ...bgworker: PG-Strom GPU memory keeper 1211MiB | +-----------------------------------------------------------------------------+","title":"Checking the memory consumption"},{"location":"gstore_fdw/#internal-data-format","text":"See the notes for details of the internal data format when gstore_fdw write on GPU device memory. Detail of the pgstrom format","title":"Internal Data Format"},{"location":"gstore_fdw/#inter-process-data-collaboration","text":"CUDA provides special APIs cuIpcGetMemHandle() and cuIpcOpenMemHandle() . The first allows to get a unique identifier of GPU device memory allocated by applications. The other one allows to reference a shared GPU device memory region from other applications. In the other words, it supports something like a shared memory on the host system. This unique identifier is CUipcMemHandle object; which is simple binary data in 64bytes. This session introduces SQL functions which exchange GPU device memory with other applications using CUipcMemHandle identifier.","title":"Inter-process Data Collaboration"},{"location":"gstore_fdw/#sql-functions-to","text":"","title":"SQL Functions to"},{"location":"gstore_fdw/#gstore_export_ipchandlereggstore","text":"This function gets CUipcMemHandle identifier of the GPU device memory which is preserved by gstore_fdw foreign table, then returns as a binary data in bytea type. If foreign table is empty and has no GPU device memory, it returns NULL. 1st arg( ftable_oid ): OID of the foreign table. Because it is reggstore type, you can specify the foreign table by name string. result: CUipcMemHandle identifier in the bytea type. # select gstore_export_ipchandle('ft'); gstore_export_ipchandle ------------------------------------------------------------------------------------------------------------------------------------ \\xe057880100000000de3a000000000000904e7909000000000000800900000000000000000000000000020000000000005c000000000000001200d0c10101005c (1 row)","title":"gstore_export_ipchandle(reggstore)"},{"location":"gstore_fdw/#lo_import_gpuint-bytea-bigint-bigint-oid0","text":"This function temporary opens the GPU device memory region acquired by external applications, then read this region and writes out as a largeobject of PostgreSQL. If largeobject already exists, its contents is replaced by the data read from the GPU device memory. It keeps owner and permission configuration. Elsewhere, it creates a new largeobject, then write out the data which is read from GPU device memory. 1st arg( device_nr ): GPU device number where device memory is acquired 2nd arg( ipc_mhandle ): CUipcMemHandle identifier in bytea type 3rd( offset ): offset of the head position to read, from the GPU device memory region. 4th( length ): size to read in bytes 5th( loid ): OID of the largeobject to be written. 0 is assumed, if no valid value is supplied. result: OID of the written largeobject","title":"lo_import_gpu(int, bytea, bigint, bigint, oid=0)"},{"location":"gstore_fdw/#lo_export_gpuoid-int-bytea-bigint-bigint","text":"1st arg( loid ): OID of the largeobject to be read 2nd arg( device_nr ): GPU device number where device memory is acquired 3rd arg( ipc_mhandle ): CUipcMemHandle identifier in bytea type 4th arg( offset ): offset of the head position to write, from the GPU device memory region. 5th arg( length ): size to write in bytes result: Length of bytes actually written. If length of the largeobject is less then length , it may return the value less than length .","title":"lo_export_gpu(oid, int, bytea, bigint, bigint)"},{"location":"install/","text":"This chapter introduces the steps to install PG-Strom. Checklist Server Hardware It requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices. note002:HW Validation List may help you to choose the hardware. SSD-to-GPU Direct SQL Execution needs SSD devices which support NVMe specification, and to be installed under the same PCIe Root Complex where GPU is located on. GPU Device PG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later; note001:GPU Availability Matrix shows more detailed information. Check this list for the support status of SSD-to-GPU Direct SQL Execution. Operating System PG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or CentOS version 7.x series. - SSD-to-GPU Direct SQL Execution needs Red Hat Enterprise Linux or CentOS version 7.3 or later. PostgreSQL PG-Strom requires PostgreSQL version 9.6 or later. PostgreSQL v9.6 renew the custom-scan interface for CPU-parallel execution or GROUP BY planning, thus, it allows cooperation of custom-plans provides by extension modules. CUDA Toolkit PG-Strom requires CUDA Toolkit version 9.2 or later. Some of CUDA Driver APIs used by PG-Strom internally are not included in the former versions. OS Installation Choose a Linux distribution which is supported by CUDA Toolkit, then install the system according to the installation process of the distribution. NVIDIA DEVELOPER ZONE introduces the list of Linux distributions which are supported by CUDA Toolkit. In case of Red Hat Enterprise Linux 7.x or CentOS 7.x series, choose \"Minimal installation\" as base environment, and also check the following add-ons. Debugging Tools Development Tools Post OS Installation Configuration Next to the OS installation, a few additionsl configurations are required to install GPU-drivers and NVMe-Strom driver on the later steps. Setup EPEL Repository Several software modules required by PG-Strom are distributed as a part of EPEL (Extra Packages for Enterprise Linux). You need to add a repository definition of EPEL packages for yum system to obtain these software. One of the package we will get from EPEL repository is DKMS (Dynamic Kernel Module Support). It is a framework to build Linux kernel module for the running Linux kernel on demand; used for NVIDIA's GPU driver or NVMe-Strom which is a kernel module to support SSD-to-GPU Direct SQL Execution. epel-release package provides the repository definition of EPEL. You can obtain this package from the public FTP site of Fedora Project. Downloads the epel-release-<distribution version>.noarch.rpm , and install the package. Once epel-release package gets installed, yum system configuration is updated to get software from the EPEL repository. Fedora Project Public FTP Site https://dl.fedoraproject.org/pub/epel/7/x86_64/ Tip Walk down the directory: Packages --> e , from the above URL. Install the epel-release package as follows. $ sudo yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: epel-release noarch 7-11 /epel-release-7-11.noarch 24 k Transaction Summary ================================================================================ Install 1 Package : Installed: epel-release.noarch 0:7-11 Complete! HeteroDB-SWDC Installation PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . You need to add a repository definition of HeteroDB-SWDC for you system to obtain these software. heterodb-swdc package provides the repository definition of HeteroDB-SWDC. Access to the HeteroDB Software Distribution Center using Web browser, download the heterodb-swdc-1.0-1.el7.noarch.rpm on top of the file list, then install this package. Once heterodb-swdc package gets installed, yum system configuration is updated to get software from the HeteroDB-SWDC repository. Install the heterodb-swdc package as follows. $ sudo yum install https://heterodb.github.io/swdc/yum/rhel7-x86_64/heterodb-swdc-1.0-1.el7.noarch.rpm : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: heterodb-swdc noarch 1.0-1.el7 /heterodb-swdc-1.0-1.el7.noarch 2.4 k Transaction Summary ================================================================================ Install 1 Package : Installed: heterodb-swdc.noarch 0:1.0-1.el7 Complete! CUDA Toolkit Installation This section introduces the installation of CUDA Toolkit. If you already installed the latest CUDA Toolkit, you can skip this section. NVIDIA offers two approach to install CUDA Toolkit; one is by self-extracting archive (called runfile), and the other is by RPM packages. We recommend RPM installation because it allows simple software updates. You can download the installation package for CUDA Toolkit from NVIDIA DEVELOPER ZONE. Choose your OS, architecture, distribution and version, then choose \"rpm(network)\" edition. The \"rpm(network)\" edition contains only yum repositoty definition to distribute CUDA Toolkit. It is similar to the EPEL repository definition at the OS installation. So, you needs to installa the related RPM packages over network after the resistoration of CUDA repository. Run the following command. $ sudo rpm -i cuda-repo-<distribution>-<version>.x86_64.rpm $ sudo yum clean all $ sudo yum install cuda --enablerepo=rhel-7-server-e4s-optional-rpms or $ sudo yum install cuda Once installation completed successfully, CUDA Toolkit is deployed at /usr/local/cuda . Tip RHEL7 does not enable rhel-7-server-e4s-optional-rpms repository in the default. It distributes vulkan-filesystem packaged required by CUDA Toolkit installation. When you kick installation of CUDA Toolkit, edit /etc/yum.repos.d/redhat.repo to enable the repository, or use --enablerepo option of yum command to resolve dependency. $ ls /usr/local/cuda bin include libnsight nvml samples tools doc jre libnvvp nvvm share version.txt extras lib64 nsightee_plugins pkgconfig src Once installation gets completed, ensure the system recognizes the GPU devices correctly. nvidia-smi command shows GPU information installed on your system, as follows. $ nvidia-smi Wed Feb 14 09:43:48 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 387.26 Driver Version: 387.26 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla V100-PCIE... Off | 00000000:02:00.0 Off | 0 | | N/A 41C P0 37W / 250W | 0MiB / 16152MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Tip If nouveau driver which conflicts to nvidia driver is loaded, system cannot load the nvidia driver immediately. In this case, reboot the operating system after a configuration to disable the nouveau driver. If CUDA Toolkit is installed by the runfile installer, it also disables the nouveau driver. Elsewhere, in case of RPM installation, do the following configuration. To disable the nouveau driver, put the following configuration onto /etc/modprobe.d/disable-nouveau.conf , then run dracut command to apply them on the boot image of Linux kernel. # cat > /etc/modprobe.d/disable-nouveau.conf <<EOF blacklist nouveau options nouveau modeset=0 EOF # dracut -f PostgreSQL Installation This section introduces PostgreSQL installation with RPM. We don't introduce the installation steps from the source because there are many documents for this approach, and there are also various options for the ./configure script. PostgreSQL is also distributed in the packages of Linux distributions, however, it is not the latest one, and often older than the version which supports PG-Strom. For example, Red Hat Enterprise Linux 7.x or CentOS 7.x distributes PostgreSQL v9.2.x series. This version had been EOL by the PostgreSQL community. PostgreSQL Global Development Group provides yum repository to distribute the latest PostgreSQL and related packages. Like the configuration of EPEL, you can install a small package to set up yum repository, then install PostgreSQL and related software. Here is the list of yum repository definition: http://yum.postgresql.org/repopackages.php . Repository definitions are per PostgreSQL major version and Linux distribution. You need to choose the one for your Linux distribution, and for PostgreSQL v9.6 or later. All you need to install are yum repository definition, and PostgreSQL packages. If you choose PostgreSQL v10, the pakages below are required to install PG-Strom. postgresql10-devel postgresql10-server $ sudo yum install -y https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm $ sudo yum install -y postgresql10-server postgresql10-devel : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: postgresql10-devel x86_64 10.2-1PGDG.rhel7 pgdg10 2.0 M postgresql10-server x86_64 10.2-1PGDG.rhel7 pgdg10 4.4 M Installing for dependencies: postgresql10 x86_64 10.2-1PGDG.rhel7 pgdg10 1.5 M postgresql10-libs x86_64 10.2-1PGDG.rhel7 pgdg10 354 k Transaction Summary ================================================================================ Install 2 Packages (+2 Dependent packages) : Installed: postgresql10-devel.x86_64 0:10.2-1PGDG.rhel7 postgresql10-server.x86_64 0:10.2-1PGDG.rhel7 Dependency Installed: postgresql10.x86_64 0:10.2-1PGDG.rhel7 postgresql10-libs.x86_64 0:10.2-1PGDG.rhel7 Complete! The RPM packages provided by PostgreSQL Global Development Group installs software under the /usr/pgsql-<version> directory, so you may pay attention whether the PATH environment variable is configured appropriately. postgresql-alternative package set up symbolic links to the related commands under /usr/local/bin , so allows to simplify the operations. Also, it enables to switch target version using alternatives command even if multiple version of PostgreSQL. $ sudo yum install postgresql-alternatives : Resolving Dependencies --> Running transaction check ---> Package postgresql-alternatives.noarch 0:1.0-1.el7 will be installed --> Finished Dependency Resolution Dependencies Resolved : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: postgresql-alternatives noarch 1.0-1.el7 heterodb 9.2 k Transaction Summary ================================================================================ : Installed: postgresql-alternatives.noarch 0:1.0-1.el7 Complete! PG-Strom Installation RPM Installation PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . If you repository definition has been added, not many tasks are needed. We provide individual RPM packages of PG-Strom for each base PostgreSQL version. pg_strom-PG96 package is built for PostgreSQL 9.6, and pg_strom-PG10 is also built for PostgreSQL v10. $ sudo yum install pg_strom-PG10 : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: pg_strom-PG10 x86_64 1.9-180301.el7 heterodb 320 k Transaction Summary ================================================================================ : Installed: pg_strom-PG10.x86_64 0:1.9-180301.el7 Complete! That's all for package installation. Installation from the source For developers, we also introduces the steps to build and install PG-Strom from the source code. Getting the source code Like RPM packages, you can download tarball of the source code from HeteroDB Software Distribution Center . On the other hands, here is a certain time-lags to release the tarball, it may be preferable to checkout the master branch of PG-Strom on GitHub to use the latest development branch. $ git clone https://github.com/heterodb/pg-strom.git Cloning into 'pg-strom'... remote: Counting objects: 13797, done. remote: Compressing objects: 100% (215/215), done. remote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400 Receiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done. Resolving deltas: 100% (10504/10504), done. Building the PG-Strom Configuration to build PG-Strom must match to the target PostgreSQL strictly. For example, if a particular strcut has inconsistent layout by the configuration at build, it may lead problematic bugs; not easy to find out. Thus, not to have inconsistency, PG-Strom does not have own configure script, but references the build configuration of PostgreSQL using pg_config command. If PATH environment variable is set to the pg_config command of the target PostgreSQL, run make and make install . Elsewhere, give PG_CONFIG=... parameter on make command to tell the full path of the pg_config command. $ cd pg-strom $ make PG_CONFIG=/usr/pgsql-10/bin/pg_config $ sudo make install PG_CONFIG=/usr/pgsql-10/bin/pg_config Post Installation Setup Creation of database cluster Database cluster is not constructed yet, run initdb command to set up initial database of PostgreSQL. The default path of the database cluster on RPM installation is /var/lib/pgsql/<version number>/data . If you install postgresql-alternatives package, this default path can be referenced by /var/lib/pgdata regardless of the PostgreSQL version. $ sudo su - postgres $ initdb -D /var/lib/pgdata/ The files belonging to this database system will be owned by user \"postgres\". This user must also own the server process. The database cluster will be initialized with locale \"en_US.UTF-8\". The default database encoding has accordingly been set to \"UTF8\". The default text search configuration will be set to \"english\". Data page checksums are disabled. fixing permissions on existing directory /var/lib/pgdata ... ok creating subdirectories ... ok selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting dynamic shared memory implementation ... posix creating configuration files ... ok running bootstrap script ... ok performing post-bootstrap initialization ... ok syncing data to disk ... ok WARNING: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /var/lib/pgdata/ -l logfile start Setup postgresql.conf Next, edit postgresql.conf which is a configuration file of PostgreSQL. The parameters below should be edited at least to work PG-Strom. Investigate other parameters according to usage of the system and expected workloads. shared_preload_libraries PG-Strom module must be loaded on startup of the postmaster process by the shared_preload_libraries . Unable to load it on demand. Therefore, you must add the configuration below. shared_preload_libraries = '$libdir/pg_strom' max_worker_processes PG-Strom internally uses several background workers, so the default configuration (= 8) is too small for other usage. So, we recommand to expand the variable for a certain margin. max_worker_processes = 100 shared_buffers Although it depends on the workloads, the initial configuration of shared_buffers is too small for the data size where PG-Strom tries to work, thus storage workloads restricts the entire performance, and may be unable to work GPU efficiently. So, we recommend to expand the variable for a certain margin. shared_buffers = 10GB Please consider to apply SSD-to-GPU Direct SQL Execution to process larger than system's physical RAM size. work_mem Although it depends on the workloads, the initial configuration of work_mem is too small to choose the optimal query execution plan on analytic queries. An typical example is, disk-based merge sort may be chosen instead of the in-memory quick-sorting. So, we recommend to expand the variable for a certain margin. work_mem = 1GB Start PostgreSQL Start PostgreSQL service. If PG-Strom is set up appropriately, it writes out log message which shows PG-Strom recognized GPU devices. The example below recognized the Tesla V100(PCIe; 16GB edition) device. # systemctl start postgresql-10 # systemctl status -l postgresql-10 * postgresql-10.service - PostgreSQL 10 database server Loaded: loaded (/usr/lib/systemd/system/postgresql-10.service; disabled; vendor preset: disabled) Active: active (running) since Sat 2018-03-03 15:45:23 JST; 2min 21s ago Docs: https://www.postgresql.org/docs/10/static/ Process: 24851 ExecStartPre=/usr/pgsql-10/bin/postgresql-10-check-db-dir ${PGDATA} (code=exited, status=0/SUCCESS) Main PID: 24858 (postmaster) CGroup: /system.slice/postgresql-10.service |-24858 /usr/pgsql-10/bin/postmaster -D /var/lib/pgsql/10/data/ |-24890 postgres: logger process |-24892 postgres: bgworker: PG-Strom GPU memory keeper |-24896 postgres: checkpointer process |-24897 postgres: writer process |-24898 postgres: wal writer process |-24899 postgres: autovacuum launcher process |-24900 postgres: stats collector process |-24901 postgres: bgworker: PG-Strom ccache-builder2 |-24902 postgres: bgworker: PG-Strom ccache-builder1 `-24903 postgres: bgworker: logical replication launcher Mar 03 15:45:19 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:19.195 JST [24858] HINT: Run 'nvidia-cuda-mps-control -d', then start server process. Check 'man nvidia-cuda-mps-control' for more details. Mar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.509 JST [24858] LOG: PG-Strom: GPU0 Tesla V100-PCIE-16GB (5120 CUDA cores; 1380MHz, L2 6144kB), RAM 15.78GB (4096bits, 856MHz), CC 7.0 Mar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.510 JST [24858] LOG: NVRTC - CUDA Runtime Compilation vertion 9.1 Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG: listening on IPv6 address \"::1\", port 5432 Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG: listening on IPv4 address \"127.0.0.1\", port 5432 Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.442 JST [24858] LOG: listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\" Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.492 JST [24858] LOG: listening on Unix socket \"/tmp/.s.PGSQL.5432\" Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] LOG: redirecting log output to logging collector process Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] HINT: Future log output will appear in directory \"log\". Mar 03 15:45:23 saba.heterodb.com systemd[1]: Started PostgreSQL 10 database server. Creation of PG-Strom related objects At the last, create database objects related to PG-Strom, like SQL functions. This steps are packaged using EXTENSION feature of PostgreSQL. So, all you needs to run is CREATE EXTENSION on the SQL command line. Please note that this step is needed for each new database. If you want PG-Strom is pre-configured on new database creation, you can create PG-Strom extension on the template1 database, its configuration will be copied to the new database on CREATE DATABASE command. $ psql postgres -U postgres psql (10.2) Type \"help\" for help. postgres=# CREATE EXTENSION pg_strom ; CREATE EXTENSION That's all for the installation. NVME-Strom module This section also introduces NVME-Strom Linux kernel module which is closely cooperating with core features of PG-Strom like SSD-to-GPU Direct SQL Execution, even if it is an independent software module. Getting the module and installation Like other PG-Strom related modules, NVME-Strom is distributed at the (https://heterodb.github.io/swdc/)[HeteroDB Software Distribution Center] as a free software. In other words, it is not an open source software. If your system already setup heterodb-swdc package, yum install command downloads the RPM file and install the nvme_strom package. $ sudo yum install nvme_strom Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.cat.net * epel: ftp.iij.ad.jp * extras: mirrors.cat.net * ius: mirrors.kernel.org * updates: mirrors.cat.net Resolving Dependencies --> Running transaction check ---> Package nvme_strom.x86_64 0:1.3-1.el7 will be installed --> Finished Dependency Resolution Dependencies Resolved ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: nvme_strom x86_64 1.3-1.el7 heterodb 273 k Transaction Summary ================================================================================ Install 1 Package Total download size: 273 k Installed size: 1.5 M Is this ok [y/d/N]: y Downloading packages: No Presto metadata available for heterodb nvme_strom-1.3-1.el7.x86_64.rpm | 273 kB 00:00 Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : nvme_strom-1.3-1.el7.x86_64 1/1 : <snip> : DKMS: install completed. Verifying : nvme_strom-1.3-1.el7.x86_64 1/1 Installed: nvme_strom.x86_64 0:1.3-1.el7 Complete! License activation License activation is needed to use all the features of NVME-Strom module, provided by HeteroDB,Inc. You can operate the system without license, but features below are restricted. - Multiple GPUs support - Striping support (md-raid0) at SSD-to-GPU Direct SQL You can obtain a license file, like as a plain text below, from HeteroDB,Inc. IAgIVdKxhe+BSer3Y67jQW0+uTzYh00K6WOSH7xQ26Qcw8aeUNYqJB9YcKJTJb+QQhjmUeQpUnboNxVwLCd3HFuLXeBWMKp11/BgG0FSrkUWu/ZCtDtw0F1hEIUY7m767zAGV8y+i7BuNXGJFvRlAkxdVO3/K47ocIgoVkuzBfLvN/h9LffOydUnHPzrFHfLc0r3nNNgtyTrfvoZiXegkGM9GBTAKyq8uWu/OGonh9ybzVKOgofhDLk0rVbLohOXDhMlwDl2oMGIr83tIpCWG+BGE+TDwsJ4n71Sv6n4bi/ZBXBS498qShNHDGrbz6cNcDVBa+EuZc6HzZoF6UrljEcl= ---- VERSION:2 SERIAL_NR:HDB-TRIAL ISSUED_AT:2019-05-09 EXPIRED_AT:2019-06-08 GPU_UUID:GPU-a137b1df-53c9-197f-2801-f2dccaf9d42f Copy the license file to /etc/heterodb.license , then restart PostgreSQL. The startup log messages of PostgreSQL dumps the license information, and it tells us the license activation is successfully done. $ pg_ctl restart : LOG: PG-Strom version 2.2 built for PostgreSQL 11 LOG: PG-Strom: GPU0 Tesla P40 (3840 CUDA cores; 1531MHz, L2 3072kB), RAM 22.38GB (384bits, 3.45GHz), CC 6.1 : LOG: HeteroDB License: { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"9-May-2019\", \"expired_at\" : \"8-Jun-2019\", \"gpus\" : [ { \"uuid\" : \"GPU-a137b1df-53c9-197f-2801-f2dccaf9d42f\", \"pci_id\" : \"0000:02:00.0\" } ] } LOG: listening on IPv6 address \"::1\", port 5432 LOG: listening on IPv4 address \"127.0.0.1\", port 5432 : Kernel module parameters NVME-Strom Linux kernel module has some parameters. Parameter Type Default Description verbose int 0 Enables detailed debug output stat_info int 1 Enables performance statistics fast_ssd_mode int 0 Operating mode for fast NVME-SSD p2p_dma_max_depth int 48 Maximum number of asynchronous P2P DMA request can be enqueued on the I/O-queue of NVME device p2p_dma_max_unitsz int 256 Maximum length of data blocks, in kB, to be read by a single P2P DMA request at once Here is an extra explanation for fast_ssd_mode parameter. When NVME-Strom Linux kernel module get a request for SSD-to-GPU direct data transfer, first of all, it checks whether the required data blocks are caches on page-caches of operating system. If fast_ssd_mode is 0 , NVME-Strom once writes back page caches of the required data blocks to the userspace buffer of the caller, then indicates application to invoke normal host-->device data transfer by CUDA API. It is suitable for non-fast NVME-SSDs such as PCIe x4 grade. On the other hands, SSD-to-GPU direct data transfer may be faster, if you use PCIe x8 grade fast NVME-SSD or use multiple SSDs in striping mode, than normal host-->device data transfer after the buffer copy. If fast_ssd_mode is not 0 , NVME-Strom kicks SSD-to-GPU direct data transfer regardless of the page cache state. However, it shall never kicks SSD-to-GPU direct data transfer if page cache is dirty. Here is an extra explanation for p2p_dma_max_depth parameter. NVME-Strom Linux kernel module makes DMA requests for SSD-to-GPU direct data transfer, then enqueues them to I/O-queue of the source NVME devices. When asynchronous DMA requests are enqueued more than the capacity of NVME devices, latency of individual DMA requests become terrible because NVME-SSD controler processes the DMA requests in order of arrival. (On the other hands, it maximizes the throughput because NVME-SSD controler receives DMA requests continuously.) If turn-around time of the DMA requests are too large, it may be wrongly considered as errors, then can lead timeout of I/O request and return an error status. Thus, it makes no sense to enqueue more DMA requests to the I/O-queue more than the reasonable amount of pending requests for full usage of NVME devices. p2p_dma_max_depth parameter controls number of asynchronous P2P DMA requests that can be enqueued at once per NVME device. If application tries to enqueue DMA requests more than the configuration, the caller thread will block until completion of the running DMA. So, it enables to avoid unintentional high-load of NVME devices.","title":"Install"},{"location":"install/#checklist","text":"Server Hardware It requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices. note002:HW Validation List may help you to choose the hardware. SSD-to-GPU Direct SQL Execution needs SSD devices which support NVMe specification, and to be installed under the same PCIe Root Complex where GPU is located on. GPU Device PG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later; note001:GPU Availability Matrix shows more detailed information. Check this list for the support status of SSD-to-GPU Direct SQL Execution. Operating System PG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or CentOS version 7.x series. - SSD-to-GPU Direct SQL Execution needs Red Hat Enterprise Linux or CentOS version 7.3 or later. PostgreSQL PG-Strom requires PostgreSQL version 9.6 or later. PostgreSQL v9.6 renew the custom-scan interface for CPU-parallel execution or GROUP BY planning, thus, it allows cooperation of custom-plans provides by extension modules. CUDA Toolkit PG-Strom requires CUDA Toolkit version 9.2 or later. Some of CUDA Driver APIs used by PG-Strom internally are not included in the former versions.","title":"Checklist"},{"location":"install/#os-installation","text":"Choose a Linux distribution which is supported by CUDA Toolkit, then install the system according to the installation process of the distribution. NVIDIA DEVELOPER ZONE introduces the list of Linux distributions which are supported by CUDA Toolkit. In case of Red Hat Enterprise Linux 7.x or CentOS 7.x series, choose \"Minimal installation\" as base environment, and also check the following add-ons. Debugging Tools Development Tools","title":"OS Installation"},{"location":"install/#post-os-installation-configuration","text":"Next to the OS installation, a few additionsl configurations are required to install GPU-drivers and NVMe-Strom driver on the later steps.","title":"Post OS Installation Configuration"},{"location":"install/#setup-epel-repository","text":"Several software modules required by PG-Strom are distributed as a part of EPEL (Extra Packages for Enterprise Linux). You need to add a repository definition of EPEL packages for yum system to obtain these software. One of the package we will get from EPEL repository is DKMS (Dynamic Kernel Module Support). It is a framework to build Linux kernel module for the running Linux kernel on demand; used for NVIDIA's GPU driver or NVMe-Strom which is a kernel module to support SSD-to-GPU Direct SQL Execution. epel-release package provides the repository definition of EPEL. You can obtain this package from the public FTP site of Fedora Project. Downloads the epel-release-<distribution version>.noarch.rpm , and install the package. Once epel-release package gets installed, yum system configuration is updated to get software from the EPEL repository. Fedora Project Public FTP Site https://dl.fedoraproject.org/pub/epel/7/x86_64/ Tip Walk down the directory: Packages --> e , from the above URL. Install the epel-release package as follows. $ sudo yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: epel-release noarch 7-11 /epel-release-7-11.noarch 24 k Transaction Summary ================================================================================ Install 1 Package : Installed: epel-release.noarch 0:7-11 Complete!","title":"Setup EPEL Repository"},{"location":"install/#heterodb-swdc-installation","text":"PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . You need to add a repository definition of HeteroDB-SWDC for you system to obtain these software. heterodb-swdc package provides the repository definition of HeteroDB-SWDC. Access to the HeteroDB Software Distribution Center using Web browser, download the heterodb-swdc-1.0-1.el7.noarch.rpm on top of the file list, then install this package. Once heterodb-swdc package gets installed, yum system configuration is updated to get software from the HeteroDB-SWDC repository. Install the heterodb-swdc package as follows. $ sudo yum install https://heterodb.github.io/swdc/yum/rhel7-x86_64/heterodb-swdc-1.0-1.el7.noarch.rpm : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: heterodb-swdc noarch 1.0-1.el7 /heterodb-swdc-1.0-1.el7.noarch 2.4 k Transaction Summary ================================================================================ Install 1 Package : Installed: heterodb-swdc.noarch 0:1.0-1.el7 Complete!","title":"HeteroDB-SWDC Installation"},{"location":"install/#cuda-toolkit-installation","text":"This section introduces the installation of CUDA Toolkit. If you already installed the latest CUDA Toolkit, you can skip this section. NVIDIA offers two approach to install CUDA Toolkit; one is by self-extracting archive (called runfile), and the other is by RPM packages. We recommend RPM installation because it allows simple software updates. You can download the installation package for CUDA Toolkit from NVIDIA DEVELOPER ZONE. Choose your OS, architecture, distribution and version, then choose \"rpm(network)\" edition. The \"rpm(network)\" edition contains only yum repositoty definition to distribute CUDA Toolkit. It is similar to the EPEL repository definition at the OS installation. So, you needs to installa the related RPM packages over network after the resistoration of CUDA repository. Run the following command. $ sudo rpm -i cuda-repo-<distribution>-<version>.x86_64.rpm $ sudo yum clean all $ sudo yum install cuda --enablerepo=rhel-7-server-e4s-optional-rpms or $ sudo yum install cuda Once installation completed successfully, CUDA Toolkit is deployed at /usr/local/cuda . Tip RHEL7 does not enable rhel-7-server-e4s-optional-rpms repository in the default. It distributes vulkan-filesystem packaged required by CUDA Toolkit installation. When you kick installation of CUDA Toolkit, edit /etc/yum.repos.d/redhat.repo to enable the repository, or use --enablerepo option of yum command to resolve dependency. $ ls /usr/local/cuda bin include libnsight nvml samples tools doc jre libnvvp nvvm share version.txt extras lib64 nsightee_plugins pkgconfig src Once installation gets completed, ensure the system recognizes the GPU devices correctly. nvidia-smi command shows GPU information installed on your system, as follows. $ nvidia-smi Wed Feb 14 09:43:48 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 387.26 Driver Version: 387.26 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla V100-PCIE... Off | 00000000:02:00.0 Off | 0 | | N/A 41C P0 37W / 250W | 0MiB / 16152MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Tip If nouveau driver which conflicts to nvidia driver is loaded, system cannot load the nvidia driver immediately. In this case, reboot the operating system after a configuration to disable the nouveau driver. If CUDA Toolkit is installed by the runfile installer, it also disables the nouveau driver. Elsewhere, in case of RPM installation, do the following configuration. To disable the nouveau driver, put the following configuration onto /etc/modprobe.d/disable-nouveau.conf , then run dracut command to apply them on the boot image of Linux kernel. # cat > /etc/modprobe.d/disable-nouveau.conf <<EOF blacklist nouveau options nouveau modeset=0 EOF # dracut -f","title":"CUDA Toolkit Installation"},{"location":"install/#postgresql-installation","text":"This section introduces PostgreSQL installation with RPM. We don't introduce the installation steps from the source because there are many documents for this approach, and there are also various options for the ./configure script. PostgreSQL is also distributed in the packages of Linux distributions, however, it is not the latest one, and often older than the version which supports PG-Strom. For example, Red Hat Enterprise Linux 7.x or CentOS 7.x distributes PostgreSQL v9.2.x series. This version had been EOL by the PostgreSQL community. PostgreSQL Global Development Group provides yum repository to distribute the latest PostgreSQL and related packages. Like the configuration of EPEL, you can install a small package to set up yum repository, then install PostgreSQL and related software. Here is the list of yum repository definition: http://yum.postgresql.org/repopackages.php . Repository definitions are per PostgreSQL major version and Linux distribution. You need to choose the one for your Linux distribution, and for PostgreSQL v9.6 or later. All you need to install are yum repository definition, and PostgreSQL packages. If you choose PostgreSQL v10, the pakages below are required to install PG-Strom. postgresql10-devel postgresql10-server $ sudo yum install -y https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm $ sudo yum install -y postgresql10-server postgresql10-devel : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: postgresql10-devel x86_64 10.2-1PGDG.rhel7 pgdg10 2.0 M postgresql10-server x86_64 10.2-1PGDG.rhel7 pgdg10 4.4 M Installing for dependencies: postgresql10 x86_64 10.2-1PGDG.rhel7 pgdg10 1.5 M postgresql10-libs x86_64 10.2-1PGDG.rhel7 pgdg10 354 k Transaction Summary ================================================================================ Install 2 Packages (+2 Dependent packages) : Installed: postgresql10-devel.x86_64 0:10.2-1PGDG.rhel7 postgresql10-server.x86_64 0:10.2-1PGDG.rhel7 Dependency Installed: postgresql10.x86_64 0:10.2-1PGDG.rhel7 postgresql10-libs.x86_64 0:10.2-1PGDG.rhel7 Complete! The RPM packages provided by PostgreSQL Global Development Group installs software under the /usr/pgsql-<version> directory, so you may pay attention whether the PATH environment variable is configured appropriately. postgresql-alternative package set up symbolic links to the related commands under /usr/local/bin , so allows to simplify the operations. Also, it enables to switch target version using alternatives command even if multiple version of PostgreSQL. $ sudo yum install postgresql-alternatives : Resolving Dependencies --> Running transaction check ---> Package postgresql-alternatives.noarch 0:1.0-1.el7 will be installed --> Finished Dependency Resolution Dependencies Resolved : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: postgresql-alternatives noarch 1.0-1.el7 heterodb 9.2 k Transaction Summary ================================================================================ : Installed: postgresql-alternatives.noarch 0:1.0-1.el7 Complete!","title":"PostgreSQL Installation"},{"location":"install/#pg-strom-installation","text":"","title":"PG-Strom Installation"},{"location":"install/#rpm-installation","text":"PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . If you repository definition has been added, not many tasks are needed. We provide individual RPM packages of PG-Strom for each base PostgreSQL version. pg_strom-PG96 package is built for PostgreSQL 9.6, and pg_strom-PG10 is also built for PostgreSQL v10. $ sudo yum install pg_strom-PG10 : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: pg_strom-PG10 x86_64 1.9-180301.el7 heterodb 320 k Transaction Summary ================================================================================ : Installed: pg_strom-PG10.x86_64 0:1.9-180301.el7 Complete! That's all for package installation.","title":"RPM Installation"},{"location":"install/#installation-from-the-source","text":"For developers, we also introduces the steps to build and install PG-Strom from the source code.","title":"Installation from the source"},{"location":"install/#getting-the-source-code","text":"Like RPM packages, you can download tarball of the source code from HeteroDB Software Distribution Center . On the other hands, here is a certain time-lags to release the tarball, it may be preferable to checkout the master branch of PG-Strom on GitHub to use the latest development branch. $ git clone https://github.com/heterodb/pg-strom.git Cloning into 'pg-strom'... remote: Counting objects: 13797, done. remote: Compressing objects: 100% (215/215), done. remote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400 Receiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done. Resolving deltas: 100% (10504/10504), done.","title":"Getting the source code"},{"location":"install/#building-the-pg-strom","text":"Configuration to build PG-Strom must match to the target PostgreSQL strictly. For example, if a particular strcut has inconsistent layout by the configuration at build, it may lead problematic bugs; not easy to find out. Thus, not to have inconsistency, PG-Strom does not have own configure script, but references the build configuration of PostgreSQL using pg_config command. If PATH environment variable is set to the pg_config command of the target PostgreSQL, run make and make install . Elsewhere, give PG_CONFIG=... parameter on make command to tell the full path of the pg_config command. $ cd pg-strom $ make PG_CONFIG=/usr/pgsql-10/bin/pg_config $ sudo make install PG_CONFIG=/usr/pgsql-10/bin/pg_config","title":"Building the PG-Strom"},{"location":"install/#post-installation-setup","text":"","title":"Post Installation Setup"},{"location":"install/#creation-of-database-cluster","text":"Database cluster is not constructed yet, run initdb command to set up initial database of PostgreSQL. The default path of the database cluster on RPM installation is /var/lib/pgsql/<version number>/data . If you install postgresql-alternatives package, this default path can be referenced by /var/lib/pgdata regardless of the PostgreSQL version. $ sudo su - postgres $ initdb -D /var/lib/pgdata/ The files belonging to this database system will be owned by user \"postgres\". This user must also own the server process. The database cluster will be initialized with locale \"en_US.UTF-8\". The default database encoding has accordingly been set to \"UTF8\". The default text search configuration will be set to \"english\". Data page checksums are disabled. fixing permissions on existing directory /var/lib/pgdata ... ok creating subdirectories ... ok selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting dynamic shared memory implementation ... posix creating configuration files ... ok running bootstrap script ... ok performing post-bootstrap initialization ... ok syncing data to disk ... ok WARNING: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /var/lib/pgdata/ -l logfile start","title":"Creation of database cluster"},{"location":"install/#setup-postgresqlconf","text":"Next, edit postgresql.conf which is a configuration file of PostgreSQL. The parameters below should be edited at least to work PG-Strom. Investigate other parameters according to usage of the system and expected workloads. shared_preload_libraries PG-Strom module must be loaded on startup of the postmaster process by the shared_preload_libraries . Unable to load it on demand. Therefore, you must add the configuration below. shared_preload_libraries = '$libdir/pg_strom' max_worker_processes PG-Strom internally uses several background workers, so the default configuration (= 8) is too small for other usage. So, we recommand to expand the variable for a certain margin. max_worker_processes = 100 shared_buffers Although it depends on the workloads, the initial configuration of shared_buffers is too small for the data size where PG-Strom tries to work, thus storage workloads restricts the entire performance, and may be unable to work GPU efficiently. So, we recommend to expand the variable for a certain margin. shared_buffers = 10GB Please consider to apply SSD-to-GPU Direct SQL Execution to process larger than system's physical RAM size. work_mem Although it depends on the workloads, the initial configuration of work_mem is too small to choose the optimal query execution plan on analytic queries. An typical example is, disk-based merge sort may be chosen instead of the in-memory quick-sorting. So, we recommend to expand the variable for a certain margin. work_mem = 1GB","title":"Setup postgresql.conf"},{"location":"install/#start-postgresql","text":"Start PostgreSQL service. If PG-Strom is set up appropriately, it writes out log message which shows PG-Strom recognized GPU devices. The example below recognized the Tesla V100(PCIe; 16GB edition) device. # systemctl start postgresql-10 # systemctl status -l postgresql-10 * postgresql-10.service - PostgreSQL 10 database server Loaded: loaded (/usr/lib/systemd/system/postgresql-10.service; disabled; vendor preset: disabled) Active: active (running) since Sat 2018-03-03 15:45:23 JST; 2min 21s ago Docs: https://www.postgresql.org/docs/10/static/ Process: 24851 ExecStartPre=/usr/pgsql-10/bin/postgresql-10-check-db-dir ${PGDATA} (code=exited, status=0/SUCCESS) Main PID: 24858 (postmaster) CGroup: /system.slice/postgresql-10.service |-24858 /usr/pgsql-10/bin/postmaster -D /var/lib/pgsql/10/data/ |-24890 postgres: logger process |-24892 postgres: bgworker: PG-Strom GPU memory keeper |-24896 postgres: checkpointer process |-24897 postgres: writer process |-24898 postgres: wal writer process |-24899 postgres: autovacuum launcher process |-24900 postgres: stats collector process |-24901 postgres: bgworker: PG-Strom ccache-builder2 |-24902 postgres: bgworker: PG-Strom ccache-builder1 `-24903 postgres: bgworker: logical replication launcher Mar 03 15:45:19 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:19.195 JST [24858] HINT: Run 'nvidia-cuda-mps-control -d', then start server process. Check 'man nvidia-cuda-mps-control' for more details. Mar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.509 JST [24858] LOG: PG-Strom: GPU0 Tesla V100-PCIE-16GB (5120 CUDA cores; 1380MHz, L2 6144kB), RAM 15.78GB (4096bits, 856MHz), CC 7.0 Mar 03 15:45:20 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:20.510 JST [24858] LOG: NVRTC - CUDA Runtime Compilation vertion 9.1 Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG: listening on IPv6 address \"::1\", port 5432 Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.378 JST [24858] LOG: listening on IPv4 address \"127.0.0.1\", port 5432 Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.442 JST [24858] LOG: listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\" Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.492 JST [24858] LOG: listening on Unix socket \"/tmp/.s.PGSQL.5432\" Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] LOG: redirecting log output to logging collector process Mar 03 15:45:23 saba.heterodb.com postmaster[24858]: 2018-03-03 15:45:23.527 JST [24858] HINT: Future log output will appear in directory \"log\". Mar 03 15:45:23 saba.heterodb.com systemd[1]: Started PostgreSQL 10 database server.","title":"Start PostgreSQL"},{"location":"install/#creation-of-pg-strom-related-objects","text":"At the last, create database objects related to PG-Strom, like SQL functions. This steps are packaged using EXTENSION feature of PostgreSQL. So, all you needs to run is CREATE EXTENSION on the SQL command line. Please note that this step is needed for each new database. If you want PG-Strom is pre-configured on new database creation, you can create PG-Strom extension on the template1 database, its configuration will be copied to the new database on CREATE DATABASE command. $ psql postgres -U postgres psql (10.2) Type \"help\" for help. postgres=# CREATE EXTENSION pg_strom ; CREATE EXTENSION That's all for the installation.","title":"Creation of PG-Strom related objects"},{"location":"install/#nvme-strom-module","text":"This section also introduces NVME-Strom Linux kernel module which is closely cooperating with core features of PG-Strom like SSD-to-GPU Direct SQL Execution, even if it is an independent software module.","title":"NVME-Strom module"},{"location":"install/#getting-the-module-and-installation","text":"Like other PG-Strom related modules, NVME-Strom is distributed at the (https://heterodb.github.io/swdc/)[HeteroDB Software Distribution Center] as a free software. In other words, it is not an open source software. If your system already setup heterodb-swdc package, yum install command downloads the RPM file and install the nvme_strom package. $ sudo yum install nvme_strom Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.cat.net * epel: ftp.iij.ad.jp * extras: mirrors.cat.net * ius: mirrors.kernel.org * updates: mirrors.cat.net Resolving Dependencies --> Running transaction check ---> Package nvme_strom.x86_64 0:1.3-1.el7 will be installed --> Finished Dependency Resolution Dependencies Resolved ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: nvme_strom x86_64 1.3-1.el7 heterodb 273 k Transaction Summary ================================================================================ Install 1 Package Total download size: 273 k Installed size: 1.5 M Is this ok [y/d/N]: y Downloading packages: No Presto metadata available for heterodb nvme_strom-1.3-1.el7.x86_64.rpm | 273 kB 00:00 Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : nvme_strom-1.3-1.el7.x86_64 1/1 : <snip> : DKMS: install completed. Verifying : nvme_strom-1.3-1.el7.x86_64 1/1 Installed: nvme_strom.x86_64 0:1.3-1.el7 Complete!","title":"Getting the module and installation"},{"location":"install/#license-activation","text":"License activation is needed to use all the features of NVME-Strom module, provided by HeteroDB,Inc. You can operate the system without license, but features below are restricted. - Multiple GPUs support - Striping support (md-raid0) at SSD-to-GPU Direct SQL You can obtain a license file, like as a plain text below, from HeteroDB,Inc. IAgIVdKxhe+BSer3Y67jQW0+uTzYh00K6WOSH7xQ26Qcw8aeUNYqJB9YcKJTJb+QQhjmUeQpUnboNxVwLCd3HFuLXeBWMKp11/BgG0FSrkUWu/ZCtDtw0F1hEIUY7m767zAGV8y+i7BuNXGJFvRlAkxdVO3/K47ocIgoVkuzBfLvN/h9LffOydUnHPzrFHfLc0r3nNNgtyTrfvoZiXegkGM9GBTAKyq8uWu/OGonh9ybzVKOgofhDLk0rVbLohOXDhMlwDl2oMGIr83tIpCWG+BGE+TDwsJ4n71Sv6n4bi/ZBXBS498qShNHDGrbz6cNcDVBa+EuZc6HzZoF6UrljEcl= ---- VERSION:2 SERIAL_NR:HDB-TRIAL ISSUED_AT:2019-05-09 EXPIRED_AT:2019-06-08 GPU_UUID:GPU-a137b1df-53c9-197f-2801-f2dccaf9d42f Copy the license file to /etc/heterodb.license , then restart PostgreSQL. The startup log messages of PostgreSQL dumps the license information, and it tells us the license activation is successfully done. $ pg_ctl restart : LOG: PG-Strom version 2.2 built for PostgreSQL 11 LOG: PG-Strom: GPU0 Tesla P40 (3840 CUDA cores; 1531MHz, L2 3072kB), RAM 22.38GB (384bits, 3.45GHz), CC 6.1 : LOG: HeteroDB License: { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"9-May-2019\", \"expired_at\" : \"8-Jun-2019\", \"gpus\" : [ { \"uuid\" : \"GPU-a137b1df-53c9-197f-2801-f2dccaf9d42f\", \"pci_id\" : \"0000:02:00.0\" } ] } LOG: listening on IPv6 address \"::1\", port 5432 LOG: listening on IPv4 address \"127.0.0.1\", port 5432 :","title":"License activation"},{"location":"install/#kernel-module-parameters","text":"NVME-Strom Linux kernel module has some parameters. Parameter Type Default Description verbose int 0 Enables detailed debug output stat_info int 1 Enables performance statistics fast_ssd_mode int 0 Operating mode for fast NVME-SSD p2p_dma_max_depth int 48 Maximum number of asynchronous P2P DMA request can be enqueued on the I/O-queue of NVME device p2p_dma_max_unitsz int 256 Maximum length of data blocks, in kB, to be read by a single P2P DMA request at once Here is an extra explanation for fast_ssd_mode parameter. When NVME-Strom Linux kernel module get a request for SSD-to-GPU direct data transfer, first of all, it checks whether the required data blocks are caches on page-caches of operating system. If fast_ssd_mode is 0 , NVME-Strom once writes back page caches of the required data blocks to the userspace buffer of the caller, then indicates application to invoke normal host-->device data transfer by CUDA API. It is suitable for non-fast NVME-SSDs such as PCIe x4 grade. On the other hands, SSD-to-GPU direct data transfer may be faster, if you use PCIe x8 grade fast NVME-SSD or use multiple SSDs in striping mode, than normal host-->device data transfer after the buffer copy. If fast_ssd_mode is not 0 , NVME-Strom kicks SSD-to-GPU direct data transfer regardless of the page cache state. However, it shall never kicks SSD-to-GPU direct data transfer if page cache is dirty. Here is an extra explanation for p2p_dma_max_depth parameter. NVME-Strom Linux kernel module makes DMA requests for SSD-to-GPU direct data transfer, then enqueues them to I/O-queue of the source NVME devices. When asynchronous DMA requests are enqueued more than the capacity of NVME devices, latency of individual DMA requests become terrible because NVME-SSD controler processes the DMA requests in order of arrival. (On the other hands, it maximizes the throughput because NVME-SSD controler receives DMA requests continuously.) If turn-around time of the DMA requests are too large, it may be wrongly considered as errors, then can lead timeout of I/O request and return an error status. Thus, it makes no sense to enqueue more DMA requests to the I/O-queue more than the reasonable amount of pending requests for full usage of NVME devices. p2p_dma_max_depth parameter controls number of asynchronous P2P DMA requests that can be enqueued at once per NVME device. If application tries to enqueue DMA requests more than the configuration, the caller thread will block until completion of the running DMA. So, it enables to avoid unintentional high-load of NVME devices.","title":"Kernel module parameters"},{"location":"operations/","text":"Basic operations Confirmation of GPU off-loading You can use EXPLAIN command to check whether query is executed on GPU device or not. A query is internally split into multiple elements and executed, and PG-Strom is capable to run SCAN, JOIN and GROUP BY in parallel on GPU device. If you can find out GpuScan, GpuJoin or GpuPreAgg was displayed instead of the standard operations by PostgreSQL, it means the query is partially executed on GPU device. Below is an example of EXPLAIN command output. postgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 NATURAL JOIN t2 GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=989186.82..989190.94 rows=27 width=20) Group Key: t0.cat -> Sort (cost=989186.82..989187.29 rows=189 width=44) Sort Key: t0.cat -> Custom Scan (GpuPreAgg) (cost=989175.89..989179.67 rows=189 width=44) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) Combined GpuJoin: enabled -> Custom Scan (GpuJoin) on t0 (cost=14744.40..875804.46 rows=99996736 width=12) GPU Projection: t0.cat, t1.ax Outer Scan: t0 (cost=0.00..1833360.36 rows=99996736 width=12) Depth 1: GpuHashJoin (nrows 99996736...99996736) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size: 10.39MB) Depth 2: GpuHashJoin (nrows 99996736...99996736) HashKeys: t0.bid JoinQuals: (t0.bid = t2.bid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) -> Seq Scan on t2 (cost=0.00..1935.00 rows=100000 width=4) (21 rows) You can notice some unusual query execution plans. GpuJoin and GpuPreAgg are implemented on the CustomScan mechanism. In this example, GpuJoin runs JOIN operation on t0 , t1 and t1 , then GpuPreAgg which receives the result of GpuJoin runs GROUP BY operation by the cat column on GPU device. PG-Strom interacts with the query optimizer during PostgreSQL is building a query execution plan, and it offers alternative query execution plan with estimated cost for PostgreSQL's optimizer, if any of SCAN, JOIN, or GROUP BY are executable on GPU device. This estimated cost is better than other query execution plans that run on CPU, it chooses the alternative execution plan that shall run on GPU device. For GPU execution, it requires operators, functions and data types in use must be supported by PG-Strom. It supports numeric types like int or float , date and time types like date or timestamp , variable length string like text and so on. It also supports arithmetic operations, comparison operators and many built-in operators. See References for the detailed list. CPU+GPU Hybrid Parallel PG-Strom also supports PostgreSQL's CPU parallel execution. In the CPU parallel execution mode, Gather node launches several background worker processes, then it gathers the result of \"partial\" execution by individual background workers. CustomScan execution plan provided by PG-Strom, like GpuJoin or GpuPreAgg, support execution at the background workers. They process their partial task using GPU individually. A CPU core usually needs much more time to set up buffer to supply data for GPU than execution of SQL workloads on GPU, so hybrid usage of CPU and GPU parallel can expect higher performance. On the other hands, each process creates CUDA context that is required to communicate GPU and consumes a certain amount of GPU resources, so higher parallelism on CPU-side is not always better. Look at the query execution plan below. Execution plan tree under the Gather is executable on background worker process. It scans t0 table which has 100million rows using four background worker processes and the coordinator process, in other words, 20million rows are handled per process by GpuJoin and GpuPreAgg, then its results are merged at Gather node. # EXPLAIN SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 GROUP by cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=955705.47..955720.93 rows=27 width=20) Group Key: t0.cat -> Sort (cost=955705.47..955707.36 rows=756 width=44) Sort Key: t0.cat -> Gather (cost=955589.95..955669.33 rows=756 width=44) Workers Planned: 4 -> Parallel Custom Scan (GpuPreAgg) (cost=954589.95..954593.73 rows=189 width=44) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on t0 (cost=27682.82..841218.52 rows=99996736 width=12) GPU Projection: t0.cat, t1.ax Outer Scan: t0 (cost=0.00..1083384.84 rows=24999184 width=8) Depth 1: GpuHashJoin (nrows 24999184...99996736) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size: 10.39MB) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) (18 rows) Pullup underlying plans PG-Strom can run SCAN, JOIN and GROUP BY workloads on GPU, however, it does not work with best performance if these custom execution plan simply replace the standard operations at PostgreSQL. An example of problematic scenario is that SCAN once writes back its result data set to the host buffer then send the same data into GPU again to execute JOIN. Once again, JOIN results are written back and send to GPU to execute GROUP BY. It causes data ping-pong between CPU and GPU. To avoid such inefficient jobs, PG-Strom has a special mode which pulls up its sub-plan to execute a bunch of jobs in a single GPU kernel invocation. Combination of the operations blow can cause pull-up of sub-plans. SCAN + JOIN SCAN + GROUP BY SCAN + JOIN + GROUP BY The execution plan example below never pulls up the sub-plans. GpuJoin receives the result of GpuScan, then its results are passed to GpuPreAgg to generate the final results. # EXPLAIN SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 WHERE aid < bid GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=1239991.03..1239995.15 rows=27 width=20) Group Key: t0.cat -> Sort (cost=1239991.03..1239991.50 rows=189 width=44) Sort Key: t0.cat -> Custom Scan (GpuPreAgg) (cost=1239980.10..1239983.88 rows=189 width=44) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) -> Custom Scan (GpuJoin) (cost=50776.43..1199522.96 rows=33332245 width=12) GPU Projection: t0.cat, t1.ax Depth 1: GpuHashJoin (nrows 33332245...33332245) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size: 10.39MB) -> Custom Scan (GpuScan) on t0 (cost=12634.49..1187710.85 rows=33332245 width=8) GPU Projection: cat, aid GPU Filter: (aid < bid) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) (18 rows) This example causes data ping-pong between GPU and host buffers for each execution stage, so not efficient and less performance. On the other hands, the query execution plan below pulls up sub-plans. # EXPLAIN ANALYZE SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 WHERE aid < bid GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=903669.50..903673.62 rows=27 width=20) (actual time=7761.630..7761.644 rows=27 loops=1) Group Key: t0.cat -> Sort (cost=903669.50..903669.97 rows=189 width=44) (actual time=7761.621..7761.626 rows=27 loops=1) Sort Key: t0.cat Sort Method: quicksort Memory: 28kB -> Custom Scan (GpuPreAgg) (cost=903658.57..903662.35 rows=189 width=44) (actual time=7761.531..7761.540 rows=27 loops=1) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) Combined GpuJoin: enabled -> Custom Scan (GpuJoin) on t0 (cost=12483.41..863201.43 rows=33332245 width=12) (never executed) GPU Projection: t0.cat, t1.ax Outer Scan: t0 (cost=12634.49..1187710.85 rows=33332245 width=8) (actual time=59.623..5557.052 rows=100000000 loops=1) Outer Scan Filter: (aid < bid) Rows Removed by Outer Scan Filter: 50002874 Depth 1: GpuHashJoin (plan nrows: 33332245...33332245, actual nrows: 49997126...49997126) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size plan: 10.39MB, exec: 64.00MB) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) (actual time=0.013..15.303 rows=100000 loops=1) Planning time: 0.506 ms Execution time: 8495.391 ms (21 rows) You may notice that SCAN on the table t0 is embedded into GpuJoin, and GpuScan gets vanished. It means GpuJoin pulls up the underlying GpuScan, then combined GPU kernel function is also responsible for evaluation of the supplied WHERE-clause. In addition, here is a strange output in EXPLAIN ANALYZE result - it displays (never executed) for GpuJoin. It means GpuJoin is never executed during the query execution, and it is right. GpuPreAgg pulls up the underlying GpuJoin, then its combined GPU kernel function runs JOIN and GROUP BY. The pg_strom.pullup_outer_scan parameter controls whether SCAN is pulled up, and the pg_strom.pullup_outer_join parameter also controls whether JOIN is pulled up. Both parameters are configured to on . Usually, no need to disable them, however, you can use the parameters to identify the problems on system troubles.","title":"Basic Operations"},{"location":"operations/#confirmation-of-gpu-off-loading","text":"You can use EXPLAIN command to check whether query is executed on GPU device or not. A query is internally split into multiple elements and executed, and PG-Strom is capable to run SCAN, JOIN and GROUP BY in parallel on GPU device. If you can find out GpuScan, GpuJoin or GpuPreAgg was displayed instead of the standard operations by PostgreSQL, it means the query is partially executed on GPU device. Below is an example of EXPLAIN command output. postgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 NATURAL JOIN t2 GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=989186.82..989190.94 rows=27 width=20) Group Key: t0.cat -> Sort (cost=989186.82..989187.29 rows=189 width=44) Sort Key: t0.cat -> Custom Scan (GpuPreAgg) (cost=989175.89..989179.67 rows=189 width=44) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) Combined GpuJoin: enabled -> Custom Scan (GpuJoin) on t0 (cost=14744.40..875804.46 rows=99996736 width=12) GPU Projection: t0.cat, t1.ax Outer Scan: t0 (cost=0.00..1833360.36 rows=99996736 width=12) Depth 1: GpuHashJoin (nrows 99996736...99996736) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size: 10.39MB) Depth 2: GpuHashJoin (nrows 99996736...99996736) HashKeys: t0.bid JoinQuals: (t0.bid = t2.bid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) -> Seq Scan on t2 (cost=0.00..1935.00 rows=100000 width=4) (21 rows) You can notice some unusual query execution plans. GpuJoin and GpuPreAgg are implemented on the CustomScan mechanism. In this example, GpuJoin runs JOIN operation on t0 , t1 and t1 , then GpuPreAgg which receives the result of GpuJoin runs GROUP BY operation by the cat column on GPU device. PG-Strom interacts with the query optimizer during PostgreSQL is building a query execution plan, and it offers alternative query execution plan with estimated cost for PostgreSQL's optimizer, if any of SCAN, JOIN, or GROUP BY are executable on GPU device. This estimated cost is better than other query execution plans that run on CPU, it chooses the alternative execution plan that shall run on GPU device. For GPU execution, it requires operators, functions and data types in use must be supported by PG-Strom. It supports numeric types like int or float , date and time types like date or timestamp , variable length string like text and so on. It also supports arithmetic operations, comparison operators and many built-in operators. See References for the detailed list.","title":"Confirmation of GPU off-loading"},{"location":"operations/#cpugpu-hybrid-parallel","text":"PG-Strom also supports PostgreSQL's CPU parallel execution. In the CPU parallel execution mode, Gather node launches several background worker processes, then it gathers the result of \"partial\" execution by individual background workers. CustomScan execution plan provided by PG-Strom, like GpuJoin or GpuPreAgg, support execution at the background workers. They process their partial task using GPU individually. A CPU core usually needs much more time to set up buffer to supply data for GPU than execution of SQL workloads on GPU, so hybrid usage of CPU and GPU parallel can expect higher performance. On the other hands, each process creates CUDA context that is required to communicate GPU and consumes a certain amount of GPU resources, so higher parallelism on CPU-side is not always better. Look at the query execution plan below. Execution plan tree under the Gather is executable on background worker process. It scans t0 table which has 100million rows using four background worker processes and the coordinator process, in other words, 20million rows are handled per process by GpuJoin and GpuPreAgg, then its results are merged at Gather node. # EXPLAIN SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 GROUP by cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=955705.47..955720.93 rows=27 width=20) Group Key: t0.cat -> Sort (cost=955705.47..955707.36 rows=756 width=44) Sort Key: t0.cat -> Gather (cost=955589.95..955669.33 rows=756 width=44) Workers Planned: 4 -> Parallel Custom Scan (GpuPreAgg) (cost=954589.95..954593.73 rows=189 width=44) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on t0 (cost=27682.82..841218.52 rows=99996736 width=12) GPU Projection: t0.cat, t1.ax Outer Scan: t0 (cost=0.00..1083384.84 rows=24999184 width=8) Depth 1: GpuHashJoin (nrows 24999184...99996736) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size: 10.39MB) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) (18 rows)","title":"CPU+GPU Hybrid Parallel"},{"location":"operations/#pullup-underlying-plans","text":"PG-Strom can run SCAN, JOIN and GROUP BY workloads on GPU, however, it does not work with best performance if these custom execution plan simply replace the standard operations at PostgreSQL. An example of problematic scenario is that SCAN once writes back its result data set to the host buffer then send the same data into GPU again to execute JOIN. Once again, JOIN results are written back and send to GPU to execute GROUP BY. It causes data ping-pong between CPU and GPU. To avoid such inefficient jobs, PG-Strom has a special mode which pulls up its sub-plan to execute a bunch of jobs in a single GPU kernel invocation. Combination of the operations blow can cause pull-up of sub-plans. SCAN + JOIN SCAN + GROUP BY SCAN + JOIN + GROUP BY The execution plan example below never pulls up the sub-plans. GpuJoin receives the result of GpuScan, then its results are passed to GpuPreAgg to generate the final results. # EXPLAIN SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 WHERE aid < bid GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=1239991.03..1239995.15 rows=27 width=20) Group Key: t0.cat -> Sort (cost=1239991.03..1239991.50 rows=189 width=44) Sort Key: t0.cat -> Custom Scan (GpuPreAgg) (cost=1239980.10..1239983.88 rows=189 width=44) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) -> Custom Scan (GpuJoin) (cost=50776.43..1199522.96 rows=33332245 width=12) GPU Projection: t0.cat, t1.ax Depth 1: GpuHashJoin (nrows 33332245...33332245) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size: 10.39MB) -> Custom Scan (GpuScan) on t0 (cost=12634.49..1187710.85 rows=33332245 width=8) GPU Projection: cat, aid GPU Filter: (aid < bid) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) (18 rows) This example causes data ping-pong between GPU and host buffers for each execution stage, so not efficient and less performance. On the other hands, the query execution plan below pulls up sub-plans. # EXPLAIN ANALYZE SELECT cat,count(*),avg(ax) FROM t0 NATURAL JOIN t1 WHERE aid < bid GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=903669.50..903673.62 rows=27 width=20) (actual time=7761.630..7761.644 rows=27 loops=1) Group Key: t0.cat -> Sort (cost=903669.50..903669.97 rows=189 width=44) (actual time=7761.621..7761.626 rows=27 loops=1) Sort Key: t0.cat Sort Method: quicksort Memory: 28kB -> Custom Scan (GpuPreAgg) (cost=903658.57..903662.35 rows=189 width=44) (actual time=7761.531..7761.540 rows=27 loops=1) Reduction: Local GPU Projection: cat, pgstrom.nrows(), pgstrom.nrows((ax IS NOT NULL)), pgstrom.psum(ax) Combined GpuJoin: enabled -> Custom Scan (GpuJoin) on t0 (cost=12483.41..863201.43 rows=33332245 width=12) (never executed) GPU Projection: t0.cat, t1.ax Outer Scan: t0 (cost=12634.49..1187710.85 rows=33332245 width=8) (actual time=59.623..5557.052 rows=100000000 loops=1) Outer Scan Filter: (aid < bid) Rows Removed by Outer Scan Filter: 50002874 Depth 1: GpuHashJoin (plan nrows: 33332245...33332245, actual nrows: 49997126...49997126) HashKeys: t0.aid JoinQuals: (t0.aid = t1.aid) KDS-Hash (size plan: 10.39MB, exec: 64.00MB) -> Seq Scan on t1 (cost=0.00..1972.85 rows=103785 width=12) (actual time=0.013..15.303 rows=100000 loops=1) Planning time: 0.506 ms Execution time: 8495.391 ms (21 rows) You may notice that SCAN on the table t0 is embedded into GpuJoin, and GpuScan gets vanished. It means GpuJoin pulls up the underlying GpuScan, then combined GPU kernel function is also responsible for evaluation of the supplied WHERE-clause. In addition, here is a strange output in EXPLAIN ANALYZE result - it displays (never executed) for GpuJoin. It means GpuJoin is never executed during the query execution, and it is right. GpuPreAgg pulls up the underlying GpuJoin, then its combined GPU kernel function runs JOIN and GROUP BY. The pg_strom.pullup_outer_scan parameter controls whether SCAN is pulled up, and the pg_strom.pullup_outer_join parameter also controls whether JOIN is pulled up. Both parameters are configured to on . Usually, no need to disable them, however, you can use the parameters to identify the problems on system troubles.","title":"Pullup underlying plans"},{"location":"partition/","text":"Partitioning This chapter introduces the way to use PG-Strom and the partitioning feature of PostgreSQL. Note that this chapter is only valid when PG-Strom works on PostgreSQL v11 or later . Also see PostgreSQL Document: Table Partitioning for more details of the partitioning feature of PostgreSQL. Brief overview PostgreSQL v10 newly support table partitioning. This mechanism splits one logically large table into physically small pieces. It is valuable because it can skip partitioned child tables which is obviously unnecessary to scan from the search qualification, and it can offer broader I/O bandwidth by physically distributed storage and so on. PostgreSQL v10 supports two kinds of them: range-partitioning and list-partitioning. Then, PostgreSQL v11 newly supports hash-partitioning and partition-wise JOINs. The diagram below shows a range-partitioning configuration with date -type key values. A record which has 2018-05-30 as key is distributed to the partition child table tbl_2018 , in the same way, a record which has 2014-03-21 is distributed to the partition child table tbl_2014 , and so on. In case when scan qualifier WHERE ymd > '2016-07-01'::date is added on scan of the partitioned table for example, it is obvious that tbl_2014 and tbl_2015 contains no records to match, therefore, PostgreSQL' optimizer constructs query execution plan which runs on only tbl_2016 , tbl_2017 and tbl_2018 then merges their results by Append node. It shall perform as if records are read from one logical table. When PG-Strom is used with table partitioning of PostgreSQL together, its optimizer may choose GpuScan to scan the individual partition child tables to be scanned, in the result of cost estimation. In this case, Append node merges the results of GpuScan . On the other hands, if query runs JOIN or GROUP BY, which can be accelerated by PG-Strom, next to the scan on partitioned table, it needs consideration from the standpoint of performance optimization. For example, in case when query scans non-partitioned table then runs JOIN with other tables and GROUP BY, under some conditions, it can handle step-step data exchange on GPU device memory. It is an optimal workload for PG-Strom due to minimized data exchange between GPU and CPU. In case when query runs corresponding workload on the partitioned table, it is problematic that Append node is injected into between the child tables scan and JOIN/GROUP BY. Under the query execution plan, the result of GpuScan must be written back to the host system, then Append merges them and send back the data to GPU to run the following GpuJoin and GpuPreAgg. It is never efficient query execution. The example below shows a query execution plan to the query which includes JOIN and GROUP BY towards the partitioned table pt by the key field ymd of date type; per year distribution. Due to the scan qualification, it omits scan on the partition child tables for 2016 or prior, in addition, a combined JOIN and GROUP BY on the pt_2017 , pt_2018 and pt_2019 shall be executed prior to the Append . # EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- HashAggregate (cost=196410.07..196412.57 rows=200 width=48) Group Key: pt_2017.cat -> Gather (cost=66085.69..196389.07 rows=1200 width=72) Workers Planned: 2 -> Parallel Append (cost=65085.69..195269.07 rows=600 width=72) -> Parallel Custom Scan (GpuPreAgg) (cost=65085.69..65089.69 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2017 (cost=32296.64..74474.20 rows=1050772 width=40) Outer Scan: pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050772...2521854) HashKeys: pt_2017.aid JoinQuals: (pt_2017.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65078.35..65082.35 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2018 (cost=32296.65..74465.75 rows=1050649 width=40) Outer Scan: pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050649...2521557) HashKeys: pt_2018.aid JoinQuals: (pt_2018.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65093.03..65097.03 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2019 (cost=32296.65..74482.64 rows=1050896 width=40) Outer Scan: pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050896...2522151) HashKeys: pt_2019.aid JoinQuals: (pt_2019.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (38 rows) Configuration and Operation By the GUC parameters below, PG-Strom enables/disables the push-down of JOIN/GROUP BY under the partition child tables. Parameter Type Default Description pg_strom.enable_partitionwise_gpujoin bool on Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.enable_partitionwise_gpupreagg bool on Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later. Default of the parameters are on . Once set to off , push-down is disabled. The query execution plan is changed as follows, by EXPLAIN command for the query above section. It uses GpuScan to scan the partition child tables, however, their results are once written back to the host system, then merged by Append and moved to GPU again to process GpuJoin . postgres=# set pg_strom.enable_partitionwise_gpujoin = off; SET postgres=# set pg_strom.enable_partitionwise_gpupreagg = off; SET postgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date group by cat; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- Finalize GroupAggregate (cost=341392.92..341399.42 rows=200 width=48) Group Key: pt.cat -> Sort (cost=341392.92..341393.92 rows=400 width=72) Sort Key: pt.cat -> Gather (cost=341333.63..341375.63 rows=400 width=72) Workers Planned: 2 -> Partial HashAggregate (cost=340333.63..340335.63 rows=200 width=72) Group Key: pt.cat -> Parallel Custom Scan (GpuJoin) (cost=283591.92..283591.92 rows=7565562 width=40) Depth 1: GpuHashJoin (nrows 3152318...7565562) HashKeys: pt.aid JoinQuals: (pt.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Append (cost=28540.80..200673.34 rows=3152318 width=36) -> Parallel Custom Scan (GpuScan) on pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (21 rows) Consideration for SSD/GPU location Limitations Experimental Feature It is an experimental feature to push down GpuJoin and GpuPreAgg to the partitioned child tables, so it may lead unexpected behavior or system crash. In such case, disable the feature using pg_strom.enable_partitionwise_gpujoin or pg_strom.enable_partitionwise_gpupreagg . And report your case to PG-Strom Issues .","title":"Partitioning"},{"location":"partition/#brief-overview","text":"PostgreSQL v10 newly support table partitioning. This mechanism splits one logically large table into physically small pieces. It is valuable because it can skip partitioned child tables which is obviously unnecessary to scan from the search qualification, and it can offer broader I/O bandwidth by physically distributed storage and so on. PostgreSQL v10 supports two kinds of them: range-partitioning and list-partitioning. Then, PostgreSQL v11 newly supports hash-partitioning and partition-wise JOINs. The diagram below shows a range-partitioning configuration with date -type key values. A record which has 2018-05-30 as key is distributed to the partition child table tbl_2018 , in the same way, a record which has 2014-03-21 is distributed to the partition child table tbl_2014 , and so on. In case when scan qualifier WHERE ymd > '2016-07-01'::date is added on scan of the partitioned table for example, it is obvious that tbl_2014 and tbl_2015 contains no records to match, therefore, PostgreSQL' optimizer constructs query execution plan which runs on only tbl_2016 , tbl_2017 and tbl_2018 then merges their results by Append node. It shall perform as if records are read from one logical table. When PG-Strom is used with table partitioning of PostgreSQL together, its optimizer may choose GpuScan to scan the individual partition child tables to be scanned, in the result of cost estimation. In this case, Append node merges the results of GpuScan . On the other hands, if query runs JOIN or GROUP BY, which can be accelerated by PG-Strom, next to the scan on partitioned table, it needs consideration from the standpoint of performance optimization. For example, in case when query scans non-partitioned table then runs JOIN with other tables and GROUP BY, under some conditions, it can handle step-step data exchange on GPU device memory. It is an optimal workload for PG-Strom due to minimized data exchange between GPU and CPU. In case when query runs corresponding workload on the partitioned table, it is problematic that Append node is injected into between the child tables scan and JOIN/GROUP BY. Under the query execution plan, the result of GpuScan must be written back to the host system, then Append merges them and send back the data to GPU to run the following GpuJoin and GpuPreAgg. It is never efficient query execution. The example below shows a query execution plan to the query which includes JOIN and GROUP BY towards the partitioned table pt by the key field ymd of date type; per year distribution. Due to the scan qualification, it omits scan on the partition child tables for 2016 or prior, in addition, a combined JOIN and GROUP BY on the pt_2017 , pt_2018 and pt_2019 shall be executed prior to the Append . # EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- HashAggregate (cost=196410.07..196412.57 rows=200 width=48) Group Key: pt_2017.cat -> Gather (cost=66085.69..196389.07 rows=1200 width=72) Workers Planned: 2 -> Parallel Append (cost=65085.69..195269.07 rows=600 width=72) -> Parallel Custom Scan (GpuPreAgg) (cost=65085.69..65089.69 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2017 (cost=32296.64..74474.20 rows=1050772 width=40) Outer Scan: pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050772...2521854) HashKeys: pt_2017.aid JoinQuals: (pt_2017.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65078.35..65082.35 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2018 (cost=32296.65..74465.75 rows=1050649 width=40) Outer Scan: pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050649...2521557) HashKeys: pt_2018.aid JoinQuals: (pt_2018.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65093.03..65097.03 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2019 (cost=32296.65..74482.64 rows=1050896 width=40) Outer Scan: pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050896...2522151) HashKeys: pt_2019.aid JoinQuals: (pt_2019.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (38 rows)","title":"Brief overview"},{"location":"partition/#configuration-and-operation","text":"By the GUC parameters below, PG-Strom enables/disables the push-down of JOIN/GROUP BY under the partition child tables. Parameter Type Default Description pg_strom.enable_partitionwise_gpujoin bool on Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.enable_partitionwise_gpupreagg bool on Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later. Default of the parameters are on . Once set to off , push-down is disabled. The query execution plan is changed as follows, by EXPLAIN command for the query above section. It uses GpuScan to scan the partition child tables, however, their results are once written back to the host system, then merged by Append and moved to GPU again to process GpuJoin . postgres=# set pg_strom.enable_partitionwise_gpujoin = off; SET postgres=# set pg_strom.enable_partitionwise_gpupreagg = off; SET postgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date group by cat; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- Finalize GroupAggregate (cost=341392.92..341399.42 rows=200 width=48) Group Key: pt.cat -> Sort (cost=341392.92..341393.92 rows=400 width=72) Sort Key: pt.cat -> Gather (cost=341333.63..341375.63 rows=400 width=72) Workers Planned: 2 -> Partial HashAggregate (cost=340333.63..340335.63 rows=200 width=72) Group Key: pt.cat -> Parallel Custom Scan (GpuJoin) (cost=283591.92..283591.92 rows=7565562 width=40) Depth 1: GpuHashJoin (nrows 3152318...7565562) HashKeys: pt.aid JoinQuals: (pt.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Append (cost=28540.80..200673.34 rows=3152318 width=36) -> Parallel Custom Scan (GpuScan) on pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (21 rows)","title":"Configuration and Operation"},{"location":"partition/#consideration-for-ssdgpu-location","text":"","title":"Consideration for SSD/GPU location"},{"location":"partition/#limitations","text":"Experimental Feature It is an experimental feature to push down GpuJoin and GpuPreAgg to the partitioned child tables, so it may lead unexpected behavior or system crash. In such case, disable the feature using pg_strom.enable_partitionwise_gpujoin or pg_strom.enable_partitionwise_gpupreagg . And report your case to PG-Strom Issues .","title":"Limitations"},{"location":"plcuda/","text":"This chapter introduces the way to implement GPU executable native program as SQL functions, using PL/CUDA procedural language. PL/CUDA Overview PG-Strom internally constructs GPU programs by CUDA language, according to the supplied SQL, then generates GPU's native binary using just-in-time compile. CUDA is a programming environment provided by NVIDIA. It allows implementing parallel program which is executable on GPU device, using C-like statement. This transformation process from SQL statement to CUDA program is an internal process, thus, no need to pay attention what GPU programs are generated and executed from the standpoint of users. On the other hands, PostgreSQL supports to add programming language to implement SQL functions by CREATE LANGUAGE statement. PL/CUDA is a language handler to supports CREATE LANGUAGE command. It also allows users to run arbitrary GPU programs manually implemented as SQL functions, but not only GPU programs automatically generated by PG-Strom based on SQL. Its argument can take the data types supported by PG-Strom, like numeric, text, or array-matrix data type. These arguments are implicitly loaded onto GPU device memory by the PL/CUDA infrastructure, so users don't need to pay attention for data loading between the database and GPU devices. In a similar fashion, the return value of PL/CUDA function (including the case of variable length data type) will be written back to CPU from GPU, then decode to the result of SQL function. You can also use foreign tables defined with gstore_fdw as arguments of PL/CUDA function. In this case, no need to load the data onto GPU for each invocation because foreign table already keeps the data, and available to use larger data than 1GB which is a restriction of variable length data in PostgreSQL. Therefore, users can focus on productive tasks like implementation of statistical analysis, code optimization and so on, without routine process like data input/output between GPU and databases. Once a PL/CUDA function is declared using CREATE FUNCTION , it generates a CUDA program source code that embeds the definition of this function, then build it for the target GPU device. This CUDA program is almost identical to usual GPU software based on CUDA runtime, except for the auxiliary code to receive arguments of SQL function and to write back its results. It also allows to include/link some libraries for CUDA device runtime. Native CUDA programs implemented by PL/CUDA are executed as child-processes of PostgreSQL backend. Therefore, it has independent address space and OS/GPU resources from PostgreSQL. CUDA program contains host code for the host system and device code to be executed on GPU devices. The host code can execute any logic we can program using C-language, so we restrict only database superuser can define PL/CUDA function from the standpoint of security. Below is an example of simple PL/CUDA function. This function takes two same length real[] array as arguments, then returns its dot product in float data type. CREATE OR REPLACE FUNCTION gpu_dot_product(real[], real[]) RETURNS float AS $$ #plcuda_decl #include \"cuda_matrix.h\" KERNEL_FUNCTION_MAXTHREADS(void) gpu_dot_product(double *p_dot, VectorTypeFloat *X, VectorTypeFloat *Y) { size_t index = get_global_id(); size_t nitems = X->height; float v[MAXTHREADS_PER_BLOCK]; float sum; if (index < nitems) v[get_local_id()] = X->values[index] * Y->values[index]; else v[get_local_id()] = 0.0; sum = pgstromTotalSum(v, MAXTHREADS_PER_BLOCK); if (get_local_id() == 0) atomicAdd(p_dot, (double)sum); __syncthreads(); } #plcuda_begin { size_t nitems; int blockSz; int gridSz; double *dot; cudaError_t rc; if (!VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg1, PG_FLOAT4OID) || !VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg2, PG_FLOAT4OID)) EEXIT(\"arguments are not vector like array\"); nitems = ARRAY_VECTOR_HEIGHT(arg1); if (nitems != ARRAY_VECTOR_HEIGHT(arg2)) EEXIT(\"length of arguments mismatch\"); rc = cudaMallocManaged(&dot, sizeof(double)); if (rc != cudaSuccess) CUEXIT(rc, \"failed on cudaMallocManaged\"); memset(dot, 0, sizeof(double)); blockSz = MAXTHREADS_PER_BLOCK; gridSz = (nitems + MAXTHREADS_PER_BLOCK - 1) / MAXTHREADS_PER_BLOCK; gpu_dot_product<<<gridSz,blockSz>>>(dot, (VectorTypeFloat *)arg1, (VectorTypeFloat *)arg2); rc = cudaStreamSynchronize(NULL); if (rc != cudaSuccess) CUEXIT(rc, \"failed on cudaStreamSynchronize\"); return *dot; } #plcuda_end $$ LANGUAGE 'plcuda'; PL/CUDA infrastructure makes entrypoint function of CUDA program by the block between #plcuda_begin and #plcuda_end with extra code to exchange arguments of SQL function. The portion enclosed by #plcuda_decl and #plcuda_begin is a block for declaration of GPU device functions and other host functions. It is placed prior to the entrypoint above. At the entrypoint of the CUDA program, you can refer the arguments of SQL function using arg1 , `arg2, and so on. In the above example, the arg1 and arg2 , real[] array type, are passed to the entrypoint, then VALIDATE_ARRAY_VECTOR_TYPE_STRICT macro checks whether it is 1-dimensional array of 32bit floating-point values without NULL. Ditto with return value, the entrypoint returns a value in CUDA C representation corresponding to the SQL data type. If entrypoint does not return any value (or, it exits the program with status code 1 by exit() ), it is considered PL/CUDA function returns NULL . The above sample program validates the array of real values passed from SQL function, then it allocates the result buffer by cudaMallocManaged , and invokes gpu_dot_product , a GPU kernel function, to compute dot product with two vectors. The result of this function is below. It computes the dot product of two vectors which contain 10,000 items randomly generated. postgres=# SELECT gpu_dot_product(array_matrix(random()::real), array_matrix(random()::real)) FROM generate_series(1,10000); gpu_dot_product ------------------ 3.71461999509484 (1 row) PL/CUDA Structure Function declaration of PL/CUDA is consists of two code blocks split by the directives of #plcuda_decl , #plcuda_begin and #plcuda_end . Users can put their custom code on the code blocks according to the purpose, then PL/CUDA language handler reconstruct them into single source file with extra logic to exchange function arguments and results. #plcuda_decl [...any declarations...] #plcuda_begin [...host code in the entrypoint...] #plcuda_end The code block, begins from #plcuda_decl , can have declaration of __host__ and __device__ functions and variables for CUDA C. This code block locates in front of the entrypoint function which contains the code block between #plcuda_begin and #plcuda_end at the source file eventually constructed. If external header files are included using #include statement of CUDA C, put the statement on this code block. The code block between #plcuda_begin and #plcuda_end is embedded to a part of entrypoint function. Therefore, it does not describe function name, arguments definition and so on. Prior to execution of the code block, the entrypoint function receives arguments of the SQL function from PostgreSQL backend, and set up arg1 , arg2 , ... variables for further references. These variables have the following CUDA C representation according to SQL data types. SQL data type CUDA C data type Examples reggstore void * OID of Gstore_fdw foreign table real float 32bit floating point float double 64bit floating point Other inline data types Datum int , date , ... Fixed-length value by reference void * uuid , ... Variable-length value (varlena) varlena * text , real[] , ... PL/CUDA language handler constructs a single CUDA C source file from the code blocks above, then builds it once by nvcc compiler at declaration or execution time. If it contains any #plcuda_include directive, its source code is not fixed until execution time, so built at the execution time only. In case when identical CUDA program is already pre-built, we can reuse it without rebuild. When SQL command invokes PL/CUDA function, PL/CUDA language handler launch the pre-built CUDA program, then copies the arguments of SQL function over pipe. These are stored in the argument buffer of the CUDA program, so custom logic can refer them using arg1 or arg2 variables. The data types by reference at CUDA C program, like variable-length datum, are initialized as pointers to the argument buffer. It is a managed memory region allocated by cudaMallocManaged() , these pointers are available without explicit DMA between host system and GPU devices. Here is a special case if argument has reggstore type. It is actually an OID (32bit integer) of Gstore_Fdw foreign table, however, it is replaced to the reference of GPU device memory acquired by the Gstore_Fdw foreign table if it is supplied as PL/CUDA argument. The argument is setup to the pointer for GstoreIpcMapping object. GstoreIpcMapping::map holds the mapped address of the GPU device memory acquired by the Gstore_Fdw foreign table. GstoreIpcHandle::device_id indicates the device-id of GPU which physically holds the region, and GstoreIpcHandle::rawsize` is raw length of the region. typedef struct { cl_uint __vl_len; /* 4B varlena header */ cl_short device_id; /* GPU device where pinning on */ cl_char format; /* one of GSTORE_FDW_FORMAT__* */ cl_char __padding__; /* reserved */ cl_long rawsize; /* length in bytes */ union { #ifdef CU_IPC_HANDLE_SIZE CUipcMemHandle d; /* CUDA driver API */ #endif #ifdef CUDA_IPC_HANDLE_SIZE cudaIpcMemHandle_t r; /* CUDA runtime API */ #endif char data[64]; } ipc_mhandle; } GstoreIpcHandle; typedef struct { GstoreIpcHandle h; /* IPChandle of Gstore_Fdw */ void *map; /* mapped device pointer */ } GstoreIpcMapping; PL/CUDA function can return its result using return of a CUDA C datum relevant to the SQL data type. In case when no return clause is executed, NULL pointer is returned if CUDA C data type is pointer, or CUDA program is terminated with status code = 1 by exit(1) , PL/CUDA function returns null to SQL. PL/CUDA References This section is a reference for PL/CUDA function's directives and related SQL functions. Advantage and disadvantage of PL/CUDA On invocation of PL/CUDA function, it launches the relevant CUDA program on behalf of the invocation, then CUDA program initialize per process context of GPU device. The series of operations are never lightweight, so we don't recommend to implement a simple comparison of scalar values using PL/CUDA, and use for full table scan on billion rows. On the other hands, once GPU device is correctly initialized, it allows to process massive amount of data using several thousands of processor cores on GPU device. Especially, it is suitable for computing intensive workloads, like machine-learning or advanced analytics that approach to the optimal values by repeated calculation for example. According to the growth of data size, we need to pay attention how to exchange data with CUDA program. PostgreSQL supports array types, and it is easy and simple way to exchange several millions of integer or real values at most. However, variable-length datum of PostgreSQL, including the array-types, is restricted to 1GB at a maximum. We need to take a little idea to handle larger data, like separation of data-set. In addition, PostgreSQL backend process set up the argument of SQL functions in single thread, so it takes a certain amount of time to manipulate gigabytes-class memory object. Please consider usage of Gstore_Fdw foreign-table when data size grows more than several hundreds megabytes. Once you preload the large data-set onto GPU device memory through Gstore_Fdw, no need to set up large arguments on invocation of PL/CUDA function. It also allows to keep larger data than gigabytes, as lond as GPU device memory capacity allows. PL/CUDA Directives #plcuda_decl This directive begins a code block which contains CUDA C functions and variables with both of __host__ and __device__ attributes. PL/CUDA language handler copies this code block in front of the program entrypoint as is. Use of this directive is optional, however, it makes no sense if here is no declaration of GPU kernel functions to be called from the entrypoint. So, we usually have more than one GPU kernel function. #plcuda_begin This directive begins a code block which consists a part of the entrypoint of CUDA program. The CUDA program setup the referable arg1 , arg2 , ... variables according to the arguments of PL/CUDA function, then switch control to the user defined portion. This code block is a host code; we can implement own control logic working on CPU or heavy calculation by GPU kernel invocation. Result of PL/CUDA function can be returned using return statement of CUDA C, according to the function definition. #plcuda_end It marks end of the kernel function code block. By the way, if a directive to start code block was put inside of the different code block, the current code block is implicitly closed by the #plcuda_end directive. #plcuda_include <function name> This directive is similar to #include of CUDA C, however, it injects result of the specified SQL function onto the location where the directive was written. The SQL function should have identical arguments and return text data. For example, when we calculate similarity of massive items, we can generate multiple variant of the algorithm on the fly that is almost equivalent but only distance definitions are different. It makes maintenance of PL/CUDA function simplified. #plcuda_library <library name> It specifies the library name to be linked when CUDA program is built by nvcc . The <library name> portion is supplied to nvcc command as -l option. For example, if libcublas.co library is linked, you need to describe cublas without prefix ( lib ) and suffix ( .so ). Right now, we can specify the libraries only installed on the standard library path of CUDA Toolkit (`/usr/local/cuda/lib64). #plcuda_sanity_check <function> It allows to specify the sanity check function that preliminary checks adequacy of the supplied arguments, prior to GPU kernel launch. No sanity check function is configured on the default. Usually, launch of GPU kernel function is heavier task than call of another function on CPU, because it also involves initialization of GPU devices. If supplied arguments have unacceptable values from the specification of the PL/CUDA function, a few thousands or millions (or more in some cases) of GPU kernel threads shall be launched just to check the arguments and return an error status. If sanity check can be applied prior to the launch of GPU kernel function with enough small cost, it is a valuable idea to raise an error using sanity check function prior to the GPU kernel function. The sanity check function takes identical arguments with PL/CUDA function, and returns bool data type. PL/CUDA Related Functions Definition Result Description plcuda_function_source(regproc) text It returns source code of the GPU kernel generated from the PL/CUDA function, towards the OID input of PL/CUDA function as argument. Support functions for PL/CUDA invocations The functions below are provided to simplify invocation of PL/CUDA functions. Definition Result Description attnums_of(regclass,text[]) smallint[] It returns attribute numbers for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. attnum_of(regclass,text) smallint It returns attribute number for the column name of the 2nd argument on the table of the 1st argument. atttypes_of(regclass,text[]) regtype[] It returns data types for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. atttype_of(regclass,text) regtype It returns data type for the column name of the 2nd argument on the table of the 1st argument. attrs_types_check(regclass,text[],regtype[]) bool It checks whether the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data types of the 3rd argument for each. attrs_type_check(regclass,text[],regtype) bool It checks whether all the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data type of the 3rd argument. Array-Matrix Functions This section introduces the SQL functions that supports array-based matrix types provided by PG-Strom. 2-dimensional Array Element of array begins from 1 for each dimension No NULL value is contained Length of the array is less than 1GB, due to the restriction of variable length datum in PostgreSQL Array with smallint , int , bigint , real or float data type If and when the array satisfies the above terms, we can determine the location of (i,j) element of the array by the index uniquely, and it enables GPU thread to fetch the datum to be processed very efficiently. Also, array-based matrix packs only the data to be used for calculation, unlike usual row-based format, so it has advantaged on memory consumption and data transfer. Definition Result Description array_matrix(variadic arg, ...) array It is an aggregate function that combines all the rows supplied. For example, when 3 float arguments were supplied by 1000 rows, it returns an array-based matrix of 3 columns X 1000 rows, with float data type. This function is declared to take variable length arguments. The arg takes one or more scalar values of either smallint , int , bigint , real or float . All the arg must have same data types. matrix_unnest(array) record It is a set function that extracts the array-based matrix to set of records. array is an array of smallint , int , bigint , real or float data. It returns record type which consists of more than one columns according to the width of matrix. For example, in case of a matrix of 10 columns X 500 rows, each records contains 10 columns with element type of the matrix, then it generates 500 of the records. It is similar to the standard unnest function, but generates record type, thus, it requires to specify the record type to be returned using AS (colname1 type[, ...]) clause. rbind(array, array) array array is an array of smallint , int , bigint , real or float data. This function combines the supplied two matrices vertically. Both matrices needs to have same element data type. If width of matrices are not equivalent, it fills up the padding area by zero. rbind(array) array array is an array of smallint , int , bigint , real or float data. This function is similar to rbind(array, array) , but performs as an aggregate function, then combines all the input matrices into one result vertically. cbind(array, array) array array is an array of smallint , int , bigint , real or float data. This function combines the supplied two matrices horizontally. Both matrices needs to have same element data type. If height of matrices are not equivalent, it fills up the padding area by zero. cbind(array) array array is an array of smallint , int , bigint , real or float data. This function is similar to cbind(array, array), but performs as an aggregate function, then combines all the input matrices into one result horizontally. transpose(array) array array is an array of smallint , int , bigint , real or float data. This function makes a transposed matrix that swaps height and width of the supplied matrix. array_matrix_validation(anyarray) bool It validates whether the supplied array ( anyarray ) is adequate for the array-based matrix. It is intended to use for sanity check prior to invocation of PL/CUDA function, or check constraint on domain type definition. array_matrix_height(array) int array is an array of either smallint , int , bigint , real or float data. This function returns the height of the supplied matrix. array_matrix_width(array) int array is an array of either smallint , int , bigint , real or float data. This function returns the width of the supplied matrix.","title":"PL/CUDA"},{"location":"plcuda/#plcuda-overview","text":"PG-Strom internally constructs GPU programs by CUDA language, according to the supplied SQL, then generates GPU's native binary using just-in-time compile. CUDA is a programming environment provided by NVIDIA. It allows implementing parallel program which is executable on GPU device, using C-like statement. This transformation process from SQL statement to CUDA program is an internal process, thus, no need to pay attention what GPU programs are generated and executed from the standpoint of users. On the other hands, PostgreSQL supports to add programming language to implement SQL functions by CREATE LANGUAGE statement. PL/CUDA is a language handler to supports CREATE LANGUAGE command. It also allows users to run arbitrary GPU programs manually implemented as SQL functions, but not only GPU programs automatically generated by PG-Strom based on SQL. Its argument can take the data types supported by PG-Strom, like numeric, text, or array-matrix data type. These arguments are implicitly loaded onto GPU device memory by the PL/CUDA infrastructure, so users don't need to pay attention for data loading between the database and GPU devices. In a similar fashion, the return value of PL/CUDA function (including the case of variable length data type) will be written back to CPU from GPU, then decode to the result of SQL function. You can also use foreign tables defined with gstore_fdw as arguments of PL/CUDA function. In this case, no need to load the data onto GPU for each invocation because foreign table already keeps the data, and available to use larger data than 1GB which is a restriction of variable length data in PostgreSQL. Therefore, users can focus on productive tasks like implementation of statistical analysis, code optimization and so on, without routine process like data input/output between GPU and databases. Once a PL/CUDA function is declared using CREATE FUNCTION , it generates a CUDA program source code that embeds the definition of this function, then build it for the target GPU device. This CUDA program is almost identical to usual GPU software based on CUDA runtime, except for the auxiliary code to receive arguments of SQL function and to write back its results. It also allows to include/link some libraries for CUDA device runtime. Native CUDA programs implemented by PL/CUDA are executed as child-processes of PostgreSQL backend. Therefore, it has independent address space and OS/GPU resources from PostgreSQL. CUDA program contains host code for the host system and device code to be executed on GPU devices. The host code can execute any logic we can program using C-language, so we restrict only database superuser can define PL/CUDA function from the standpoint of security. Below is an example of simple PL/CUDA function. This function takes two same length real[] array as arguments, then returns its dot product in float data type. CREATE OR REPLACE FUNCTION gpu_dot_product(real[], real[]) RETURNS float AS $$ #plcuda_decl #include \"cuda_matrix.h\" KERNEL_FUNCTION_MAXTHREADS(void) gpu_dot_product(double *p_dot, VectorTypeFloat *X, VectorTypeFloat *Y) { size_t index = get_global_id(); size_t nitems = X->height; float v[MAXTHREADS_PER_BLOCK]; float sum; if (index < nitems) v[get_local_id()] = X->values[index] * Y->values[index]; else v[get_local_id()] = 0.0; sum = pgstromTotalSum(v, MAXTHREADS_PER_BLOCK); if (get_local_id() == 0) atomicAdd(p_dot, (double)sum); __syncthreads(); } #plcuda_begin { size_t nitems; int blockSz; int gridSz; double *dot; cudaError_t rc; if (!VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg1, PG_FLOAT4OID) || !VALIDATE_ARRAY_VECTOR_TYPE_STRICT(arg2, PG_FLOAT4OID)) EEXIT(\"arguments are not vector like array\"); nitems = ARRAY_VECTOR_HEIGHT(arg1); if (nitems != ARRAY_VECTOR_HEIGHT(arg2)) EEXIT(\"length of arguments mismatch\"); rc = cudaMallocManaged(&dot, sizeof(double)); if (rc != cudaSuccess) CUEXIT(rc, \"failed on cudaMallocManaged\"); memset(dot, 0, sizeof(double)); blockSz = MAXTHREADS_PER_BLOCK; gridSz = (nitems + MAXTHREADS_PER_BLOCK - 1) / MAXTHREADS_PER_BLOCK; gpu_dot_product<<<gridSz,blockSz>>>(dot, (VectorTypeFloat *)arg1, (VectorTypeFloat *)arg2); rc = cudaStreamSynchronize(NULL); if (rc != cudaSuccess) CUEXIT(rc, \"failed on cudaStreamSynchronize\"); return *dot; } #plcuda_end $$ LANGUAGE 'plcuda'; PL/CUDA infrastructure makes entrypoint function of CUDA program by the block between #plcuda_begin and #plcuda_end with extra code to exchange arguments of SQL function. The portion enclosed by #plcuda_decl and #plcuda_begin is a block for declaration of GPU device functions and other host functions. It is placed prior to the entrypoint above. At the entrypoint of the CUDA program, you can refer the arguments of SQL function using arg1 , `arg2, and so on. In the above example, the arg1 and arg2 , real[] array type, are passed to the entrypoint, then VALIDATE_ARRAY_VECTOR_TYPE_STRICT macro checks whether it is 1-dimensional array of 32bit floating-point values without NULL. Ditto with return value, the entrypoint returns a value in CUDA C representation corresponding to the SQL data type. If entrypoint does not return any value (or, it exits the program with status code 1 by exit() ), it is considered PL/CUDA function returns NULL . The above sample program validates the array of real values passed from SQL function, then it allocates the result buffer by cudaMallocManaged , and invokes gpu_dot_product , a GPU kernel function, to compute dot product with two vectors. The result of this function is below. It computes the dot product of two vectors which contain 10,000 items randomly generated. postgres=# SELECT gpu_dot_product(array_matrix(random()::real), array_matrix(random()::real)) FROM generate_series(1,10000); gpu_dot_product ------------------ 3.71461999509484 (1 row)","title":"PL/CUDA Overview"},{"location":"plcuda/#plcuda-structure","text":"Function declaration of PL/CUDA is consists of two code blocks split by the directives of #plcuda_decl , #plcuda_begin and #plcuda_end . Users can put their custom code on the code blocks according to the purpose, then PL/CUDA language handler reconstruct them into single source file with extra logic to exchange function arguments and results. #plcuda_decl [...any declarations...] #plcuda_begin [...host code in the entrypoint...] #plcuda_end The code block, begins from #plcuda_decl , can have declaration of __host__ and __device__ functions and variables for CUDA C. This code block locates in front of the entrypoint function which contains the code block between #plcuda_begin and #plcuda_end at the source file eventually constructed. If external header files are included using #include statement of CUDA C, put the statement on this code block. The code block between #plcuda_begin and #plcuda_end is embedded to a part of entrypoint function. Therefore, it does not describe function name, arguments definition and so on. Prior to execution of the code block, the entrypoint function receives arguments of the SQL function from PostgreSQL backend, and set up arg1 , arg2 , ... variables for further references. These variables have the following CUDA C representation according to SQL data types. SQL data type CUDA C data type Examples reggstore void * OID of Gstore_fdw foreign table real float 32bit floating point float double 64bit floating point Other inline data types Datum int , date , ... Fixed-length value by reference void * uuid , ... Variable-length value (varlena) varlena * text , real[] , ... PL/CUDA language handler constructs a single CUDA C source file from the code blocks above, then builds it once by nvcc compiler at declaration or execution time. If it contains any #plcuda_include directive, its source code is not fixed until execution time, so built at the execution time only. In case when identical CUDA program is already pre-built, we can reuse it without rebuild. When SQL command invokes PL/CUDA function, PL/CUDA language handler launch the pre-built CUDA program, then copies the arguments of SQL function over pipe. These are stored in the argument buffer of the CUDA program, so custom logic can refer them using arg1 or arg2 variables. The data types by reference at CUDA C program, like variable-length datum, are initialized as pointers to the argument buffer. It is a managed memory region allocated by cudaMallocManaged() , these pointers are available without explicit DMA between host system and GPU devices. Here is a special case if argument has reggstore type. It is actually an OID (32bit integer) of Gstore_Fdw foreign table, however, it is replaced to the reference of GPU device memory acquired by the Gstore_Fdw foreign table if it is supplied as PL/CUDA argument. The argument is setup to the pointer for GstoreIpcMapping object. GstoreIpcMapping::map holds the mapped address of the GPU device memory acquired by the Gstore_Fdw foreign table. GstoreIpcHandle::device_id indicates the device-id of GPU which physically holds the region, and GstoreIpcHandle::rawsize` is raw length of the region. typedef struct { cl_uint __vl_len; /* 4B varlena header */ cl_short device_id; /* GPU device where pinning on */ cl_char format; /* one of GSTORE_FDW_FORMAT__* */ cl_char __padding__; /* reserved */ cl_long rawsize; /* length in bytes */ union { #ifdef CU_IPC_HANDLE_SIZE CUipcMemHandle d; /* CUDA driver API */ #endif #ifdef CUDA_IPC_HANDLE_SIZE cudaIpcMemHandle_t r; /* CUDA runtime API */ #endif char data[64]; } ipc_mhandle; } GstoreIpcHandle; typedef struct { GstoreIpcHandle h; /* IPChandle of Gstore_Fdw */ void *map; /* mapped device pointer */ } GstoreIpcMapping; PL/CUDA function can return its result using return of a CUDA C datum relevant to the SQL data type. In case when no return clause is executed, NULL pointer is returned if CUDA C data type is pointer, or CUDA program is terminated with status code = 1 by exit(1) , PL/CUDA function returns null to SQL.","title":"PL/CUDA Structure"},{"location":"plcuda/#plcuda-references","text":"This section is a reference for PL/CUDA function's directives and related SQL functions.","title":"PL/CUDA References"},{"location":"plcuda/#advantage-and-disadvantage-of-plcuda","text":"On invocation of PL/CUDA function, it launches the relevant CUDA program on behalf of the invocation, then CUDA program initialize per process context of GPU device. The series of operations are never lightweight, so we don't recommend to implement a simple comparison of scalar values using PL/CUDA, and use for full table scan on billion rows. On the other hands, once GPU device is correctly initialized, it allows to process massive amount of data using several thousands of processor cores on GPU device. Especially, it is suitable for computing intensive workloads, like machine-learning or advanced analytics that approach to the optimal values by repeated calculation for example. According to the growth of data size, we need to pay attention how to exchange data with CUDA program. PostgreSQL supports array types, and it is easy and simple way to exchange several millions of integer or real values at most. However, variable-length datum of PostgreSQL, including the array-types, is restricted to 1GB at a maximum. We need to take a little idea to handle larger data, like separation of data-set. In addition, PostgreSQL backend process set up the argument of SQL functions in single thread, so it takes a certain amount of time to manipulate gigabytes-class memory object. Please consider usage of Gstore_Fdw foreign-table when data size grows more than several hundreds megabytes. Once you preload the large data-set onto GPU device memory through Gstore_Fdw, no need to set up large arguments on invocation of PL/CUDA function. It also allows to keep larger data than gigabytes, as lond as GPU device memory capacity allows.","title":"Advantage and disadvantage of PL/CUDA"},{"location":"plcuda/#plcuda-directives","text":"","title":"PL/CUDA Directives"},{"location":"plcuda/#plcuda_decl","text":"This directive begins a code block which contains CUDA C functions and variables with both of __host__ and __device__ attributes. PL/CUDA language handler copies this code block in front of the program entrypoint as is. Use of this directive is optional, however, it makes no sense if here is no declaration of GPU kernel functions to be called from the entrypoint. So, we usually have more than one GPU kernel function.","title":"#plcuda_decl"},{"location":"plcuda/#plcuda_begin","text":"This directive begins a code block which consists a part of the entrypoint of CUDA program. The CUDA program setup the referable arg1 , arg2 , ... variables according to the arguments of PL/CUDA function, then switch control to the user defined portion. This code block is a host code; we can implement own control logic working on CPU or heavy calculation by GPU kernel invocation. Result of PL/CUDA function can be returned using return statement of CUDA C, according to the function definition.","title":"#plcuda_begin"},{"location":"plcuda/#plcuda_end","text":"It marks end of the kernel function code block. By the way, if a directive to start code block was put inside of the different code block, the current code block is implicitly closed by the #plcuda_end directive.","title":"#plcuda_end"},{"location":"plcuda/#plcuda_include-ltfunction-namegt","text":"This directive is similar to #include of CUDA C, however, it injects result of the specified SQL function onto the location where the directive was written. The SQL function should have identical arguments and return text data. For example, when we calculate similarity of massive items, we can generate multiple variant of the algorithm on the fly that is almost equivalent but only distance definitions are different. It makes maintenance of PL/CUDA function simplified.","title":"#plcuda_include &lt;function name&gt;"},{"location":"plcuda/#plcuda_library-ltlibrary-namegt","text":"It specifies the library name to be linked when CUDA program is built by nvcc . The <library name> portion is supplied to nvcc command as -l option. For example, if libcublas.co library is linked, you need to describe cublas without prefix ( lib ) and suffix ( .so ). Right now, we can specify the libraries only installed on the standard library path of CUDA Toolkit (`/usr/local/cuda/lib64).","title":"#plcuda_library &lt;library name&gt;"},{"location":"plcuda/#plcuda_sanity_check-ltfunctiongt","text":"It allows to specify the sanity check function that preliminary checks adequacy of the supplied arguments, prior to GPU kernel launch. No sanity check function is configured on the default. Usually, launch of GPU kernel function is heavier task than call of another function on CPU, because it also involves initialization of GPU devices. If supplied arguments have unacceptable values from the specification of the PL/CUDA function, a few thousands or millions (or more in some cases) of GPU kernel threads shall be launched just to check the arguments and return an error status. If sanity check can be applied prior to the launch of GPU kernel function with enough small cost, it is a valuable idea to raise an error using sanity check function prior to the GPU kernel function. The sanity check function takes identical arguments with PL/CUDA function, and returns bool data type.","title":"#plcuda_sanity_check &lt;function&gt;"},{"location":"plcuda/#plcuda-related-functions","text":"Definition Result Description plcuda_function_source(regproc) text It returns source code of the GPU kernel generated from the PL/CUDA function, towards the OID input of PL/CUDA function as argument.","title":"PL/CUDA Related Functions"},{"location":"plcuda/#support-functions-for-plcuda-invocations","text":"The functions below are provided to simplify invocation of PL/CUDA functions. Definition Result Description attnums_of(regclass,text[]) smallint[] It returns attribute numbers for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. attnum_of(regclass,text) smallint It returns attribute number for the column name of the 2nd argument on the table of the 1st argument. atttypes_of(regclass,text[]) regtype[] It returns data types for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. atttype_of(regclass,text) regtype It returns data type for the column name of the 2nd argument on the table of the 1st argument. attrs_types_check(regclass,text[],regtype[]) bool It checks whether the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data types of the 3rd argument for each. attrs_type_check(regclass,text[],regtype) bool It checks whether all the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data type of the 3rd argument.","title":"Support functions for PL/CUDA invocations"},{"location":"plcuda/#array-matrix-functions","text":"This section introduces the SQL functions that supports array-based matrix types provided by PG-Strom. 2-dimensional Array Element of array begins from 1 for each dimension No NULL value is contained Length of the array is less than 1GB, due to the restriction of variable length datum in PostgreSQL Array with smallint , int , bigint , real or float data type If and when the array satisfies the above terms, we can determine the location of (i,j) element of the array by the index uniquely, and it enables GPU thread to fetch the datum to be processed very efficiently. Also, array-based matrix packs only the data to be used for calculation, unlike usual row-based format, so it has advantaged on memory consumption and data transfer. Definition Result Description array_matrix(variadic arg, ...) array It is an aggregate function that combines all the rows supplied. For example, when 3 float arguments were supplied by 1000 rows, it returns an array-based matrix of 3 columns X 1000 rows, with float data type. This function is declared to take variable length arguments. The arg takes one or more scalar values of either smallint , int , bigint , real or float . All the arg must have same data types. matrix_unnest(array) record It is a set function that extracts the array-based matrix to set of records. array is an array of smallint , int , bigint , real or float data. It returns record type which consists of more than one columns according to the width of matrix. For example, in case of a matrix of 10 columns X 500 rows, each records contains 10 columns with element type of the matrix, then it generates 500 of the records. It is similar to the standard unnest function, but generates record type, thus, it requires to specify the record type to be returned using AS (colname1 type[, ...]) clause. rbind(array, array) array array is an array of smallint , int , bigint , real or float data. This function combines the supplied two matrices vertically. Both matrices needs to have same element data type. If width of matrices are not equivalent, it fills up the padding area by zero. rbind(array) array array is an array of smallint , int , bigint , real or float data. This function is similar to rbind(array, array) , but performs as an aggregate function, then combines all the input matrices into one result vertically. cbind(array, array) array array is an array of smallint , int , bigint , real or float data. This function combines the supplied two matrices horizontally. Both matrices needs to have same element data type. If height of matrices are not equivalent, it fills up the padding area by zero. cbind(array) array array is an array of smallint , int , bigint , real or float data. This function is similar to cbind(array, array), but performs as an aggregate function, then combines all the input matrices into one result horizontally. transpose(array) array array is an array of smallint , int , bigint , real or float data. This function makes a transposed matrix that swaps height and width of the supplied matrix. array_matrix_validation(anyarray) bool It validates whether the supplied array ( anyarray ) is adequate for the array-based matrix. It is intended to use for sanity check prior to invocation of PL/CUDA function, or check constraint on domain type definition. array_matrix_height(array) int array is an array of either smallint , int , bigint , real or float data. This function returns the height of the supplied matrix. array_matrix_width(array) int array is an array of either smallint , int , bigint , real or float data. This function returns the width of the supplied matrix.","title":"Array-Matrix Functions"},{"location":"ref_devfuncs/","text":"Functions and operators This chapter introduces the functions and operators executable on GPU devices. Type cast destination type source type description bool int4 int2 int4,int8,float2,float4,float8,numeric int4 int2,int8,float2,float4,float8,numeric int8 int2,int4,float2,float4,float8,numeric float2 int2,int4,int8,float4,float8,numeric float4 int2,int4,int8,float2,float8,numeric float8 int2,int4,int8,float2,float4,numeric numeric int2,int4,int8,float2,float4,float8 money int4,int8,numeric inet cidr date timestamp,timestamptz time timetz,timestamp,timestamptz timetz time,timestamptz timestamp date,timestamptz timestamptz date,timestamp Numeric functions/operators function/operator description TYPE COMP TYPE Comparison of two values TYPE is any of int2,int4,int8 COMP is any of =,<>,<,<=,>=,> TYPE COMP TYPE Comparison of two values TYPE is any of float2,float4,float8 COMP is any of =,<>,<,<=,>=,> numeric COMP numeric Comparison of two values COMP is any of =,<>,<,<=,>=,> TYPE + TYPE Arithemetic addition TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE - TYPE Arithemetic substract TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE * TYPE Arithemetic multiplication TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE / TYPE Arithemetic division TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE % TYPE Reminer operator TYPE is any of int2,int4,int8 TYPE & TYPE Bitwise AND TYPE is any of int2,int4,int8 TYPE | TYPE Bitwise OR TYPE is any of int2,int4,int8 TYPE # TYPE Bitwise XOR TYPE is any of int2,int4,int8 ~ TYPE Bitwise NOT TYPE is any if int2,int4,int8 TYPE >> int4 Right shift TYPE is any of int2,int4,int8 TYPE << int4 Left shift TYPE is any of int2,int4,int8 + TYPE Unary plus TYPE is any of int2,int4,int8,float2,float4,float8,numeric - TYPE Unary minus TYPE is any of int2,int4,int8,float2,float4,float8,numeric @TYPE Absolute value TYPE is any of int2,int4,int8,float2,float4,float8,numeric Mathematical functions functions/operators description cbrt(float8) cube root dcbrt(float8) cube root ceil(float8) nearest integer greater than or equal to argument ceiling(float8) nearest integer greater than or equal to argument exp(float8) exponential dexp(float8) exponential floor(float8) nearest integer less than or equal to argument ln(float8) natural logarithm dlog1(float8) natural logarithm log(float8) base 10 logarithm dlog10(float8) base 10 logarithm pi() circumference ratio power(float8,float8) power pow(float8,float8) power dpow(float8,float8) power round(float8) round to the nearest integer dround(float8) round to the nearest integer sign(float8) sign of the argument sqrt(float8) square root dsqrt(float8) square root trunc(float8) truncate toward zero dtrunc(float8) truncate toward zero Trigonometric functions functions/operators description degrees(float8) radians to degrees radians(float8) degrees to radians acos(float8) inverse cosine asin(float8) inverse sine atan(float8) inverse tangent atan2(float8,float8) inverse tangent of arg1 / arg2 cos(float8) cosine cot(float8) cotangent sin(float8) sine tan(float8) tangent Date and time operators functions/operators description date COMP date COMP is any of =,<>,<,<=,>=,> date COMP timestamp COMP is any of =,<>,<,<=,>=,> date COMP timestamptz COMP is any of =,<>,<,<=,>=,> time COMP time COMP is any of =,<>,<,<=,>=,> timetz COMP timetz COMP is any of =,<>,<,<=,>=,> timestamp COMP timestamp COMP is any of =,<>,<,<=,>=,> timestamp COMP date COMP is any of =,<>,<,<=,>=,> timestamptz COMP timestamptz COMP is any of =,<>,<,<=,>=,> timestamptz COMP date COMP is any of =,<>,<,<=,>=,> timestamp COMP timestamptz COMP is any of =,<>,<,<=,>=,> timestamptz COMP timestamp COMP is any of =,<>,<,<=,>=,> interval COMP interval COMP is any of =,<>,<,<=,>=,> date OP int4 OP is either of +,- int4 + date date - date date + time date + timetz time + date time - time timestamp - timestamp timetz OP interval OP is either of +,- timestamptz OP interval OP is either of +,- overlaps(TYPE,TYPE,TYPE,TYPE) TYPE is any of time,timetz,timestamp,timestamptz extract(text FROM TYPE) TYPE is any of time,timetz,timestamp,timestamptz,interval now() - interval unary minus operator interval OP interval OP is either of +,- Text functions/operators functions/operators description {text,bpchar} COMP {text,bpchar} COMP is either of =,<> {text,bpchar} COMP {text,bpchar} COMP is either of <,<=,>=,> Only available on no-locale or UTF-8 varchar || varchar Both side must be varchar(n) with maximum length. substring , substr length(TYPE) length of the string TYPE is either of text,bpchar TYPE LIKE text TYPE is either of text,bpchar TYPE NOT LIKE text TYPE is either of text,bpchar TYPE ILIKE text TYPE is either of text,bpchar Only available on no-locale or UTF-8 TYPE NOT ILIKE text TYPE is either of text,bpchar Only available on no-locale or UTF-8 Network functions/operators functions/operators description macaddr COMP macaddr COMP is any of =,<>,<,<=,>=,> macaddr & macaddr Bitwise AND operator macaddr | macaddr Bitwise OR operator ~ macaddr Bitwise NOT operator trunc(macaddr) Set last 3 bytes to zero inet COMP inet COMP is any of =,<>,<,<=,>=,> inet INCL inet INCL is any of <<,<<=,>>,>>=,&& ~ inet inet & inet inet | inet inet + int8 inet - int8 inet - inet broadcast(inet) family(inet) hostmask(inet) masklen(inet) netmask(inet) network(inet) set_masklen(cidr,int) set_masklen(inet,int) inet_same_family(inet, inet) inet_merge(inet,inet) Currency operators functions/operators description money COMP money COMP is any of =,<>,<,<=,>=,> money OP money OP is any of +,-,/ money * TYPE TYPE is any of int2,int4,float2,float4,float8 TYPE * money TYPE is any of int2,int4,float2,float4,float8 money / TYPE TYPE is any of int2,int4,float2,float4,float8 UUID operators functions/operators description uuid COMP uuid COMP is any of =,<>,<,<=,>=,> JSONB operators functions/operators description jsonb -> KEY Get a JSON object field specified by the KEY jsonb -> NUM Get a JSON array element indexed by NUM jsonb ->> KEY Get a JSON object field specified by the KEY , as text jsonb ->> NUM Get a JSON array element indexed by NUM (jsonb ->> KEY)::TYPE TYPE is any of int2,int4,int8,float4,float8,numeric Get a JSON object field specified by KEY , as numeric data type. See the note below. (jsonb ->> NUM)::TYPE TYPE is any of int2,int4,int8,float4,float8,numeric Get a JSON array element indexed by NUM , as numeric data type. See the note below. jsonb ? KEY Check whether jsonb object contains the KEY Note When we convert a jsonb element fetched by jsonb ->> KEY operator into numerical data types like float or numeric , PostgreSQL takes 2 steps operations; an internal numerical form is printed as text first, then it is converted into numerical data type. PG-Strom optimizes the GPU code using a special device function to fetch a numerical datum from jsonb object/array, if jsonb ->> KEY operator and text-to-numeric case are continuously used. Range type functions/operators functions/operators description RANGE = RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE <> RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE < RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE <= RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE > RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE >= RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE @ RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE @ TYPE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange TYPE is element type of RANGE . RANGE <@RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange TYPE <@RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange TYPE is element type of RANGE . RANGE && RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE << RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE >> RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE &< RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE &> RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE -|- RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE + RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE * RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE - RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange lower(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange upper(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange isempty(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange lower_inc(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange upper_inc(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange lower_inf(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange upper_inf(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange range_merge(RANGE,RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange Miscellaneous device functions functions/operators result description as_int8(float8) int8 Re-interpret double-precision floating point bit-pattern as 64bit integer value as_int4(float4) int4 Re-interpret single-precision floating point bit-pattern as 32bit integer value as_int2(float2) int2 Re-interpret half-precision floating point bit-pattern as 16bit integer value as_float8(int8) float8 Re-interpret 64bit integer bit-pattern as double-precision floating point value as_float4(int4) float4 Re-interpret 32bit integer bit-pattern as single-precision floating point value as_float2(int2) float2 Re-interpret 16bit integer bit-pattern as half-precision floating point value","title":"Functions and Operators"},{"location":"ref_devfuncs/#type-cast","text":"destination type source type description bool int4 int2 int4,int8,float2,float4,float8,numeric int4 int2,int8,float2,float4,float8,numeric int8 int2,int4,float2,float4,float8,numeric float2 int2,int4,int8,float4,float8,numeric float4 int2,int4,int8,float2,float8,numeric float8 int2,int4,int8,float2,float4,numeric numeric int2,int4,int8,float2,float4,float8 money int4,int8,numeric inet cidr date timestamp,timestamptz time timetz,timestamp,timestamptz timetz time,timestamptz timestamp date,timestamptz timestamptz date,timestamp","title":"Type cast"},{"location":"ref_devfuncs/#numeric-functionsoperators","text":"function/operator description TYPE COMP TYPE Comparison of two values TYPE is any of int2,int4,int8 COMP is any of =,<>,<,<=,>=,> TYPE COMP TYPE Comparison of two values TYPE is any of float2,float4,float8 COMP is any of =,<>,<,<=,>=,> numeric COMP numeric Comparison of two values COMP is any of =,<>,<,<=,>=,> TYPE + TYPE Arithemetic addition TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE - TYPE Arithemetic substract TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE * TYPE Arithemetic multiplication TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE / TYPE Arithemetic division TYPE is any of int2,int4,int8,float2,float4,float8,numeric TYPE % TYPE Reminer operator TYPE is any of int2,int4,int8 TYPE & TYPE Bitwise AND TYPE is any of int2,int4,int8 TYPE | TYPE Bitwise OR TYPE is any of int2,int4,int8 TYPE # TYPE Bitwise XOR TYPE is any of int2,int4,int8 ~ TYPE Bitwise NOT TYPE is any if int2,int4,int8 TYPE >> int4 Right shift TYPE is any of int2,int4,int8 TYPE << int4 Left shift TYPE is any of int2,int4,int8 + TYPE Unary plus TYPE is any of int2,int4,int8,float2,float4,float8,numeric - TYPE Unary minus TYPE is any of int2,int4,int8,float2,float4,float8,numeric @TYPE Absolute value TYPE is any of int2,int4,int8,float2,float4,float8,numeric","title":"Numeric functions/operators"},{"location":"ref_devfuncs/#mathematical-functions","text":"functions/operators description cbrt(float8) cube root dcbrt(float8) cube root ceil(float8) nearest integer greater than or equal to argument ceiling(float8) nearest integer greater than or equal to argument exp(float8) exponential dexp(float8) exponential floor(float8) nearest integer less than or equal to argument ln(float8) natural logarithm dlog1(float8) natural logarithm log(float8) base 10 logarithm dlog10(float8) base 10 logarithm pi() circumference ratio power(float8,float8) power pow(float8,float8) power dpow(float8,float8) power round(float8) round to the nearest integer dround(float8) round to the nearest integer sign(float8) sign of the argument sqrt(float8) square root dsqrt(float8) square root trunc(float8) truncate toward zero dtrunc(float8) truncate toward zero","title":"Mathematical functions"},{"location":"ref_devfuncs/#trigonometric-functions","text":"functions/operators description degrees(float8) radians to degrees radians(float8) degrees to radians acos(float8) inverse cosine asin(float8) inverse sine atan(float8) inverse tangent atan2(float8,float8) inverse tangent of arg1 / arg2 cos(float8) cosine cot(float8) cotangent sin(float8) sine tan(float8) tangent","title":"Trigonometric functions"},{"location":"ref_devfuncs/#date-and-time-operators","text":"functions/operators description date COMP date COMP is any of =,<>,<,<=,>=,> date COMP timestamp COMP is any of =,<>,<,<=,>=,> date COMP timestamptz COMP is any of =,<>,<,<=,>=,> time COMP time COMP is any of =,<>,<,<=,>=,> timetz COMP timetz COMP is any of =,<>,<,<=,>=,> timestamp COMP timestamp COMP is any of =,<>,<,<=,>=,> timestamp COMP date COMP is any of =,<>,<,<=,>=,> timestamptz COMP timestamptz COMP is any of =,<>,<,<=,>=,> timestamptz COMP date COMP is any of =,<>,<,<=,>=,> timestamp COMP timestamptz COMP is any of =,<>,<,<=,>=,> timestamptz COMP timestamp COMP is any of =,<>,<,<=,>=,> interval COMP interval COMP is any of =,<>,<,<=,>=,> date OP int4 OP is either of +,- int4 + date date - date date + time date + timetz time + date time - time timestamp - timestamp timetz OP interval OP is either of +,- timestamptz OP interval OP is either of +,- overlaps(TYPE,TYPE,TYPE,TYPE) TYPE is any of time,timetz,timestamp,timestamptz extract(text FROM TYPE) TYPE is any of time,timetz,timestamp,timestamptz,interval now() - interval unary minus operator interval OP interval OP is either of +,-","title":"Date and time operators"},{"location":"ref_devfuncs/#text-functionsoperators","text":"functions/operators description {text,bpchar} COMP {text,bpchar} COMP is either of =,<> {text,bpchar} COMP {text,bpchar} COMP is either of <,<=,>=,> Only available on no-locale or UTF-8 varchar || varchar Both side must be varchar(n) with maximum length. substring , substr length(TYPE) length of the string TYPE is either of text,bpchar TYPE LIKE text TYPE is either of text,bpchar TYPE NOT LIKE text TYPE is either of text,bpchar TYPE ILIKE text TYPE is either of text,bpchar Only available on no-locale or UTF-8 TYPE NOT ILIKE text TYPE is either of text,bpchar Only available on no-locale or UTF-8","title":"Text functions/operators"},{"location":"ref_devfuncs/#network-functionsoperators","text":"functions/operators description macaddr COMP macaddr COMP is any of =,<>,<,<=,>=,> macaddr & macaddr Bitwise AND operator macaddr | macaddr Bitwise OR operator ~ macaddr Bitwise NOT operator trunc(macaddr) Set last 3 bytes to zero inet COMP inet COMP is any of =,<>,<,<=,>=,> inet INCL inet INCL is any of <<,<<=,>>,>>=,&& ~ inet inet & inet inet | inet inet + int8 inet - int8 inet - inet broadcast(inet) family(inet) hostmask(inet) masklen(inet) netmask(inet) network(inet) set_masklen(cidr,int) set_masklen(inet,int) inet_same_family(inet, inet) inet_merge(inet,inet)","title":"Network functions/operators"},{"location":"ref_devfuncs/#currency-operators","text":"functions/operators description money COMP money COMP is any of =,<>,<,<=,>=,> money OP money OP is any of +,-,/ money * TYPE TYPE is any of int2,int4,float2,float4,float8 TYPE * money TYPE is any of int2,int4,float2,float4,float8 money / TYPE TYPE is any of int2,int4,float2,float4,float8","title":"Currency operators"},{"location":"ref_devfuncs/#uuid-operators","text":"functions/operators description uuid COMP uuid COMP is any of =,<>,<,<=,>=,>","title":"UUID operators"},{"location":"ref_devfuncs/#jsonb-operators","text":"functions/operators description jsonb -> KEY Get a JSON object field specified by the KEY jsonb -> NUM Get a JSON array element indexed by NUM jsonb ->> KEY Get a JSON object field specified by the KEY , as text jsonb ->> NUM Get a JSON array element indexed by NUM (jsonb ->> KEY)::TYPE TYPE is any of int2,int4,int8,float4,float8,numeric Get a JSON object field specified by KEY , as numeric data type. See the note below. (jsonb ->> NUM)::TYPE TYPE is any of int2,int4,int8,float4,float8,numeric Get a JSON array element indexed by NUM , as numeric data type. See the note below. jsonb ? KEY Check whether jsonb object contains the KEY Note When we convert a jsonb element fetched by jsonb ->> KEY operator into numerical data types like float or numeric , PostgreSQL takes 2 steps operations; an internal numerical form is printed as text first, then it is converted into numerical data type. PG-Strom optimizes the GPU code using a special device function to fetch a numerical datum from jsonb object/array, if jsonb ->> KEY operator and text-to-numeric case are continuously used.","title":"JSONB operators"},{"location":"ref_devfuncs/#range-type-functionsoperators","text":"functions/operators description RANGE = RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE <> RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE < RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE <= RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE > RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE >= RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE @ RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE @ TYPE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange TYPE is element type of RANGE . RANGE <@RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange TYPE <@RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange TYPE is element type of RANGE . RANGE && RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE << RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE >> RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE &< RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE &> RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE -|- RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE + RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE * RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange RANGE - RANGE RANGE is any of int4range,int8range,tsrange,tstzrange,daterange lower(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange upper(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange isempty(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange lower_inc(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange upper_inc(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange lower_inf(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange upper_inf(RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange range_merge(RANGE,RANGE) RANGE is any of int4range,int8range,tsrange,tstzrange,daterange","title":"Range type functions/operators"},{"location":"ref_devfuncs/#miscellaneous-device-functions","text":"functions/operators result description as_int8(float8) int8 Re-interpret double-precision floating point bit-pattern as 64bit integer value as_int4(float4) int4 Re-interpret single-precision floating point bit-pattern as 32bit integer value as_int2(float2) int2 Re-interpret half-precision floating point bit-pattern as 16bit integer value as_float8(int8) float8 Re-interpret 64bit integer bit-pattern as double-precision floating point value as_float4(int4) float4 Re-interpret 32bit integer bit-pattern as single-precision floating point value as_float2(int2) float2 Re-interpret 16bit integer bit-pattern as half-precision floating point value","title":"Miscellaneous device functions"},{"location":"ref_params/","text":"GUC Parameters This session introduces PG-Strom's configuration parameters. Enables/disables a particular feature Parameter Type Default Description pg_strom.enabled bool on Enables/disables entire PG-Strom features at once pg_strom.enable_gpuscan bool on Enables/disables GpuScan pg_strom.enable_gpuhashjoin bool on Enables/disables GpuJoin by HashJoin pg_strom.enable_gpunestloop bool on Enables/disables GpuJoin by NestLoop pg_strom.enable_gpupreagg bool on Enables/disables GpuPreAgg pg_strom.enable_partitionwise_gpujoin bool on Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.enable_partitionwise_gpupreagg bool on Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.pullup_outer_scan bool on Enables/disables to pull up full-table scan if it is just below GpuPreAgg/GpuJoin, to reduce data transfer between CPU/RAM and GPU. pg_strom.pullup_outer_join bool on Enables/disables to pull up tables-join if GpuJoin is just below GpuPreAgg, to reduce data transfer between CPU/RAM and GPU. pg_strom.enable_numeric_aggfuncs bool on Enables/disables support of aggregate function that takes numeric data type. pg_strom.cpu_fallback bool off Controls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\" Optimizer Configuration Parameter Type Default Description pg_strom.chunk_size int 65534kB Size of the data blocks processed by a single GPU kernel invocation. It was configurable, but makes less sense, so fixed to about 64MB in the current version. pg_strom.gpu_setup_cost real 4000 Cost value for initialization of GPU device pg_strom.gpu_dma_cost real 10 Cost value for DMA transfer over PCIe bus per data-chunk (64MB) pg_strom.gpu_operator_cost real 0.00015 Cost value to process an expression formula on GPU. If larger value than cpu_operator_cost is configured, no chance to choose PG-Strom towards any size of tables Executor Configuration Parameter Type Default Description pg_strom.global_max_async_tasks int 160 Number of asynchronous taks PG-Strom can throw into GPU's execution queue in the whole system. pg_strom.local_max_async_tasks int 8 Number of asynchronous taks PG-Strom can throw into GPU's execution queue per process. If CPU parallel is used in combination, this limitation shall be applied for each background worker. So, more than pg_strom.local_max_async_tasks asynchronous tasks are executed in parallel on the entire batch job. pg_strom.max_number_of_gpucontext int auto Specifies the number of internal data structure GpuContext to abstract GPU device. Usually, no need to expand the initial value. SSD-to-GPU Direct Configuration Parameter Type Default Description pg_strom.nvme_strom_enabled bool on Enables/disables SSD-to-GPU Direct SQL mechanism pg_strom.nvme_strom_threshold int auto Controls the table-size threshold to invoke SSD-to-GPU Direct SQL mechanism pg_strom.nvme_distance_map string NULL Manually configures the closest GPU for each NVME-SSD. Usually, it is configured automatically according to the PCIe bus topology information by sysfs. Arrow_Fdw Configuration Parameter Type Default Description arrow_fdw.enabled bool on By adjustment of estimated cost value, it turns on/off Arrow_Fdw. Note that only Foreign Scan (Arrow_Fdw) can scan on Arrow files, if GpuScan is not capable to run on. arrow_fdw.metadata_cache_size int 32MB Size of shared memory to cache metadata of Arrow files. It needs to restart to update the parameter. arrow_fdw.metadata_cache_width int 80 Max number of columns for each metadata entry. Arrow_Fdw does not cache metadata if Arrow file has more columns than this configuration, so it loads metadata from the files on behalf of the foreign table for each references. It needs to restart to update the parameter. gstore_fdw Configuration Parameter Type Default Description pg_strom.gstore_max_relations int 100 Upper limit of the number of foreign tables with gstore_fdw. It needs restart to update the parameter. Configuration of GPU code generation and build Parameter Type Default Description pg_strom.program_cache_size int 256MB Amount of the shared memory size to cache GPU programs already built. It needs restart to update the parameter. pg_strom.num_program_builders int 2 Number of background workers to build GPU programs asynchronously. It needs restart to update the parameter. pg_strom.debug_jit_compile_options bool off Controls to include debug option (line-numbers and symbol information) on JIT compile of GPU programs. It is valuable for complicated bug analysis using GPU core dump, however, should not be enabled on daily use because of performance degradation. pg_strom.debug_kernel_source bool off If enables, EXPLAIN VERBOSE command also prints out file paths of GPU programs written out. GPU Device Configuration Parameter Type Default Description pg_strom.cuda_visible_devices string '' List of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable CUDAVISIBLE_DEVICES pg_strom.gpu_memory_segment_size int 512MB Specifies the amount of device memory to be allocated per CUDA API call. Larger configuration will reduce the overhead of API calls, but not efficient usage of device memory. pg_strom.max_num_preserved_gpu_memory int 2048 Upper limit of the number of preserved GPU device memory segment. Usually, don't need to change from the default value.","title":"GUC Parameters"},{"location":"ref_params/#enablesdisables-a-particular-feature","text":"Parameter Type Default Description pg_strom.enabled bool on Enables/disables entire PG-Strom features at once pg_strom.enable_gpuscan bool on Enables/disables GpuScan pg_strom.enable_gpuhashjoin bool on Enables/disables GpuJoin by HashJoin pg_strom.enable_gpunestloop bool on Enables/disables GpuJoin by NestLoop pg_strom.enable_gpupreagg bool on Enables/disables GpuPreAgg pg_strom.enable_partitionwise_gpujoin bool on Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.enable_partitionwise_gpupreagg bool on Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.pullup_outer_scan bool on Enables/disables to pull up full-table scan if it is just below GpuPreAgg/GpuJoin, to reduce data transfer between CPU/RAM and GPU. pg_strom.pullup_outer_join bool on Enables/disables to pull up tables-join if GpuJoin is just below GpuPreAgg, to reduce data transfer between CPU/RAM and GPU. pg_strom.enable_numeric_aggfuncs bool on Enables/disables support of aggregate function that takes numeric data type. pg_strom.cpu_fallback bool off Controls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\"","title":"Enables/disables a particular feature"},{"location":"ref_params/#optimizer-configuration","text":"Parameter Type Default Description pg_strom.chunk_size int 65534kB Size of the data blocks processed by a single GPU kernel invocation. It was configurable, but makes less sense, so fixed to about 64MB in the current version. pg_strom.gpu_setup_cost real 4000 Cost value for initialization of GPU device pg_strom.gpu_dma_cost real 10 Cost value for DMA transfer over PCIe bus per data-chunk (64MB) pg_strom.gpu_operator_cost real 0.00015 Cost value to process an expression formula on GPU. If larger value than cpu_operator_cost is configured, no chance to choose PG-Strom towards any size of tables","title":"Optimizer Configuration"},{"location":"ref_params/#executor-configuration","text":"Parameter Type Default Description pg_strom.global_max_async_tasks int 160 Number of asynchronous taks PG-Strom can throw into GPU's execution queue in the whole system. pg_strom.local_max_async_tasks int 8 Number of asynchronous taks PG-Strom can throw into GPU's execution queue per process. If CPU parallel is used in combination, this limitation shall be applied for each background worker. So, more than pg_strom.local_max_async_tasks asynchronous tasks are executed in parallel on the entire batch job. pg_strom.max_number_of_gpucontext int auto Specifies the number of internal data structure GpuContext to abstract GPU device. Usually, no need to expand the initial value.","title":"Executor Configuration"},{"location":"ref_params/#ssd-to-gpu-direct-configuration","text":"Parameter Type Default Description pg_strom.nvme_strom_enabled bool on Enables/disables SSD-to-GPU Direct SQL mechanism pg_strom.nvme_strom_threshold int auto Controls the table-size threshold to invoke SSD-to-GPU Direct SQL mechanism pg_strom.nvme_distance_map string NULL Manually configures the closest GPU for each NVME-SSD. Usually, it is configured automatically according to the PCIe bus topology information by sysfs.","title":"SSD-to-GPU Direct Configuration"},{"location":"ref_params/#arrow_fdw-configuration","text":"Parameter Type Default Description arrow_fdw.enabled bool on By adjustment of estimated cost value, it turns on/off Arrow_Fdw. Note that only Foreign Scan (Arrow_Fdw) can scan on Arrow files, if GpuScan is not capable to run on. arrow_fdw.metadata_cache_size int 32MB Size of shared memory to cache metadata of Arrow files. It needs to restart to update the parameter. arrow_fdw.metadata_cache_width int 80 Max number of columns for each metadata entry. Arrow_Fdw does not cache metadata if Arrow file has more columns than this configuration, so it loads metadata from the files on behalf of the foreign table for each references. It needs to restart to update the parameter.","title":"Arrow_Fdw Configuration"},{"location":"ref_params/#gstore_fdw-configuration","text":"Parameter Type Default Description pg_strom.gstore_max_relations int 100 Upper limit of the number of foreign tables with gstore_fdw. It needs restart to update the parameter.","title":"gstore_fdw Configuration"},{"location":"ref_params/#configuration-of-gpu-code-generation-and-build","text":"Parameter Type Default Description pg_strom.program_cache_size int 256MB Amount of the shared memory size to cache GPU programs already built. It needs restart to update the parameter. pg_strom.num_program_builders int 2 Number of background workers to build GPU programs asynchronously. It needs restart to update the parameter. pg_strom.debug_jit_compile_options bool off Controls to include debug option (line-numbers and symbol information) on JIT compile of GPU programs. It is valuable for complicated bug analysis using GPU core dump, however, should not be enabled on daily use because of performance degradation. pg_strom.debug_kernel_source bool off If enables, EXPLAIN VERBOSE command also prints out file paths of GPU programs written out.","title":"Configuration of GPU code generation and build"},{"location":"ref_params/#gpu-device-configuration","text":"Parameter Type Default Description pg_strom.cuda_visible_devices string '' List of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable CUDAVISIBLE_DEVICES pg_strom.gpu_memory_segment_size int 512MB Specifies the amount of device memory to be allocated per CUDA API call. Larger configuration will reduce the overhead of API calls, but not efficient usage of device memory. pg_strom.max_num_preserved_gpu_memory int 2048 Upper limit of the number of preserved GPU device memory segment. Usually, don't need to change from the default value.","title":"GPU Device Configuration"},{"location":"ref_sqlfuncs/","text":"SQL Objects This chapter introduces SQL objects additionally provided by PG-Strom. Device Information Function Result Description gpu_device_name(int = 0) text It tells name of the specified GPU device. gpu_global_memsize(int = 0) bigint It tells amount of the specified GPU device in bytes. gpu_max_blocksize(int = 0) int It tells maximum block-size on the specified GPU device. 1024, in the currently supported GPU models. gpu_warp_size(int = 0) int It tells warp-size on the specified GPU device. 32, in the currently supported GPU models. gpu_max_shared_memory_perblock(int = 0) int It tells maximum shared memory size per block on the specified GPU device. gpu_num_registers_perblock(int = 0) int It tells total number of registers per block on the specified GPU device. gpu_num_multiptocessors(int = 0) int It tells number of SM(Streaming Multiprocessor) units on the specified GPU device. gpu_num_cuda_cores(int = 0) int It tells number of CUDA cores on the specified GPU device. gpu_cc_major(int = 0) int It tells major CC(Compute Capability) version of the specified GPU device. gpu_cc_minor(int = 0) int It tells minor CC(Compute Capability) version of the specified GPU device. gpu_pci_id(int = 0) int It tells PCI bus-id of the specified GPU device. Array-based matrix support You can use array data type of PostgreSQL to deliver matrix-data for PL/CUDA functions. A two-dimensional array of fixed-length boolean/numeric values without NULL has flat data structure (expect for the array header). It allows to identify the address of elements by indexes of the matrix uniquely. PG-Strom provides several SQL functions to handle array-based matrix. Type cast destination type source type description int[] bit convert bit-string to 32bit integer array. Unaligned bits are filled up by 0. bit int[] convert 32bit integer to bit-string Array-based matrix functions functions/operators result description array_matrix_validation(anyarray) bool It checks whether the supplied array satisfies the requirement of array-based matrix. array_matrix_height(anyarray) int It tells height of the array-based matrix. array_matrix_width(anyarray) int It tells width of the array-based matrix. array_vector_rawsize(regtype,int) bigint It tells expected size if N-items vector is created with the specified type. array_matrix_rawsize(regtype,int,int) bigint It tells expected size if HxW matrix is created with the specified type. array_cube_rawsize(regtype,int,int,int) bigint It tells expected size if HxWxD cube is created with the specified type. type_len(regtype) bigint It tells unit length of the specified type. composite_type_rawsize(LEN,...) bigint It tells expected size of the composite type if constructed with the specified data-length order. We expect to use the function with type_len() LEN is either of int,bigint matrix_unnest(anyarray) record It is a function to return set, to fetch rows from top of the supplied array-based matrix. PostgreSQL has no type information of the record, so needs to give type information using ROW() clause. rbind(MATRIX,MATRIX) MATRIX It combines two array-based matrix vertically. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 rbind(TYPE,MATRIX) MATRIX It adds a scalar value on head of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the head row. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 . TYPE is element of MATRIX rbind(MATRIX,TYPE) MATRIX It adds a scalar value on bottom of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the bottom row. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 TYPE is element type of MATRIX cbind(MATRIX,MATRIX) MATRIX It combines two array-based matrix horizontally. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 cbind(TYPE,MATRIX) MATRIX It adds a scalar value on left of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the left column. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 TYPE is element type of MATRIX cbind(MATRIX,TYPE) MATRIX It adds a scalar value on right of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the right column. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 . TYPE is element type of MATRIX transpose(MATRIX) MATRIX It transposes the array-based matrix. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 Aggregate functions functions/operators result description array_matrix(TYPE,...) TYPE[] An aggregate function with varidic arguments. It produces M-cols x N-rows array-based matrix if N-rows were supplied with M-columns. TYPE is any of bool,int2,int4,int8,float4,float8 array_matrix(bit) bit[] An aggregate function to produce int4[] array-based matrix. It considers bit-string as a set of 32bits integer values. rbind(MATRIX) MATRIX An aggregate function to combine the supplied array-based matrix vertically. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 cbind(MATRIX) MATRIX An aggregate function to combine the supplied array-based matrix horizontally. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 Miscellaneous functions Function Result Description gstore_fdw_format(reggstore) text It tells internal format of the specified gstore_fdw foreign table. gstore_fdw_nitems(reggstore) bigint It tells number of rows of the specified gstore_fdw foreign table. gstore_fdw_nattrs(reggstore) bigint It tells number of columns of the specified gstore_fdw foreign table. gstore_fdw_rawsize(reggstore) bigint It tells raw size of the specified gstore_fdw foreign table in bytes. Function Result Description gstore_export_ipchandle(reggstore) bytea It tells IPC-handle of the GPU device memory region of the specified gstore_fdw foreign table. lo_import_gpu(int, bytea, bigint, bigint, oid=0) oid It maps GPU device memory region acquired by external application, then import its contents into a largeobject. lo_export_gpu(oid, int, bytea, bigint, bigint) bigint It maps GPU device memory region acquired by external application, then export contents of the specified largeobject into the region. Definition Result Description attnums_of(regclass,text[]) smallint[] It returns attribute numbers for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. attnum_of(regclass,text) smallint It returns attribute number for the column name of the 2nd argument on the table of the 1st argument. atttypes_of(regclass,text[]) regtype[] It returns data types for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. atttype_of(regclass,text) regtype It returns data type for the column name of the 2nd argument on the table of the 1st argument. attrs_types_check(regclass,text[],regtype[]) bool It checks whether the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data types of the 3rd argument for each. attrs_type_check(regclass,text[],regtype) bool It checks whether all the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data type of the 3rd argument. Function Result Description pgstrom.license_validation() text It validates commercial subscription. pgstrom.license_query() text It shows the active commercial subscription. System View PG-Strom provides several system view to export its internal state for users or applications. The future version may add extra fields here. So, it is not recommended to reference these information schemas using SELECT * FROM ... . pgstrom.device_info pgstrom.device_into system view exports device attributes of the GPUs recognized by PG-Strom. GPU has different specification for each model, like number of cores, capacity of global memory, maximum number of threads and etc, user's software should be optimized according to the information if you try raw GPU programming with PL/CUDA functions. Name Data Type Description device_nr int GPU device number aindex int Attribute index attribute text Attribute name value text Value of the attribute pgstrom.device_preserved_meminfo pgstrom.device_preserved_meminfo system view exports information of the preserved device memory; which can be shared multiple PostgreSQL backend. Right now, only gstore_fdw uses this feature. Name Data Type Description device_nr int GPU device number handle bytea IPC handle of the preserved device memory owner regrole Owner of the preserved device memory length bigint Length of the preserved device memory in bytes ctime timestamp with time zone Timestamp when the preserved device memory is created","title":"SQL Objects"},{"location":"ref_sqlfuncs/#device-information","text":"Function Result Description gpu_device_name(int = 0) text It tells name of the specified GPU device. gpu_global_memsize(int = 0) bigint It tells amount of the specified GPU device in bytes. gpu_max_blocksize(int = 0) int It tells maximum block-size on the specified GPU device. 1024, in the currently supported GPU models. gpu_warp_size(int = 0) int It tells warp-size on the specified GPU device. 32, in the currently supported GPU models. gpu_max_shared_memory_perblock(int = 0) int It tells maximum shared memory size per block on the specified GPU device. gpu_num_registers_perblock(int = 0) int It tells total number of registers per block on the specified GPU device. gpu_num_multiptocessors(int = 0) int It tells number of SM(Streaming Multiprocessor) units on the specified GPU device. gpu_num_cuda_cores(int = 0) int It tells number of CUDA cores on the specified GPU device. gpu_cc_major(int = 0) int It tells major CC(Compute Capability) version of the specified GPU device. gpu_cc_minor(int = 0) int It tells minor CC(Compute Capability) version of the specified GPU device. gpu_pci_id(int = 0) int It tells PCI bus-id of the specified GPU device.","title":"Device Information"},{"location":"ref_sqlfuncs/#array-based-matrix-support","text":"You can use array data type of PostgreSQL to deliver matrix-data for PL/CUDA functions. A two-dimensional array of fixed-length boolean/numeric values without NULL has flat data structure (expect for the array header). It allows to identify the address of elements by indexes of the matrix uniquely. PG-Strom provides several SQL functions to handle array-based matrix.","title":"Array-based matrix support"},{"location":"ref_sqlfuncs/#type-cast","text":"destination type source type description int[] bit convert bit-string to 32bit integer array. Unaligned bits are filled up by 0. bit int[] convert 32bit integer to bit-string","title":"Type cast"},{"location":"ref_sqlfuncs/#array-based-matrix-functions","text":"functions/operators result description array_matrix_validation(anyarray) bool It checks whether the supplied array satisfies the requirement of array-based matrix. array_matrix_height(anyarray) int It tells height of the array-based matrix. array_matrix_width(anyarray) int It tells width of the array-based matrix. array_vector_rawsize(regtype,int) bigint It tells expected size if N-items vector is created with the specified type. array_matrix_rawsize(regtype,int,int) bigint It tells expected size if HxW matrix is created with the specified type. array_cube_rawsize(regtype,int,int,int) bigint It tells expected size if HxWxD cube is created with the specified type. type_len(regtype) bigint It tells unit length of the specified type. composite_type_rawsize(LEN,...) bigint It tells expected size of the composite type if constructed with the specified data-length order. We expect to use the function with type_len() LEN is either of int,bigint matrix_unnest(anyarray) record It is a function to return set, to fetch rows from top of the supplied array-based matrix. PostgreSQL has no type information of the record, so needs to give type information using ROW() clause. rbind(MATRIX,MATRIX) MATRIX It combines two array-based matrix vertically. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 rbind(TYPE,MATRIX) MATRIX It adds a scalar value on head of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the head row. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 . TYPE is element of MATRIX rbind(MATRIX,TYPE) MATRIX It adds a scalar value on bottom of the array-based matrix. If multiple columns exist, the scalar value shall be set on all the column of the bottom row. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 TYPE is element type of MATRIX cbind(MATRIX,MATRIX) MATRIX It combines two array-based matrix horizontally. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 cbind(TYPE,MATRIX) MATRIX It adds a scalar value on left of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the left column. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 TYPE is element type of MATRIX cbind(MATRIX,TYPE) MATRIX It adds a scalar value on right of the array-based matrix. If multiple rows exist, the scalar value shall be set on all the rows of the right column. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 . TYPE is element type of MATRIX transpose(MATRIX) MATRIX It transposes the array-based matrix. MATRIX is array type of any of bool,int2,int4,int8,float4,float8","title":"Array-based matrix functions"},{"location":"ref_sqlfuncs/#aggregate-functions","text":"functions/operators result description array_matrix(TYPE,...) TYPE[] An aggregate function with varidic arguments. It produces M-cols x N-rows array-based matrix if N-rows were supplied with M-columns. TYPE is any of bool,int2,int4,int8,float4,float8 array_matrix(bit) bit[] An aggregate function to produce int4[] array-based matrix. It considers bit-string as a set of 32bits integer values. rbind(MATRIX) MATRIX An aggregate function to combine the supplied array-based matrix vertically. MATRIX is array type of any of bool,int2,int4,int8,float4,float8 cbind(MATRIX) MATRIX An aggregate function to combine the supplied array-based matrix horizontally. MATRIX is array type of any of bool,int2,int4,int8,float4,float8","title":"Aggregate functions"},{"location":"ref_sqlfuncs/#miscellaneous-functions","text":"Function Result Description gstore_fdw_format(reggstore) text It tells internal format of the specified gstore_fdw foreign table. gstore_fdw_nitems(reggstore) bigint It tells number of rows of the specified gstore_fdw foreign table. gstore_fdw_nattrs(reggstore) bigint It tells number of columns of the specified gstore_fdw foreign table. gstore_fdw_rawsize(reggstore) bigint It tells raw size of the specified gstore_fdw foreign table in bytes. Function Result Description gstore_export_ipchandle(reggstore) bytea It tells IPC-handle of the GPU device memory region of the specified gstore_fdw foreign table. lo_import_gpu(int, bytea, bigint, bigint, oid=0) oid It maps GPU device memory region acquired by external application, then import its contents into a largeobject. lo_export_gpu(oid, int, bytea, bigint, bigint) bigint It maps GPU device memory region acquired by external application, then export contents of the specified largeobject into the region. Definition Result Description attnums_of(regclass,text[]) smallint[] It returns attribute numbers for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. attnum_of(regclass,text) smallint It returns attribute number for the column name of the 2nd argument on the table of the 1st argument. atttypes_of(regclass,text[]) regtype[] It returns data types for the column names (may be multiple) of the 2nd argument on the table of the 1st argument. atttype_of(regclass,text) regtype It returns data type for the column name of the 2nd argument on the table of the 1st argument. attrs_types_check(regclass,text[],regtype[]) bool It checks whether the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data types of the 3rd argument for each. attrs_type_check(regclass,text[],regtype) bool It checks whether all the data types of the columns (may be multiple) of the 2nd argument on the table of the 1st argument match with the data type of the 3rd argument. Function Result Description pgstrom.license_validation() text It validates commercial subscription. pgstrom.license_query() text It shows the active commercial subscription.","title":"Miscellaneous functions"},{"location":"ref_sqlfuncs/#system-view","text":"PG-Strom provides several system view to export its internal state for users or applications. The future version may add extra fields here. So, it is not recommended to reference these information schemas using SELECT * FROM ... . pgstrom.device_info pgstrom.device_into system view exports device attributes of the GPUs recognized by PG-Strom. GPU has different specification for each model, like number of cores, capacity of global memory, maximum number of threads and etc, user's software should be optimized according to the information if you try raw GPU programming with PL/CUDA functions. Name Data Type Description device_nr int GPU device number aindex int Attribute index attribute text Attribute name value text Value of the attribute pgstrom.device_preserved_meminfo pgstrom.device_preserved_meminfo system view exports information of the preserved device memory; which can be shared multiple PostgreSQL backend. Right now, only gstore_fdw uses this feature. Name Data Type Description device_nr int GPU device number handle bytea IPC handle of the preserved device memory owner regrole Owner of the preserved device memory length bigint Length of the preserved device memory in bytes ctime timestamp with time zone Timestamp when the preserved device memory is created","title":"System View"},{"location":"ref_types/","text":"Data Types PG-Strom support the following data types for use on GPU device. Built-in numeric types SQL data types Internal format Length Memo smallint cl_short 2 bytes integer cl_int 4 bytes bigint cl_long 8 bytes real cl_float 4 bytes float cl_double 8 bytes numeric cl_ulong variable length mapped to 64bit internal format Note When GPU processes values in numeric data type, it is converted to an internal 64bit format because of implementation reason. It is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert numaric datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if numeric data with large number of digits are supplied to GPU device. To avoid the problem, turn off the GUC option pg_strom.enable_numeric_type not to run operational expression including numeric data types on GPU devices. Built-in date and time types SQL data types Internal format Length Memo date DateADT 4 bytes time TimeADT 8 bytes timetz TimeTzADT 12 bytes timestamp Timestamp 8 bytes timestamptz TimestampTz 8 bytes interval Interval 16 bytes Built-in variable length types SQL data types Internal format Length Memo bpchar varlena * variable length varchar varlena * variable length bytea varlena * variable length text varlena * variable length Built-in miscellaneous types SQL data types Internal format Length Memo boolean cl_bool 1 byte money cl_long 8 bytes uuid pg_uuid 16 bytes macaddr macaddr 6 bytes inet inet_struct 7 bytes or 19 bytes cidr inet_struct 7 bytes or 19 bytes Built-in range data types SQL data types Internal format Length Memo int4range __int4range 14 bytes int8range __int8range 22 bytes tsrange __tsrange 22 bytes tstzrange __tstzrange 22 bytes daterange __daterange 14 bytes Extra Types SQL data types Internal format Length Memo float2 half_t 2 bytes Half precision data type reggstore cl_uint 4 bytes Specific version of regclass for gstore_fdw. Special handling at PL/CUDA function invocation.","title":"Data Types"},{"location":"ref_types/#built-in-numeric-types","text":"SQL data types Internal format Length Memo smallint cl_short 2 bytes integer cl_int 4 bytes bigint cl_long 8 bytes real cl_float 4 bytes float cl_double 8 bytes numeric cl_ulong variable length mapped to 64bit internal format Note When GPU processes values in numeric data type, it is converted to an internal 64bit format because of implementation reason. It is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert numaric datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if numeric data with large number of digits are supplied to GPU device. To avoid the problem, turn off the GUC option pg_strom.enable_numeric_type not to run operational expression including numeric data types on GPU devices.","title":"Built-in numeric types"},{"location":"ref_types/#built-in-date-and-time-types","text":"SQL data types Internal format Length Memo date DateADT 4 bytes time TimeADT 8 bytes timetz TimeTzADT 12 bytes timestamp Timestamp 8 bytes timestamptz TimestampTz 8 bytes interval Interval 16 bytes","title":"Built-in date and time types"},{"location":"ref_types/#built-in-variable-length-types","text":"SQL data types Internal format Length Memo bpchar varlena * variable length varchar varlena * variable length bytea varlena * variable length text varlena * variable length","title":"Built-in variable length types"},{"location":"ref_types/#built-in-miscellaneous-types","text":"SQL data types Internal format Length Memo boolean cl_bool 1 byte money cl_long 8 bytes uuid pg_uuid 16 bytes macaddr macaddr 6 bytes inet inet_struct 7 bytes or 19 bytes cidr inet_struct 7 bytes or 19 bytes","title":"Built-in miscellaneous types"},{"location":"ref_types/#built-in-range-data-types","text":"SQL data types Internal format Length Memo int4range __int4range 14 bytes int8range __int8range 22 bytes tsrange __tsrange 22 bytes tstzrange __tstzrange 22 bytes daterange __daterange 14 bytes","title":"Built-in range data types"},{"location":"ref_types/#extra-types","text":"SQL data types Internal format Length Memo float2 half_t 2 bytes Half precision data type reggstore cl_uint 4 bytes Specific version of regclass for gstore_fdw. Special handling at PL/CUDA function invocation.","title":"Extra Types"},{"location":"release_note/","text":"PG-Strom v2.2 Release PG-Strom Development Team (1-May-2019) Overview Major enhancement in PG-Strom v2.2 includes: Table partitioning support Columnar store support with Arrow_Fdw Pre-built GPU binary support Enables to implement GPU functions that returns variable length data GpuSort support on GPU memory store (Gstore_Fdw) NVME-oF support (Experimental) Prerequisites PostgreSQL v9.6, v10, v11 CUDA Toolkit 10.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta) New Features Table partitioning support If multi-GPUs configuration, an optimal GPU shall be chosen according to the physical distance between GPU and child tables that construct a partition. If PG-Strom cannot identify the distance from PCIe-bus topology, like NVME-oF configuration, DBA can configure the relation of GPU and NVME-SSD using pg_strom.nvme_distance_map . When we join a partitioned table with non-partition tables, this version can produce a query execution plan that preliminary joins the non-partitioned table with partition child tables for each, and gather the results from child tables. This feature is proposed to PostgreSQL v13 core, as Asymmetric Partition-wise JOIN. Columnar store support with Arrow_Fdw It supports to read external Apache Arrow files using foreign table. It also supports SSD-to-GPU Direct SQL on Apache Arrow files. Pre-built GPU binary support When GPU binary code is generated from SQL, the older version wrote out eitire CUDA C source code, including static portions like libraries, then NVRTC(NVIDIA Run-Time Compiker) built them on the fly. However, a part of complicated function consumed much longer compilation time. v2.2 preliminary builds static functions preliminary, and only dynamic portion from SQL are built dynamically. It reduces the time for GPU binary generation. JSONB data type support This version allows to reference elements of JSONB object, and to utilize them as numeric or test . Enables to implement GPU functions that returns variable length data This version allows to implement SQL functions that returns variable-length data, like textcat , on GPU devices. GpuSort support on GPU memory store (Gstore_Fdw) This version allows to read data from GPU memory store for SQL workloads execution, not only PL/CUDA. Addition of regression test Several simple regression tests are added. NVME-oF support (Experimental) It supports SSD-to-GPU Direct SQL from remote SSD disks which are mounted using NVME-over-Fabric. Please note that it is an experimental feature, and it needs to replace the nvme_rdma kernel module on Red Hat Enterprise Linux 7.x / CentOS 7.x. Features to be deprecated PostgreSQL v9.6 support CustomScan API in PostgreSQL v9.6 lacks a few APIs to handle dynamic shared memory (DSM), so it is unable to collect run-time statistics. It also changes the way to keep expression objects internally, therefore, we had to put #if ... #endif blocks at no little points. It has damaged to code maintainability. Due to the problems, this is the last version to support PostgreSQL v9.6. If you applied PG-Strom on PostgreSQL v9.6, let us recommend to move PostgreSQL v11 as soon as possible. The pgstrom format of Gstore_Fdw foreign table The internal data format on GPU memory store (Gstore_Fdw) is originally designed for data source of PL/CUDA procedures. It is our own format, and used PostgreSQL's data representations as is, like variable-length data, numeric, and so on. After that, NVIDIA released RAPIDS(cuDF), based on Apache Arrow, for data exchange on GPU, then its adoption becomes wider on machine-learning application and Python software stack. PG-Strom will switch its internal data format of Gstore_Fdw, to improve interoperability with these machine-learning software, then existing data format shall be deprecated. Dropped Features In-memory columnar cache As results of use-case analysis, we concluded Arrow_Fdw can replace this feature in most cases. Due to feature duplication, we dropped the in-memory columnar cache. PG-Strom v2.0 Release PG-Strom Development Team (17-Apr-2018) Overview Major enhancement in PG-Strom v2.0 includes: Overall redesign of the internal infrastructure to manage GPU and stabilization CPU+GPU hybrid parallel execution SSD-to-GPU Direct SQL Execution In-memory columnar cache GPU memory store (gstore_fdw) Redesign of GpuJoin and GpuPreAgg and speed-up GpuPreAgg + GpuJoin + GpuScan combined GPU kernel You can download the summary of new features from: PG-Strom v2.0 Technical Brief . Prerequisites PostgreSQL v9.6, v10 CUDA Toolkit 9.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta) New Features Entire re-design and stabilization of the internal infrastructure to manage GPU device. PostgreSQL backend process simultaneously uses only one GPU at most. In case of multi-GPUs installation, it assumes combination use with CPU parallel execution of PostgreSQL. Usually, it is not a matter because throughput of CPU to provide data to GPU is much narrower than capability of GPU processors. We prioritized simpleness of the software architecture. We began to utilize the demand paging feature of GPU device memory supported at the GPU models since Pascal generation. In most of SQL workloads, we cannot know exact size of the required result buffer prior to its execution, therefore, we had allocated more buffer than estimated buffer length, and retried piece of the workloads if estimated buffer size is not sufficient actually. This design restricts available resources of GPU which can be potentially used for other concurrent processes, and complicated error-retry logic was a nightmare for software quality. The demand paging feature allows to eliminate and simplify these stuffs. We stop to use CUDA asynchronous interface. Use of the demand paging feature on GPU device memory makes asynchronous APIs for DMA (like cuMemCpyHtoD ) perform synchronously, then it reduces concurrency and usage ratio of GPU kernels. Instead of the CUDA asynchronous APIs, PG-Strom manages its own worker threads which call synchronous APIs for each. As a by-product, we also could eliminate asynchronous callbacks ( cuStreamAddCallback ), it allows to use MPS daemon which has a restriction at this API. CPU+GPU Hybrid Parallel Execution CPU parallel execution at PostgreSQL v9.6 is newly supported. CustomScan logic of GpuScan, GpuJoin and GpuPreAgg provided by PG-Strom are executable on multiple background worker processes of PostgreSQL in parallel. Limitation: PG-Strom's own statistics displayed at EXPLAIN ANALYZE if CPU parallel execution. Because PostgreSQL v9.6 does not provide ShutdownCustomScan callback of the CustomScan interface, coordinator process has no way to reclaim information of worker processes prior to the release of DSM (Dynamic Shared Memory) segment. SSD-to-GPU Direct SQL Execution By cooperation with the nvme_strom Linux kernel module, it enables to load PostgreSQL's data blocks on NVMe-SSD to GPU device memory directly, bypassing the CPU and host buffer. This feature enables to apply PG-Strom on the area which have to process large data set more than system RAM size. It allows to pull out pretty high throughput close to the hardware limitation because its data stream skips block-device or filesystem layer. Then, GPU runs SQL workloads that usually reduce the amount of data to be processed by CPU. The chemical reaction of these characteristics enables to redefine GPU's role as accelerator of I/O workloads also, not only computing intensive workloads. In-memory Columnar Cache For middle size data-set loadable onto the system RAM, it allows to cache data-blocks in column format which is more suitable for GPU computing. If cached data-blocks are found during table scan, PG-Strom prefers to reference the columnar cache more than shared buffer of PostgreSQL. In-memory columnar cache can be built synchronously, or asynchronously by the background workers. You may remember very early revision of PG-Strom had similar feature. In case when a cached tuple gets updated, the latest in-memory columnar cache which we newly implemented in v2.0 invalidates the cache block which includes the updated tuples. It never updates the columnar cache according to the updates of row-store, so performance degradation is quite limited. GPU Memory Store (gstore_fdw) It enables to write to / read from preserved GPU device memory region by SELECT/INSERT/UPDATE/DELETE in SQL-level, using foreign table interface. In v2.0, only pgstrom internal data format is supported. It saves written data using PG-Strom's buffer format of KDS_FORMAT_COLUMN . It can compress variable length data using LZ algorithm. In v2.0, GPU memory store can be used as data source of PL/CUDA user defined function. Redesign and performance improvement of GpuJoin and GpuPreAgg Stop using Dynamic Parallelism which we internally used in GpuJoin and GpuPreAgg, and revised entire logic of these operations. Old design had a problem of less GPU usage ratio because a GPU kernel which launches GPU sub-kernel and just waits for its completion occupied GPU's execution slot. A coproduct of this redesign is suspend/resume of GpuJoin. In principle, JOIN operation of SQL may generate larger number of rows than number of input rows, but preliminary not predictive. The new design allows to suspend GPU kernel once buffer available space gets lacked, then resume with new result buffer. It simplifies size estimation logic of the result buffer, and eliminates GPU kernel retry by lack of buffer on run-time. GpuPreAgg+GpuJoin+GpuScan combined GPU kernel In case when GPU executable SCAN, JOIN and GROUP BY are serially cascaded, a single GPU kernel invocation runs a series of tasks equivalent to the GpuScan, GpuJoin and GpuPreAgg. This is an approach to minimize data exchange between CPU and GPU. For example, result buffer of GpuJoin is used as input buffer of GpuPreAgg. This feature is especially valuable if combined with SSD-to-GPU Direct SQL Execution. PL/CUDA Enhancement #plcuda_include is enhanced to specify SQL function which returns text type. It can change the code block to inject according to the argument, so it also allows to generate multiple GPU kernel variations, not only inclusion of externally defined functions. If PL/CUDA takes reggstore type argument, GPU kernel function receives pointer of the GPU memory store. Note that it does not pass the OID value. Other Enhancement lo_import_gpu and lo_export_gpu functions allows to import contents of the GPU device memory acquired by external applications directly, or export contents of the largeobject to the GPU device memory. Packaging Add RPM packages to follow the PostgreSQL packages distributed by PostgreSQL Global Development Group. All the software packages are available at HeteroDB SWDC(Software Distribution Center) and downloadable. Document PG-Strom documentation was entirely rewritten using markdown and mkdocs. It makes documentation maintenance easier than the previous HTML based approach, so expects timely updates according to the development of new features. Test Regression test for PG-Strom was built on top of the regression test framework of PostgreSQL. Dropped features PostgreSQL v9.5 Support PostgreSQL v9.6 had big changes in both of the optimizer and executor to support CPU parallel query execution. The biggest change for extension modules that interact them is an enhancement of the interface called \"upper planner path-ification\". It allows to choose an optimal execution-plan from the multiple candidates based on the estimated cost, even if it is aggregation or sorting. It is fundamentally different from the older way where we rewrote query execution plan to inject GpuPreAgg using the hooks. It allows to inject GpuPreAgg node in more reasonable and reliable way, and we could drop complicated (and buggy) logic to rewrite query execution plan once constructed. CustomScan interface is also enhanced to support CPU parallel execution. Due to the reason, we dropped PostgreSQL v9.5 support to follow these new enhancement. GpuSort feature We dropped GpuSort because we have little advantages in the performance. Sorting is one of the GPU suitable workloads. However, in case when we try to sort data blocks larger than GPU device memory, we have to split the data blocks into multiple chunks, then partially sort them and merge them by CPU to generate final results. Larger chunk size is better to reduce the load to merge multiple chunks by CPU, on the other hands, larger chunk size takes larger lead time to launch GPU kernel to sort. It means here is a trade-off; which disallows asynchronous processing by PG-Strom to make data transfer latency invisible. It is hard to solve the problem, or too early to solve the problem, we dropped GpuSort feature once.","title":"Release Note"},{"location":"release_note/#pg-strom-v22-release","text":"PG-Strom Development Team (1-May-2019)","title":"PG-Strom v2.2 Release"},{"location":"release_note/#overview","text":"Major enhancement in PG-Strom v2.2 includes: Table partitioning support Columnar store support with Arrow_Fdw Pre-built GPU binary support Enables to implement GPU functions that returns variable length data GpuSort support on GPU memory store (Gstore_Fdw) NVME-oF support (Experimental)","title":"Overview"},{"location":"release_note/#prerequisites","text":"PostgreSQL v9.6, v10, v11 CUDA Toolkit 10.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta)","title":"Prerequisites"},{"location":"release_note/#new-features","text":"Table partitioning support If multi-GPUs configuration, an optimal GPU shall be chosen according to the physical distance between GPU and child tables that construct a partition. If PG-Strom cannot identify the distance from PCIe-bus topology, like NVME-oF configuration, DBA can configure the relation of GPU and NVME-SSD using pg_strom.nvme_distance_map . When we join a partitioned table with non-partition tables, this version can produce a query execution plan that preliminary joins the non-partitioned table with partition child tables for each, and gather the results from child tables. This feature is proposed to PostgreSQL v13 core, as Asymmetric Partition-wise JOIN. Columnar store support with Arrow_Fdw It supports to read external Apache Arrow files using foreign table. It also supports SSD-to-GPU Direct SQL on Apache Arrow files. Pre-built GPU binary support When GPU binary code is generated from SQL, the older version wrote out eitire CUDA C source code, including static portions like libraries, then NVRTC(NVIDIA Run-Time Compiker) built them on the fly. However, a part of complicated function consumed much longer compilation time. v2.2 preliminary builds static functions preliminary, and only dynamic portion from SQL are built dynamically. It reduces the time for GPU binary generation. JSONB data type support This version allows to reference elements of JSONB object, and to utilize them as numeric or test . Enables to implement GPU functions that returns variable length data This version allows to implement SQL functions that returns variable-length data, like textcat , on GPU devices. GpuSort support on GPU memory store (Gstore_Fdw) This version allows to read data from GPU memory store for SQL workloads execution, not only PL/CUDA. Addition of regression test Several simple regression tests are added. NVME-oF support (Experimental) It supports SSD-to-GPU Direct SQL from remote SSD disks which are mounted using NVME-over-Fabric. Please note that it is an experimental feature, and it needs to replace the nvme_rdma kernel module on Red Hat Enterprise Linux 7.x / CentOS 7.x.","title":"New Features"},{"location":"release_note/#features-to-be-deprecated","text":"PostgreSQL v9.6 support CustomScan API in PostgreSQL v9.6 lacks a few APIs to handle dynamic shared memory (DSM), so it is unable to collect run-time statistics. It also changes the way to keep expression objects internally, therefore, we had to put #if ... #endif blocks at no little points. It has damaged to code maintainability. Due to the problems, this is the last version to support PostgreSQL v9.6. If you applied PG-Strom on PostgreSQL v9.6, let us recommend to move PostgreSQL v11 as soon as possible. The pgstrom format of Gstore_Fdw foreign table The internal data format on GPU memory store (Gstore_Fdw) is originally designed for data source of PL/CUDA procedures. It is our own format, and used PostgreSQL's data representations as is, like variable-length data, numeric, and so on. After that, NVIDIA released RAPIDS(cuDF), based on Apache Arrow, for data exchange on GPU, then its adoption becomes wider on machine-learning application and Python software stack. PG-Strom will switch its internal data format of Gstore_Fdw, to improve interoperability with these machine-learning software, then existing data format shall be deprecated.","title":"Features to be deprecated"},{"location":"release_note/#dropped-features","text":"In-memory columnar cache As results of use-case analysis, we concluded Arrow_Fdw can replace this feature in most cases. Due to feature duplication, we dropped the in-memory columnar cache.","title":"Dropped Features"},{"location":"release_note/#pg-strom-v20-release","text":"PG-Strom Development Team (17-Apr-2018)","title":"PG-Strom v2.0 Release"},{"location":"release_note/#overview_1","text":"Major enhancement in PG-Strom v2.0 includes: Overall redesign of the internal infrastructure to manage GPU and stabilization CPU+GPU hybrid parallel execution SSD-to-GPU Direct SQL Execution In-memory columnar cache GPU memory store (gstore_fdw) Redesign of GpuJoin and GpuPreAgg and speed-up GpuPreAgg + GpuJoin + GpuScan combined GPU kernel You can download the summary of new features from: PG-Strom v2.0 Technical Brief .","title":"Overview"},{"location":"release_note/#prerequisites_1","text":"PostgreSQL v9.6, v10 CUDA Toolkit 9.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta)","title":"Prerequisites"},{"location":"release_note/#new-features_1","text":"Entire re-design and stabilization of the internal infrastructure to manage GPU device. PostgreSQL backend process simultaneously uses only one GPU at most. In case of multi-GPUs installation, it assumes combination use with CPU parallel execution of PostgreSQL. Usually, it is not a matter because throughput of CPU to provide data to GPU is much narrower than capability of GPU processors. We prioritized simpleness of the software architecture. We began to utilize the demand paging feature of GPU device memory supported at the GPU models since Pascal generation. In most of SQL workloads, we cannot know exact size of the required result buffer prior to its execution, therefore, we had allocated more buffer than estimated buffer length, and retried piece of the workloads if estimated buffer size is not sufficient actually. This design restricts available resources of GPU which can be potentially used for other concurrent processes, and complicated error-retry logic was a nightmare for software quality. The demand paging feature allows to eliminate and simplify these stuffs. We stop to use CUDA asynchronous interface. Use of the demand paging feature on GPU device memory makes asynchronous APIs for DMA (like cuMemCpyHtoD ) perform synchronously, then it reduces concurrency and usage ratio of GPU kernels. Instead of the CUDA asynchronous APIs, PG-Strom manages its own worker threads which call synchronous APIs for each. As a by-product, we also could eliminate asynchronous callbacks ( cuStreamAddCallback ), it allows to use MPS daemon which has a restriction at this API. CPU+GPU Hybrid Parallel Execution CPU parallel execution at PostgreSQL v9.6 is newly supported. CustomScan logic of GpuScan, GpuJoin and GpuPreAgg provided by PG-Strom are executable on multiple background worker processes of PostgreSQL in parallel. Limitation: PG-Strom's own statistics displayed at EXPLAIN ANALYZE if CPU parallel execution. Because PostgreSQL v9.6 does not provide ShutdownCustomScan callback of the CustomScan interface, coordinator process has no way to reclaim information of worker processes prior to the release of DSM (Dynamic Shared Memory) segment. SSD-to-GPU Direct SQL Execution By cooperation with the nvme_strom Linux kernel module, it enables to load PostgreSQL's data blocks on NVMe-SSD to GPU device memory directly, bypassing the CPU and host buffer. This feature enables to apply PG-Strom on the area which have to process large data set more than system RAM size. It allows to pull out pretty high throughput close to the hardware limitation because its data stream skips block-device or filesystem layer. Then, GPU runs SQL workloads that usually reduce the amount of data to be processed by CPU. The chemical reaction of these characteristics enables to redefine GPU's role as accelerator of I/O workloads also, not only computing intensive workloads. In-memory Columnar Cache For middle size data-set loadable onto the system RAM, it allows to cache data-blocks in column format which is more suitable for GPU computing. If cached data-blocks are found during table scan, PG-Strom prefers to reference the columnar cache more than shared buffer of PostgreSQL. In-memory columnar cache can be built synchronously, or asynchronously by the background workers. You may remember very early revision of PG-Strom had similar feature. In case when a cached tuple gets updated, the latest in-memory columnar cache which we newly implemented in v2.0 invalidates the cache block which includes the updated tuples. It never updates the columnar cache according to the updates of row-store, so performance degradation is quite limited. GPU Memory Store (gstore_fdw) It enables to write to / read from preserved GPU device memory region by SELECT/INSERT/UPDATE/DELETE in SQL-level, using foreign table interface. In v2.0, only pgstrom internal data format is supported. It saves written data using PG-Strom's buffer format of KDS_FORMAT_COLUMN . It can compress variable length data using LZ algorithm. In v2.0, GPU memory store can be used as data source of PL/CUDA user defined function. Redesign and performance improvement of GpuJoin and GpuPreAgg Stop using Dynamic Parallelism which we internally used in GpuJoin and GpuPreAgg, and revised entire logic of these operations. Old design had a problem of less GPU usage ratio because a GPU kernel which launches GPU sub-kernel and just waits for its completion occupied GPU's execution slot. A coproduct of this redesign is suspend/resume of GpuJoin. In principle, JOIN operation of SQL may generate larger number of rows than number of input rows, but preliminary not predictive. The new design allows to suspend GPU kernel once buffer available space gets lacked, then resume with new result buffer. It simplifies size estimation logic of the result buffer, and eliminates GPU kernel retry by lack of buffer on run-time. GpuPreAgg+GpuJoin+GpuScan combined GPU kernel In case when GPU executable SCAN, JOIN and GROUP BY are serially cascaded, a single GPU kernel invocation runs a series of tasks equivalent to the GpuScan, GpuJoin and GpuPreAgg. This is an approach to minimize data exchange between CPU and GPU. For example, result buffer of GpuJoin is used as input buffer of GpuPreAgg. This feature is especially valuable if combined with SSD-to-GPU Direct SQL Execution. PL/CUDA Enhancement #plcuda_include is enhanced to specify SQL function which returns text type. It can change the code block to inject according to the argument, so it also allows to generate multiple GPU kernel variations, not only inclusion of externally defined functions. If PL/CUDA takes reggstore type argument, GPU kernel function receives pointer of the GPU memory store. Note that it does not pass the OID value. Other Enhancement lo_import_gpu and lo_export_gpu functions allows to import contents of the GPU device memory acquired by external applications directly, or export contents of the largeobject to the GPU device memory. Packaging Add RPM packages to follow the PostgreSQL packages distributed by PostgreSQL Global Development Group. All the software packages are available at HeteroDB SWDC(Software Distribution Center) and downloadable. Document PG-Strom documentation was entirely rewritten using markdown and mkdocs. It makes documentation maintenance easier than the previous HTML based approach, so expects timely updates according to the development of new features. Test Regression test for PG-Strom was built on top of the regression test framework of PostgreSQL.","title":"New Features"},{"location":"release_note/#dropped-features_1","text":"PostgreSQL v9.5 Support PostgreSQL v9.6 had big changes in both of the optimizer and executor to support CPU parallel query execution. The biggest change for extension modules that interact them is an enhancement of the interface called \"upper planner path-ification\". It allows to choose an optimal execution-plan from the multiple candidates based on the estimated cost, even if it is aggregation or sorting. It is fundamentally different from the older way where we rewrote query execution plan to inject GpuPreAgg using the hooks. It allows to inject GpuPreAgg node in more reasonable and reliable way, and we could drop complicated (and buggy) logic to rewrite query execution plan once constructed. CustomScan interface is also enhanced to support CPU parallel execution. Due to the reason, we dropped PostgreSQL v9.5 support to follow these new enhancement. GpuSort feature We dropped GpuSort because we have little advantages in the performance. Sorting is one of the GPU suitable workloads. However, in case when we try to sort data blocks larger than GPU device memory, we have to split the data blocks into multiple chunks, then partially sort them and merge them by CPU to generate final results. Larger chunk size is better to reduce the load to merge multiple chunks by CPU, on the other hands, larger chunk size takes larger lead time to launch GPU kernel to sort. It means here is a trade-off; which disallows asynchronous processing by PG-Strom to make data transfer latency invisible. It is hard to solve the problem, or too early to solve the problem, we dropped GpuSort feature once.","title":"Dropped features"},{"location":"ssd2gpu/","text":"SSD-to-GPU Direct SQL Execution Overview For the fast execution of SQL workloads, it needs to provide processors rapid data stream from storage or memory, in addition to processor's execution efficiency. Processor will run idle if data stream would not be delivered. SSD-to-GPU Direct SQL Execution directly connects NVMe-SSD which enables high-speed I/O processing by direct attach to the PCIe bus and GPU device that is also attached on the same PCIe bus, and runs SQL workloads very high speed by supplying data stream close to the wired speed of the hardware. Usually, PostgreSQL data blocks on the storage shall be once loaded to CPU/RAM through the PCIe bus, then, PostgreSQL runs WHERE-clause for filtering or JOIN/GROUP BY according to the query execution plan. Due to the characteristics of analytic workloads, the amount of result data set is much smaller than the source data set. For example, it is not rare case to read billions rows but output just hundreds rows after the aggregation operations with GROUP BY. In the other words, we consume bandwidth of the PCIe bus to move junk data, however, we cannot determine whether rows are necessary or not prior to the evaluation by SQL workloads on CPU. So, it is not avoidable restriction in usual implementation. SSD-to-GPU Direct SQL Execution changes the flow to read blocks from the storage sequentially. It directly loads data blocks to GPU using peer-to-peer DMA over PCIe bus, then runs SQL workloads on GPU device to reduce number of rows to be processed by CPU. In other words, it utilizes GPU as a pre-processor of SQL which locates in the middle of the storage and CPU/RAM for reduction of CPU's load, then tries to accelerate I/O processing in the results. This feature internally uses NVIDIA GPUDirect RDMA. It allows peer-to-peer data transfer over PCIe bus between GPU device memory and third parth device by coordination using a custom Linux kernel module. So, this feature requires NVMe-Strom driver which is a Linux kernel module in addition to PG-Strom which is a PostgreSQL extension module. Also note that this feature supports only NVMe-SSD. It does not support SAS or SATA SSD. We have tested several NVMe-SSD models. You can refer 002: HW Validation List for your information. System Setup Driver Installation nvme_strom package is required to activate SSD-to-GPU Direct SQL Execution. This package contains a custom Linux kernel module which intermediates P2P DMA from NVME-SSD to GPU. You can obtain the package from the HeteroDB Software Distribution Center . If heterodb-swdc package is already installed, you can install the package by yum command. $ sudo yum install nvme_strom : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: nvme_strom x86_64 0.8-1.el7 heterodb 178 k Transaction Summary ================================================================================ Install 1 Package : DKMS: install completed. Verifying : nvme_strom-0.8-1.el7.x86_64 1/1 Installed: nvme_strom.x86_64 0:0.8-1.el7 Complete! Once nvme_strom package gets installed, you can see nvme_strom module using lsmod command below. $ lsmod | grep nvme nvme_strom 12625 0 nvme 27722 4 nvme_core 52964 9 nvme Designing Tablespace SSD-to-GPU Direct SQL Execution shall be invoked in the following case. The target table to be scanned locates on the partition being consist of NVMe-SSD. /dev/nvmeXXXX block device, or md-raid0 volume which consists of NVMe-SSDs only. The target table size is larger than pg_strom.nvme_strom_threshold . You can adjust this configuration. Its default is physical RAM size of the system plus 1/3 of shared_buffers configuration. Note Striped read from multiple NVMe-SSD using md-raid0 requires the enterprise subscription provided by HeteroDB,Inc. In order to deploy the tables on the partition consists of NVMe-SSD, you can use the tablespace function of PostgreSQL to specify particular tables or databases to place them on NVMe-SSD volume, in addition to construction of the entire database cluster on the NVMe-SSD volume. For example, you can create a new tablespace below, if NVMe-SSD is mounted at /opt/nvme . CREATE TABLESPACE my_nvme LOCATION '/opt/nvme'; In order to create a new table on the tablespace, specify the TABLESPACE option at the CREATE TABLE command below. CREATE TABLE my_table (...) TABLESPACE my_nvme; Or, use ALTER DATABASE command as follows, to change the default tablespace of the database. Note that tablespace of the existing tables are not changed in thie case. ALTER DATABASE my_database SET TABLESPACE my_nvme; Distance between GPU and NVME-SSD On selection of server hardware and installation of GPU and NVME-SSD, hardware configuration needs to pay attention to the distance between devices, to pull out maximum performance of the device. NVIDIA GPUDirect RDMA , basis of the SSD-to-GPU Direct SQL mechanism, requires both of the edge devices of P2P DMA are connected on the same PCIe root complex. In the other words, unable to configure the P2P DMA traverses QPI between CPUs when NVME-SSD is attached on CPU1 and GPU is attached on CPU2 at dual socket system. From standpoint of the performance, it is recommended to use dedicated PCIe-switch to connect both of the devices more than the PCIe controller built in CPU. The photo below is a motherboard of HPC server. It has 8 of PCIe x16 slots, and each pair is linked to the other over the PCIe switch. The slots in the left-side of the photo are connected to CPU1, and right-side are connected to CPU2. When a table on SSD-2 is scanned using SSD-to-GPU Direct SQL, the optimal GPU choice is GPU-2, and it may be able to use GPU1. However, we have to avoid to choose GPU-3 and GPU-4 due to the restriction of GPUDirect RDMA. PG-Strom calculate logical distances on any pairs of GPU and NVME-SSD using PCIe bus topology information of the system on startup time. It is displayed at the start up log. Each NVME-SSD determines the preferable GPU based on the distance, for example, GPU1 shall be used on scan of the /dev/nvme2 . $ pg_ctl restart : LOG: GPU<->SSD Distance Matrix LOG: GPU0 GPU1 GPU2 LOG: nvme0 ( 3) 7 7 LOG: nvme5 7 7 ( 3) LOG: nvme4 7 7 ( 3) LOG: nvme2 7 ( 3) 7 LOG: nvme1 ( 3) 7 7 LOG: nvme3 7 ( 3) 7 : Usually automatic configuration works well. In case when NVME-over-Fabric(RDMA) is used, unable to identify the location of nvme device on the PCIe-bus, so you need to configure the logical distance between NVME-SSD and GPU manually. The example below shows the configuration of gpu2 for nvme1 , and gpu1 for nvme2 and nvme3 . It shall be added to postgresql.conf . Please note than manual configuration takes priority than the automatic configuration. pg_strom.nvme_distance_map = nvme1:gpu2, nvme2:gpu1, nvme3:gpu1 Operations Controls using GUC parameters There are two GPU parameters related to SSD-to-GPU Direct SQL Execution. The first is pg_strom.nvme_strom_enabled that simply turn on/off the function of SSD-to-GPU Direct SQL Execution. If off , SSD-to-GPU Direct SQL Execution should not be used regardless of the table size or physical location. Default is on . The other one is pg_strom.nvme_strom_threshold which specifies the least table size to invoke SSD-to-GPU Direct SQL Execution. PG-Strom will choose SSD-to-GPU Direct SQL Execution when target table is located on NVMe-SSD volume (or md-raid0 volume which consists of NVMe-SSD only), and the table size is larger than this parameter. Its default is sum of the physical memory size and 1/3 of the shared_buffers . It means default configuration invokes SSD-to-GPU Direct SQL Execution only for the tables where we certainly cannot process them on memory. Even if SSD-to-GPU Direct SQL Execution has advantages on a single table scan workload, usage of disk cache may work better on the second or later trial for the tables which are available to load onto the main memory. On course, this assumption is not always right depending on the workload charasteristics. Ensure usage of SSD-to-GPU Direct SQL Execution EXPLAIN command allows to ensure whether SSD-to-GPU Direct SQL Execution shall be used in the target query, or not. In the example below, a scan on the lineorder table by Custom Scan (GpuJoin) shows NVMe-Strom: enabled . In this case, SSD-to-GPU Direct SQL Execution shall be used to read from the lineorder table. # explain (costs off) select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_category = 'MFGR#12' and s_region = 'AMERICA' group by d_year, p_brand1 order by d_year, p_brand1; QUERY PLAN ---------------------------------------------------------------------------------------------- GroupAggregate Group Key: date1.d_year, part.p_brand1 -> Sort Sort Key: date1.d_year, part.p_brand1 -> Custom Scan (GpuPreAgg) Reduction: Local GPU Projection: pgstrom.psum((lo_revenue)::double precision), d_year, p_brand1 Combined GpuJoin: enabled -> Custom Scan (GpuJoin) on lineorder GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue Outer Scan: lineorder Depth 1: GpuHashJoin (nrows 2406009600...97764190) HashKeys: lineorder.lo_partkey JoinQuals: (lineorder.lo_partkey = part.p_partkey) KDS-Hash (size: 10.67MB) Depth 2: GpuHashJoin (nrows 97764190...18544060) HashKeys: lineorder.lo_suppkey JoinQuals: (lineorder.lo_suppkey = supplier.s_suppkey) KDS-Hash (size: 131.59MB) Depth 3: GpuHashJoin (nrows 18544060...18544060) HashKeys: lineorder.lo_orderdate JoinQuals: (lineorder.lo_orderdate = date1.d_datekey) KDS-Hash (size: 461.89KB) NVMe-Strom: enabled -> Custom Scan (GpuScan) on part GPU Projection: p_brand1, p_partkey GPU Filter: (p_category = 'MFGR#12'::bpchar) -> Custom Scan (GpuScan) on supplier GPU Projection: s_suppkey GPU Filter: (s_region = 'AMERICA'::bpchar) -> Seq Scan on date1 (31 rows) Attension for visibility map Right now, GPU routines of PG-Strom cannot run MVCC visibility checks per row, because only host code has a special data structure for visibility checks. It also leads a problem. We cannot know which row is visible, or invisible at the time when PG-Strom requires P2P DMA for NVMe-SSD, because contents of the storage blocks are not yet loaded to CPU/RAM, and MVCC related attributes are written with individual records. PostgreSQL had similar problem when it supports IndexOnlyScan. To address the problem, PostgreSQL has an infrastructure of visibility map which is a bunch of flags to indicate whether any records in a particular data block are visible from all the transactions. If associated bit is set, we can know the associated block has no invisible records without reading the block itself. SSD-to-GPU Direct SQL Execution utilizes this infrastructure. It checks the visibility map first, then only \"all-visible\" blocks are required to read with SSD-to-GPU P2P DMA. VACUUM constructs visibility map, so you can enforce PostgreSQL to construct visibility map by explicit launch of VACUUM command. VACUUM ANALYZE linerorder;","title":"SSD2GPU Direct SQL"},{"location":"ssd2gpu/#overview","text":"For the fast execution of SQL workloads, it needs to provide processors rapid data stream from storage or memory, in addition to processor's execution efficiency. Processor will run idle if data stream would not be delivered. SSD-to-GPU Direct SQL Execution directly connects NVMe-SSD which enables high-speed I/O processing by direct attach to the PCIe bus and GPU device that is also attached on the same PCIe bus, and runs SQL workloads very high speed by supplying data stream close to the wired speed of the hardware. Usually, PostgreSQL data blocks on the storage shall be once loaded to CPU/RAM through the PCIe bus, then, PostgreSQL runs WHERE-clause for filtering or JOIN/GROUP BY according to the query execution plan. Due to the characteristics of analytic workloads, the amount of result data set is much smaller than the source data set. For example, it is not rare case to read billions rows but output just hundreds rows after the aggregation operations with GROUP BY. In the other words, we consume bandwidth of the PCIe bus to move junk data, however, we cannot determine whether rows are necessary or not prior to the evaluation by SQL workloads on CPU. So, it is not avoidable restriction in usual implementation. SSD-to-GPU Direct SQL Execution changes the flow to read blocks from the storage sequentially. It directly loads data blocks to GPU using peer-to-peer DMA over PCIe bus, then runs SQL workloads on GPU device to reduce number of rows to be processed by CPU. In other words, it utilizes GPU as a pre-processor of SQL which locates in the middle of the storage and CPU/RAM for reduction of CPU's load, then tries to accelerate I/O processing in the results. This feature internally uses NVIDIA GPUDirect RDMA. It allows peer-to-peer data transfer over PCIe bus between GPU device memory and third parth device by coordination using a custom Linux kernel module. So, this feature requires NVMe-Strom driver which is a Linux kernel module in addition to PG-Strom which is a PostgreSQL extension module. Also note that this feature supports only NVMe-SSD. It does not support SAS or SATA SSD. We have tested several NVMe-SSD models. You can refer 002: HW Validation List for your information.","title":"Overview"},{"location":"ssd2gpu/#system-setup","text":"","title":"System Setup"},{"location":"ssd2gpu/#driver-installation","text":"nvme_strom package is required to activate SSD-to-GPU Direct SQL Execution. This package contains a custom Linux kernel module which intermediates P2P DMA from NVME-SSD to GPU. You can obtain the package from the HeteroDB Software Distribution Center . If heterodb-swdc package is already installed, you can install the package by yum command. $ sudo yum install nvme_strom : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: nvme_strom x86_64 0.8-1.el7 heterodb 178 k Transaction Summary ================================================================================ Install 1 Package : DKMS: install completed. Verifying : nvme_strom-0.8-1.el7.x86_64 1/1 Installed: nvme_strom.x86_64 0:0.8-1.el7 Complete! Once nvme_strom package gets installed, you can see nvme_strom module using lsmod command below. $ lsmod | grep nvme nvme_strom 12625 0 nvme 27722 4 nvme_core 52964 9 nvme","title":"Driver Installation"},{"location":"ssd2gpu/#designing-tablespace","text":"SSD-to-GPU Direct SQL Execution shall be invoked in the following case. The target table to be scanned locates on the partition being consist of NVMe-SSD. /dev/nvmeXXXX block device, or md-raid0 volume which consists of NVMe-SSDs only. The target table size is larger than pg_strom.nvme_strom_threshold . You can adjust this configuration. Its default is physical RAM size of the system plus 1/3 of shared_buffers configuration. Note Striped read from multiple NVMe-SSD using md-raid0 requires the enterprise subscription provided by HeteroDB,Inc. In order to deploy the tables on the partition consists of NVMe-SSD, you can use the tablespace function of PostgreSQL to specify particular tables or databases to place them on NVMe-SSD volume, in addition to construction of the entire database cluster on the NVMe-SSD volume. For example, you can create a new tablespace below, if NVMe-SSD is mounted at /opt/nvme . CREATE TABLESPACE my_nvme LOCATION '/opt/nvme'; In order to create a new table on the tablespace, specify the TABLESPACE option at the CREATE TABLE command below. CREATE TABLE my_table (...) TABLESPACE my_nvme; Or, use ALTER DATABASE command as follows, to change the default tablespace of the database. Note that tablespace of the existing tables are not changed in thie case. ALTER DATABASE my_database SET TABLESPACE my_nvme;","title":"Designing Tablespace"},{"location":"ssd2gpu/#distance-between-gpu-and-nvme-ssd","text":"On selection of server hardware and installation of GPU and NVME-SSD, hardware configuration needs to pay attention to the distance between devices, to pull out maximum performance of the device. NVIDIA GPUDirect RDMA , basis of the SSD-to-GPU Direct SQL mechanism, requires both of the edge devices of P2P DMA are connected on the same PCIe root complex. In the other words, unable to configure the P2P DMA traverses QPI between CPUs when NVME-SSD is attached on CPU1 and GPU is attached on CPU2 at dual socket system. From standpoint of the performance, it is recommended to use dedicated PCIe-switch to connect both of the devices more than the PCIe controller built in CPU. The photo below is a motherboard of HPC server. It has 8 of PCIe x16 slots, and each pair is linked to the other over the PCIe switch. The slots in the left-side of the photo are connected to CPU1, and right-side are connected to CPU2. When a table on SSD-2 is scanned using SSD-to-GPU Direct SQL, the optimal GPU choice is GPU-2, and it may be able to use GPU1. However, we have to avoid to choose GPU-3 and GPU-4 due to the restriction of GPUDirect RDMA. PG-Strom calculate logical distances on any pairs of GPU and NVME-SSD using PCIe bus topology information of the system on startup time. It is displayed at the start up log. Each NVME-SSD determines the preferable GPU based on the distance, for example, GPU1 shall be used on scan of the /dev/nvme2 . $ pg_ctl restart : LOG: GPU<->SSD Distance Matrix LOG: GPU0 GPU1 GPU2 LOG: nvme0 ( 3) 7 7 LOG: nvme5 7 7 ( 3) LOG: nvme4 7 7 ( 3) LOG: nvme2 7 ( 3) 7 LOG: nvme1 ( 3) 7 7 LOG: nvme3 7 ( 3) 7 : Usually automatic configuration works well. In case when NVME-over-Fabric(RDMA) is used, unable to identify the location of nvme device on the PCIe-bus, so you need to configure the logical distance between NVME-SSD and GPU manually. The example below shows the configuration of gpu2 for nvme1 , and gpu1 for nvme2 and nvme3 . It shall be added to postgresql.conf . Please note than manual configuration takes priority than the automatic configuration. pg_strom.nvme_distance_map = nvme1:gpu2, nvme2:gpu1, nvme3:gpu1","title":"Distance between GPU and NVME-SSD"},{"location":"ssd2gpu/#operations","text":"","title":"Operations"},{"location":"ssd2gpu/#controls-using-guc-parameters","text":"There are two GPU parameters related to SSD-to-GPU Direct SQL Execution. The first is pg_strom.nvme_strom_enabled that simply turn on/off the function of SSD-to-GPU Direct SQL Execution. If off , SSD-to-GPU Direct SQL Execution should not be used regardless of the table size or physical location. Default is on . The other one is pg_strom.nvme_strom_threshold which specifies the least table size to invoke SSD-to-GPU Direct SQL Execution. PG-Strom will choose SSD-to-GPU Direct SQL Execution when target table is located on NVMe-SSD volume (or md-raid0 volume which consists of NVMe-SSD only), and the table size is larger than this parameter. Its default is sum of the physical memory size and 1/3 of the shared_buffers . It means default configuration invokes SSD-to-GPU Direct SQL Execution only for the tables where we certainly cannot process them on memory. Even if SSD-to-GPU Direct SQL Execution has advantages on a single table scan workload, usage of disk cache may work better on the second or later trial for the tables which are available to load onto the main memory. On course, this assumption is not always right depending on the workload charasteristics.","title":"Controls using GUC parameters"},{"location":"ssd2gpu/#ensure-usage-of-ssd-to-gpu-direct-sql-execution","text":"EXPLAIN command allows to ensure whether SSD-to-GPU Direct SQL Execution shall be used in the target query, or not. In the example below, a scan on the lineorder table by Custom Scan (GpuJoin) shows NVMe-Strom: enabled . In this case, SSD-to-GPU Direct SQL Execution shall be used to read from the lineorder table. # explain (costs off) select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_category = 'MFGR#12' and s_region = 'AMERICA' group by d_year, p_brand1 order by d_year, p_brand1; QUERY PLAN ---------------------------------------------------------------------------------------------- GroupAggregate Group Key: date1.d_year, part.p_brand1 -> Sort Sort Key: date1.d_year, part.p_brand1 -> Custom Scan (GpuPreAgg) Reduction: Local GPU Projection: pgstrom.psum((lo_revenue)::double precision), d_year, p_brand1 Combined GpuJoin: enabled -> Custom Scan (GpuJoin) on lineorder GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue Outer Scan: lineorder Depth 1: GpuHashJoin (nrows 2406009600...97764190) HashKeys: lineorder.lo_partkey JoinQuals: (lineorder.lo_partkey = part.p_partkey) KDS-Hash (size: 10.67MB) Depth 2: GpuHashJoin (nrows 97764190...18544060) HashKeys: lineorder.lo_suppkey JoinQuals: (lineorder.lo_suppkey = supplier.s_suppkey) KDS-Hash (size: 131.59MB) Depth 3: GpuHashJoin (nrows 18544060...18544060) HashKeys: lineorder.lo_orderdate JoinQuals: (lineorder.lo_orderdate = date1.d_datekey) KDS-Hash (size: 461.89KB) NVMe-Strom: enabled -> Custom Scan (GpuScan) on part GPU Projection: p_brand1, p_partkey GPU Filter: (p_category = 'MFGR#12'::bpchar) -> Custom Scan (GpuScan) on supplier GPU Projection: s_suppkey GPU Filter: (s_region = 'AMERICA'::bpchar) -> Seq Scan on date1 (31 rows)","title":"Ensure usage of SSD-to-GPU Direct SQL Execution"},{"location":"ssd2gpu/#attension-for-visibility-map","text":"Right now, GPU routines of PG-Strom cannot run MVCC visibility checks per row, because only host code has a special data structure for visibility checks. It also leads a problem. We cannot know which row is visible, or invisible at the time when PG-Strom requires P2P DMA for NVMe-SSD, because contents of the storage blocks are not yet loaded to CPU/RAM, and MVCC related attributes are written with individual records. PostgreSQL had similar problem when it supports IndexOnlyScan. To address the problem, PostgreSQL has an infrastructure of visibility map which is a bunch of flags to indicate whether any records in a particular data block are visible from all the transactions. If associated bit is set, we can know the associated block has no invisible records without reading the block itself. SSD-to-GPU Direct SQL Execution utilizes this infrastructure. It checks the visibility map first, then only \"all-visible\" blocks are required to read with SSD-to-GPU P2P DMA. VACUUM constructs visibility map, so you can enforce PostgreSQL to construct visibility map by explicit launch of VACUUM command. VACUUM ANALYZE linerorder;","title":"Attension for visibility map"},{"location":"sys_admin/","text":"System Administration Usage of MPS daemon In case when multi-process application like PostgreSQL uses GPU device, it is a well known solution to use MPS (Multi-Process Service) to reduce context switch on GPU side and resource consumption for device management. https://docs.nvidia.com/deploy/mps/index.html It is also recommended for PG-Strom to apply MPS, however, you need to pay attention for several limitations below. One MPS daemon can provide its service for up to 48 clients (16 clients if Pascal or older). So, DB administration must ensure number of PostgreSQL processes using GPU (including background workers in CPU parallelism) is less than 48 (or 16 if Pascal). MPS does not support dynamic parallelism, and load GPU programs using the feature. GPU programs automatically generated from SQL will never use dynamic parallelism, however, PL/CUDA user defined function may use dynamic parallelism if it links CUDA device runtime to invoke sub-kernels. So, we don't use MPS for invocation of PL/CUDA functions. MPS document recommends to set compute-mode EXCLUSIVE_PROCESS , however, PG-Strom requires DEFAULT mode. Several operations, including PL/CUDA above, call CUDA APIs with MPS disabled explicitly, so other processes than MPS daemon must be able to use GPU devices. The following commands start MPS daemon. Replace <UID> with user-id of PostgreSQL process. $ nvidia-cuda-mps-control -d $ echo start_server -uid <UID> | nvidia-cuda-mps-control nvidia-smi command shows MPS daemon is using GPU device. $ nvidia-smi Sat Nov 3 12:22:26 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 410.48 Driver Version: 410.48 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla V100-PCIE... Off | 00000000:02:00.0 Off | 0 | | N/A 45C P0 38W / 250W | 40MiB / 16130MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 11080 C nvidia-cuda-mps-server 29MiB | +-----------------------------------------------------------------------------+ Knowledge base We publish several articles, just called \"notes\", on the project wiki-site of PG-Strom. https://github.com/heterodb/pg-strom/wiki","title":"System Administration"},{"location":"sys_admin/#system-administration","text":"","title":"System Administration"},{"location":"sys_admin/#usage-of-mps-daemon","text":"In case when multi-process application like PostgreSQL uses GPU device, it is a well known solution to use MPS (Multi-Process Service) to reduce context switch on GPU side and resource consumption for device management. https://docs.nvidia.com/deploy/mps/index.html It is also recommended for PG-Strom to apply MPS, however, you need to pay attention for several limitations below. One MPS daemon can provide its service for up to 48 clients (16 clients if Pascal or older). So, DB administration must ensure number of PostgreSQL processes using GPU (including background workers in CPU parallelism) is less than 48 (or 16 if Pascal). MPS does not support dynamic parallelism, and load GPU programs using the feature. GPU programs automatically generated from SQL will never use dynamic parallelism, however, PL/CUDA user defined function may use dynamic parallelism if it links CUDA device runtime to invoke sub-kernels. So, we don't use MPS for invocation of PL/CUDA functions. MPS document recommends to set compute-mode EXCLUSIVE_PROCESS , however, PG-Strom requires DEFAULT mode. Several operations, including PL/CUDA above, call CUDA APIs with MPS disabled explicitly, so other processes than MPS daemon must be able to use GPU devices. The following commands start MPS daemon. Replace <UID> with user-id of PostgreSQL process. $ nvidia-cuda-mps-control -d $ echo start_server -uid <UID> | nvidia-cuda-mps-control nvidia-smi command shows MPS daemon is using GPU device. $ nvidia-smi Sat Nov 3 12:22:26 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 410.48 Driver Version: 410.48 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla V100-PCIE... Off | 00000000:02:00.0 Off | 0 | | N/A 45C P0 38W / 250W | 40MiB / 16130MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 11080 C nvidia-cuda-mps-server 29MiB | +-----------------------------------------------------------------------------+","title":"Usage of MPS daemon"},{"location":"sys_admin/#knowledge-base","text":"We publish several articles, just called \"notes\", on the project wiki-site of PG-Strom. https://github.com/heterodb/pg-strom/wiki","title":"Knowledge base"},{"location":"troubles/","text":"Trouble Shooting Identify the problem In case when a particular workloads produce problems, it is the first step to identify which stuff may cause the problem. Unfortunately, much smaller number of developer supports the PG-Strom development community than PostgreSQL developer's community, thus, due to the standpoint of software quality and history, it is a reasonable estimation to suspect PG-Strom first. The pg_strom.enabled parameter allows to turn on/off all the functionality of PG-Strom at once. The configuration below disables PG-Strom, thus identically performs with the standard PostgreSQL. # SET pg_strom.enabled = off; In addition, we provide parameters to disable particular execution plan like GpuScan, GpuJoin and GpuPreAgg. See references/GUC Parameters for more details. Collecting crash dump Crash dump is very helpful for analysis of serious problems which lead system crash for example. This session introduces the way to collect crash dump of the PostgreSQL and PG-Strom process (CPU side) and PG-Strom's GPU kernel, and show the back trace on the serious problems. Add configuration on PostgreSQL startup For generation of crash dump (CPU-side) on process crash, you need to change the resource limitation of the operating system for size of core file PostgreSQL server process can generate. For generation of crash dump (GPU-size) on errors of GPU kernel, PostgreSQL server process has CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable, and its value has 1 . You can put a configuration file at /etc/systemd/system/postgresql-<version>.service.d/ when PostgreSQL is kicked by systemd. In case of RPM installation, a configuration file pg_strom.conf is also installed on the directory, and contains the following initial configuration. [Service] LimitNOFILE=65536 LimitCORE=infinity #Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 In CUDA 9.1, it usually takes more than several minutes to generate crash dump of GPU kernel, and it entirely stops response of the PostgreSQL session which causes an error. So, we recommend to set CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable only if you investigate errors of GPU kernels which happen on a certain query. The default configuration on RPM installation comments out the line of CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable. PostgreSQL server process should have unlimited Max core file size configuration, after the next restart. You can check it as follows. # cat /proc/<PID of postmaster>/limits Limit Soft Limit Hard Limit Units : : : : Max core file size unlimited unlimited bytes : : : : Installation of debuginfo package # yum install postgresql10-debuginfo pg_strom-PG10-debuginfo : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: pg_strom-PG10-debuginfo x86_64 1.9-180301.el7 heterodb-debuginfo 766 k postgresql10-debuginfo x86_64 10.3-1PGDG.rhel7 pgdg10 9.7 M Transaction Summary ================================================================================ Install 2 Packages : Installed: pg_strom-PG10-debuginfo.x86_64 0:1.9-180301.el7 postgresql10-debuginfo.x86_64 0:10.3-1PGDG.rhel7 Complete! Checking the back-trace on CPU side The kernel parameter kernel.core_pattern and kernel.core_uses_pid determine the path where crash dump is written out. It is usually created on the current working directory of the process, check /var/lib/pgdata where the database cluster is deployed, if you start PostgreSQL server using systemd. Once core.<PID> file gets generated, you can check its back-trace to reach system crash using gdb . gdb speficies the core file by -c option, and the crashed program by -f option. # gdb -c /var/lib/pgdata/core.134680 -f /usr/pgsql-10/bin/postgres GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7_4.1 : (gdb) bt #0 0x00007fb942af3903 in __epoll_wait_nocancel () from /lib64/libc.so.6 #1 0x00000000006f71ae in WaitEventSetWaitBlock (nevents=1, occurred_events=0x7ffee51e1d70, cur_timeout=-1, set=0x2833298) at latch.c:1048 #2 WaitEventSetWait (set=0x2833298, timeout=timeout@entry-1, occurred_events=occurred_events@entry0x7ffee51e1d70, nevents=nevents@entry1, wait_event_info=wait_event_info@entry100663296) at latch.c:1000 #3 0x00000000006210fb in secure_read (port=0x2876120, ptr=0xcaa7e0 <PqRecvBuffer>, len=8192) at be-secure.c:166 #4 0x000000000062b6e8 in pq_recvbuf () at pqcomm.c:963 #5 0x000000000062c345 in pq_getbyte () at pqcomm.c:1006 #6 0x0000000000718682 in SocketBackend (inBuf=0x7ffee51e1ef0) at postgres.c:328 #7 ReadCommand (inBuf=0x7ffee51e1ef0) at postgres.c:501 #8 PostgresMain (argc=<optimized out>, argv=argv@entry0x287bb68, dbname=0x28333f8 \"postgres\", username=<optimized out>) at postgres.c:4030 #9 0x000000000047adbc in BackendRun (port=0x2876120) at postmaster.c:4405 #10 BackendStartup (port=0x2876120) at postmaster.c:4077 #11 ServerLoop () at postmaster.c:1755 #12 0x00000000006afb7f in PostmasterMain (argc=argc@entry3, argv=argv@entry0x2831280) at postmaster.c:1363 #13 0x000000000047bbef in main (argc=3, argv=0x2831280) at main.c:228 bt command of gdb displays the backtrace. In this case, I sent SIGSEGV signal to the PostgreSQL backend which is waiting for queries from the client for intentional crash, the process got crashed at __epoll_wait_nocancel invoked by WaitEventSetWait . Checking the backtrace on GPU Crash dump of GPU kernel is generated on the current working directory of PostgreSQL server process, unless you don't specify the path using CUDA_COREDUMP_FILE environment variable explicitly. Check /var/lib/pgdata where the database cluster is deployed, if systemd started PostgreSQL. Dump file will have the following naming convension. core_<timestamp>_<hostname>_<PID>.nvcudmp Note that the dump-file of GPU kernel contains no debug information like symbol information in the default configuration. It is nearly impossible to investigate the problem, so enable inclusion of debug information for the GPU programs generated by PG-Strom, as follows. Also note than we don't recommend to turn on the configuration for daily usage, because it makes query execution performan slow down. Turn on only when you investigate the troubles. nvme=# set pg_strom.debug_jit_compile_options = on; SET You can check crash dump of the GPU kernel using cuda-gdb command. # /usr/local/cuda/bin/cuda-gdb NVIDIA (R) CUDA Debugger 9.1 release Portions Copyright (C) 2007-2017 NVIDIA Corporation : For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\". (cuda-gdb) Run cuda-gdb command, then load the crash dump file above using target command on the prompt. (cuda-gdb) target cudacore /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp Opening GPU coredump: /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp [New Thread 216240] CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x7ff4dc82f930 (cuda_gpujoin.h:1159) [Current focus set to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 1159 while (khitem && khitem->hash != hash_value) You can check backtrace where the error happened on GPU kernel using bt command. (cuda-gdb) bt #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 #1 0x00007ff4dc9428f0 in gpujoin_main<<<(30,1,1),(256,1,1)>>> (kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, kds_dst=0x7fe9e8800030, kparams_gpreagg=0x0) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1347 Please check CUDA Toolkit Documentation - CUDA-GDB for more detailed usage of cuda-gdb command.","title":"Trouble Shooting"},{"location":"troubles/#identify-the-problem","text":"In case when a particular workloads produce problems, it is the first step to identify which stuff may cause the problem. Unfortunately, much smaller number of developer supports the PG-Strom development community than PostgreSQL developer's community, thus, due to the standpoint of software quality and history, it is a reasonable estimation to suspect PG-Strom first. The pg_strom.enabled parameter allows to turn on/off all the functionality of PG-Strom at once. The configuration below disables PG-Strom, thus identically performs with the standard PostgreSQL. # SET pg_strom.enabled = off; In addition, we provide parameters to disable particular execution plan like GpuScan, GpuJoin and GpuPreAgg. See references/GUC Parameters for more details.","title":"Identify the problem"},{"location":"troubles/#collecting-crash-dump","text":"Crash dump is very helpful for analysis of serious problems which lead system crash for example. This session introduces the way to collect crash dump of the PostgreSQL and PG-Strom process (CPU side) and PG-Strom's GPU kernel, and show the back trace on the serious problems.","title":"Collecting crash dump"},{"location":"troubles/#add-configuration-on-postgresql-startup","text":"For generation of crash dump (CPU-side) on process crash, you need to change the resource limitation of the operating system for size of core file PostgreSQL server process can generate. For generation of crash dump (GPU-size) on errors of GPU kernel, PostgreSQL server process has CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable, and its value has 1 . You can put a configuration file at /etc/systemd/system/postgresql-<version>.service.d/ when PostgreSQL is kicked by systemd. In case of RPM installation, a configuration file pg_strom.conf is also installed on the directory, and contains the following initial configuration. [Service] LimitNOFILE=65536 LimitCORE=infinity #Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 In CUDA 9.1, it usually takes more than several minutes to generate crash dump of GPU kernel, and it entirely stops response of the PostgreSQL session which causes an error. So, we recommend to set CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable only if you investigate errors of GPU kernels which happen on a certain query. The default configuration on RPM installation comments out the line of CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable. PostgreSQL server process should have unlimited Max core file size configuration, after the next restart. You can check it as follows. # cat /proc/<PID of postmaster>/limits Limit Soft Limit Hard Limit Units : : : : Max core file size unlimited unlimited bytes : : : :","title":"Add configuration on PostgreSQL startup"},{"location":"troubles/#installation-of-debuginfo-package","text":"# yum install postgresql10-debuginfo pg_strom-PG10-debuginfo : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: pg_strom-PG10-debuginfo x86_64 1.9-180301.el7 heterodb-debuginfo 766 k postgresql10-debuginfo x86_64 10.3-1PGDG.rhel7 pgdg10 9.7 M Transaction Summary ================================================================================ Install 2 Packages : Installed: pg_strom-PG10-debuginfo.x86_64 0:1.9-180301.el7 postgresql10-debuginfo.x86_64 0:10.3-1PGDG.rhel7 Complete!","title":"Installation of debuginfo package"},{"location":"troubles/#checking-the-back-trace-on-cpu-side","text":"The kernel parameter kernel.core_pattern and kernel.core_uses_pid determine the path where crash dump is written out. It is usually created on the current working directory of the process, check /var/lib/pgdata where the database cluster is deployed, if you start PostgreSQL server using systemd. Once core.<PID> file gets generated, you can check its back-trace to reach system crash using gdb . gdb speficies the core file by -c option, and the crashed program by -f option. # gdb -c /var/lib/pgdata/core.134680 -f /usr/pgsql-10/bin/postgres GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7_4.1 : (gdb) bt #0 0x00007fb942af3903 in __epoll_wait_nocancel () from /lib64/libc.so.6 #1 0x00000000006f71ae in WaitEventSetWaitBlock (nevents=1, occurred_events=0x7ffee51e1d70, cur_timeout=-1, set=0x2833298) at latch.c:1048 #2 WaitEventSetWait (set=0x2833298, timeout=timeout@entry-1, occurred_events=occurred_events@entry0x7ffee51e1d70, nevents=nevents@entry1, wait_event_info=wait_event_info@entry100663296) at latch.c:1000 #3 0x00000000006210fb in secure_read (port=0x2876120, ptr=0xcaa7e0 <PqRecvBuffer>, len=8192) at be-secure.c:166 #4 0x000000000062b6e8 in pq_recvbuf () at pqcomm.c:963 #5 0x000000000062c345 in pq_getbyte () at pqcomm.c:1006 #6 0x0000000000718682 in SocketBackend (inBuf=0x7ffee51e1ef0) at postgres.c:328 #7 ReadCommand (inBuf=0x7ffee51e1ef0) at postgres.c:501 #8 PostgresMain (argc=<optimized out>, argv=argv@entry0x287bb68, dbname=0x28333f8 \"postgres\", username=<optimized out>) at postgres.c:4030 #9 0x000000000047adbc in BackendRun (port=0x2876120) at postmaster.c:4405 #10 BackendStartup (port=0x2876120) at postmaster.c:4077 #11 ServerLoop () at postmaster.c:1755 #12 0x00000000006afb7f in PostmasterMain (argc=argc@entry3, argv=argv@entry0x2831280) at postmaster.c:1363 #13 0x000000000047bbef in main (argc=3, argv=0x2831280) at main.c:228 bt command of gdb displays the backtrace. In this case, I sent SIGSEGV signal to the PostgreSQL backend which is waiting for queries from the client for intentional crash, the process got crashed at __epoll_wait_nocancel invoked by WaitEventSetWait .","title":"Checking the back-trace on CPU side"},{"location":"troubles/#checking-the-backtrace-on-gpu","text":"Crash dump of GPU kernel is generated on the current working directory of PostgreSQL server process, unless you don't specify the path using CUDA_COREDUMP_FILE environment variable explicitly. Check /var/lib/pgdata where the database cluster is deployed, if systemd started PostgreSQL. Dump file will have the following naming convension. core_<timestamp>_<hostname>_<PID>.nvcudmp Note that the dump-file of GPU kernel contains no debug information like symbol information in the default configuration. It is nearly impossible to investigate the problem, so enable inclusion of debug information for the GPU programs generated by PG-Strom, as follows. Also note than we don't recommend to turn on the configuration for daily usage, because it makes query execution performan slow down. Turn on only when you investigate the troubles. nvme=# set pg_strom.debug_jit_compile_options = on; SET You can check crash dump of the GPU kernel using cuda-gdb command. # /usr/local/cuda/bin/cuda-gdb NVIDIA (R) CUDA Debugger 9.1 release Portions Copyright (C) 2007-2017 NVIDIA Corporation : For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\". (cuda-gdb) Run cuda-gdb command, then load the crash dump file above using target command on the prompt. (cuda-gdb) target cudacore /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp Opening GPU coredump: /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp [New Thread 216240] CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x7ff4dc82f930 (cuda_gpujoin.h:1159) [Current focus set to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 1159 while (khitem && khitem->hash != hash_value) You can check backtrace where the error happened on GPU kernel using bt command. (cuda-gdb) bt #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 #1 0x00007ff4dc9428f0 in gpujoin_main<<<(30,1,1),(256,1,1)>>> (kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, kds_dst=0x7fe9e8800030, kparams_gpreagg=0x0) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1347 Please check CUDA Toolkit Documentation - CUDA-GDB for more detailed usage of cuda-gdb command.","title":"Checking the backtrace on GPU"}]}