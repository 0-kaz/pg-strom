/*
 * opencl_gpupreagg.h
 *
 * Preprocess of aggregate using GPU acceleration, to reduce number of
 * rows to be processed by CPU; including the Sort reduction.
 * --
 * Copyright 2011-2014 (C) KaiGai Kohei <kaigai@kaigai.gr.jp>
 * Copyright 2014 (C) The PG-Strom Development Team
 *
 * This software is an extension of PostgreSQL; You can use, copy,
 * modify or distribute it under the terms of 'LICENSE' included
 * within this package.
 */
#ifndef OPENCL_GPUPREAGG_H
#define OPENCL_GPUPREAGG_H

/*
 * Sequential Scan using GPU/MIC acceleration
 *
 * It packs a kern_parambuf and kern_resultbuf structure within a continuous
 * memory ares, to transfer (usually) small chunk by one DMA call.
 *
 * +----------------+  -----
 * | status         |    ^
 * +----------------+    |
 * | rindex_len     |    |
 * +----------------+    |
 * | kern_parambuf  |    |
 * | +--------------+    |
 * | | length   o--------------+
 * | +--------------+    |     | kern_row_map is located just after
 * | | nparams      |    |     | the kern_parambuf (because of DMA
 * | +--------------+    |     | optimization), so head address of
 * | | poffset[0]   |    |     | kern_gpuscan + parambuf.length
 * | | poffset[1]   |    |     | points kern_row_map.
 * | |    :         |    |     |
 * | | poffset[M-1] |    |     |
 * | +--------------+    |     |
 * | | variable     |    |     |
 * | | length field |    |     |
 * | | for Param /  |    |     |
 * | | Const values |    |     |
 * | |     :        |    |     |
 * +-+--------------+ <--------+
 * | kern_row_map   |    |
 * | +--------------+    |
 * | | nvalids (=N) |    |
 * | +--------------+    |
 * | | rindex[0]    |    |
 * | | rindex[1]    |    |
 * | |    :         |    |
 * | | rindex[N]    |    V
 * +-+--------------+  -----
 * | rindex[] for   |    ^
 * |  working of    |    |
 * |  bitonic sort  |  device onlye memory
 * |       :        |    |
 * |       :        |    V
 * +----------------+  -----
 */

typedef struct
{
	cl_int			status;		/* result of kernel execution */
	cl_int			sortbuf_len;/* length of sorting rindex[] that holds
								 * row-index being sorted; must be length
								 * of get_next_log2(nitems)
								 */
	char			__padding[8];	/* align to 128bits */
	kern_parambuf	kparams;
	/*
	 * kern_row_map and rindexp[] for sorting will be here
	 */
} kern_gpupreagg;

/* macro definitions to reference packed values */
#define KERN_GPUPREAGG_PARAMBUF(kgpreagg)				\
	((__global kern_parambuf *)(&(kgpreagg)->kparams))
#define KERN_GPUPREAGG_PARAMBUF_LENGTH(kgpreagg)		\
	(KERN_GPUPREAGG_PARAMBUF(kgpreagg)->length)
#define KERN_GPUPREAGG_KROWMAP(kgpreagg)				\
	((__global kern_row_map *)							\
	 ((__global char *)(kgpreagg) +						\
	  STROMALIGN(offsetof(kern_gpupreagg, kparams) +	\
				 KERN_GPUPREAGG_PARAMBUF_LENGTH(kgpreagg))))
#define KERN_GPUPREAGG_SORT_RINDEX(kgpreagg)			\
	(KERN_GPUPREAGG_KROWMAP(kgpreagg)->rindex +			\
	 (KERN_GPUPREAGG_KROWMAP(kgpreagg)->nvalids < 0		\
	  ? 0 : KERN_GPUPREAGG_KROWMAP(kgpreagg)->nvalids))
#define KERN_GPUPREAGG_BUFFER_SIZE(kgpreagg)			\
	((uintptr_t)(KERN_GPUPREAGG_SORT_RINDEX(kgpreagg) +	\
				 (kgpreagg)->sortbuf_len) -				\
	 (uintptr_t)(kgpreagg))
#define KERN_GPUPREAGG_DMASEND_OFFSET(kgpreagg)			0
#define KERN_GPUPREAGG_DMASEND_LENGTH(kgpreagg)			\
	((uintptr_t)KERN_GPUPREAGG_SORT_RINDEX(kgpreagg) -	\
	 (uintptr_t)(kgpreagg))
#define KERN_GPUPREAGG_DMARECV_OFFSET(kgpreagg)			\
	offsetof(kern_gpupreagg, status)
#define KERN_GPUPREAGG_DMARECV_LENGTH(kgpreagg)			\
	sizeof(cl_uint)

/*
 * NOTE: pagg_datum is a set of information to calculate running total.
 * group_id indicates which group does this work-item belong to, instead
 * of gpupreagg_keycomp().
 * isnull indicates whether the current running total is NULL, or not.
 * XXX_val is a running total itself.
 */
typedef struct
{
	cl_uint			group_id;
	cl_char			isnull;
	cl_char			__padding__[3];
	union {
		cl_uint		int_val;
		cl_ulong	long_val;
		cl_float	float_val;
		cl_double	double_val;
	};
} pagg_datum;

#ifdef OPENCL_DEVICE_CODE

/*
 * comparison function - to be generated by PG-Strom on the fly
 *
 * It compares two records indexed by 'x_index' and 'y_index' on the supplied
 * kern_data_store, then returns -1 if record[X] is less than record[Y],
 * 0 if record[X] is equivalent to record[Y], or 1 if record[X] is greater
 * than record[Y].
 * (auto generated function)
 */
static cl_int
gpupreagg_keycomp(__private cl_int *errcode,
				  __global kern_data_store *kds,
				  __global kern_toastbuf *ktoast,
				  size_t x_index,
				  size_t y_index);
/*
 * calculation function - to be generated by PG-Strom on the fly
 *
 * It updates the supplied 'accum' value by 'newval' value. Both of data
 * structure is expected to be on the local memory.
 * (auto generated function)
 */
static void
gpupreagg_aggcalc(__private cl_int *errcode,
				  cl_int resno,
				  __local pagg_datum *accum,
				  __local pagg_datum *newval);

/*
 * translate a kern_data_store (input) into an output form
 * (auto generated function)
 */
static void
gpupreagg_projection(__private cl_int *errcode,
					 __global kern_data_store *kds_in,
					 __global kern_data_store *kds_out,
					 __global kern_toastbuf *ktoast,
					 size_t kds_index);

/*
 * load the data from kern_data_store to pagg_datum structure
 */
static void
gpupreagg_data_load(__local pagg_datum *pdatum,
					__private cl_int *errcode,
					__global kern_data_store *kds,
					__global kern_toastbuf *ktoast,
					cl_uint colidx, cl_uint rowidx)
{
	kern_colmeta	cmeta;

	if (colidx >= kds->ncols)
	{
		if (!StromErrorIsSignificant(*errcode))
			*errcode = StromError_DataStoreCorruption;
		return;
	}
	cmeta = kds->colmeta[colidx];
	/*
	 * Right now, expected data length for running total of partial aggregate
	 * are 4, or 8. Elasewhere, it may be a bug.
	 */
	if (cmeta.attlen == sizeof(cl_uint))		/* also, cl_float */
	{
		__global cl_uint   *addr = kern_get_datum(kds,ktoast,colidx,rowidx);
		if (!addr)
			pdatum->isnull	= true;
		else
		{
			pdatum->isnull	= false;
			pdatum->int_val	= *addr;
		}
	}
	else if (cmeta.attlen == sizeof(cl_ulong))	/* also, cl_double */
	{
		__global cl_ulong  *addr = kern_get_datum(kds,ktoast,colidx,rowidx);
		if (!addr)
			pdatum->isnull	= true;
		else
		{
			pdatum->isnull	= false;
			pdatum->long_val= *addr;
		}
	}
	else
	{
		if (!StromErrorIsSignificant(*errcode))
			*errcode = StromError_DataStoreCorruption;
	}
}

/*
 * store the data from pagg_datum structure to kern_data_store
 */
static void
gpupreagg_data_store(__local pagg_datum *pdatum,
					 __private cl_int *errcode,
					 __global kern_data_store *kds,
					 __global kern_toastbuf *ktoast,
					 cl_uint colidx, cl_uint rowidx,
					 __local void *local_workbuf)
{
	kern_colmeta	cmeta;

	if (colidx >= kds->ncols)
	{
		if (!StromErrorIsSignificant(*errcode))
			*errcode = StromError_DataStoreCorruption;
		return;
	}
	cmeta = kds->colmeta[colidx];
	/*
	 * Right now, expected data length for running total of partial aggregate
	 * are 4, or 8. Elasewhere, it may be a bug.
	 */
	if (cmeta.attlen == sizeof(cl_uint))		/* also, cl_float */
	{
		pg_int4_t	temp;

		temp.isnull	= pdatum->isnull;
		temp.value	= pdatum->int_val;
		pg_int4_vstore(kds, ktoast, errcode, colidx, rowidx, temp, local_workbuf);
	}
	else if (cmeta.attlen == sizeof(cl_ulong))	/* also, cl_double */
	{
		pg_int8_t	temp;

		temp.isnull	= pdatum->isnull;
		temp.value	= pdatum->long_val;
		pg_int8_vstore(kds, ktoast, errcode, colidx, rowidx, temp, local_workbuf);
	}
	else
	{
		if (!StromErrorIsSignificant(*errcode))
			*errcode = StromError_DataStoreCorruption;
	}
}

/*
 * gpupreagg_preparation - It translaes an input kern_data_store (that
 * reflects outer relation's tupdesc) into the form of running total
 * and final result of gpupreagg (that reflects target-list of GpuPreAgg).
 */
__kernel void
gpupreagg_preparation(__global kern_gpupreagg *kgpreagg,
					  __global kern_data_store *kds_in,
					  __global kern_data_store *kds_out,
					  __global kern_toastbuf *ktoast,
					  __local void *local_memory)
{
	__global kern_parambuf *kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);
	__global cl_int		   *rindex = KERN_GPUPREAGG_ROW_INDEX(kgpreagg);
	cl_int					errcode = StromError_Success;

	if (get_global_id(0) < kds_in->nitems)
		gpupreagg_projection(&errcode,
							 kds_in, kds_out, ktoast,
							 get_global_id(0));

	kern_writeback_error_status(&kgpreagg->status, errcode, local_workbuf);
}

/*
 * gpupreagg_reduction - entrypoint of the main logic for GpuPreAgg.
 * The both of kern_data_store have identical form that reflects running 
 * total and final results. rindex will show the sorted order according
 * to the gpupreagg_keycomp() being constructed on the fly.
 * This function makes grouping at first, then run data reduction within
 * the same group. 
 */
__kernel void
gpupreagg_reduction(__global kern_gpupreagg *kgpreagg,
					__global kern_data_store *kds_src,
					__global kern_data_store *kds_dst,
					__global kern_toastbuf *ktoast,
					__local void *local_memory)
{
	__global kern_parambuf	   *kparams	= KERN_GPUPREAGG_PARAMBUF(kgpreagg);
	__global cl_int			   *rindex  = KERN_GPUPREAGG_SORT_RINDEX(kgpreagg);
	__local struct pagg_datum  *l_data  = local_workbuf;
	__local void			   *l_workbuf = (void *)&l_data[get_local_size(0)];

	cl_int ncols		= kds_src->ncols;
	cl_int nrows		= kds_src->nrows;
	cl_int errcode		= StromError_Success;

	cl_int localID     = get_local_id(0);
	cl_int globalID        = get_global_id(0);
    cl_int localSize   = get_local_size(0);

	cl_int prtID		= globalID / localSize;	/* partition ID */
	cl_int prtSize		= localSize;			/* partition Size */
	cl_int prtMask		= prtSize - 1;			/* partition Mask */
	cl_int prtPos		= prtID * prtSize;		/* partition Position */

	cl_int localEntry  = (prtPos+prtSize < nrows) ? prtSize : (nrows-prtPos);
	/* Check no data for this work group */
	if(localEntry <= 0) {
		goto out;
	}

	/* Generate group id of local work group. */
	cl_int groupID;
	cl_int ngroups;
	{
		cl_int isNewID = 0;

		if(localID == 0)
		{
	        isNewID = 1;
		}
		else if(localID < localEntry)
		{
			int rv = gpupreagg_keycomp(&errcode, kds_src, ktoast,
									   rindex[globalID-1], rindex[globalID]);
			isNewID = (rv != 0) ? 1 : 0;
		}
		groupID = arithmetic_stairlike_add(isNewID, local_workbuf, &ngroups);
	}

	/* allocation of result buffer */
	__local cl_uint base;
	{
		if (get_local_id(0) == 0)
			base = atomic_add(&kds_dst->nitems, ngroups);
		barrier(CLK_LOCAL_MEM_FENCE);

		if (kds_dst->nrooms <= base + nitems) {
			errcode = StromError_DataStoreNoSpace;
			goto out;
		}
	}
	/* Aggregate for each item. */
	for (cl_int cindex=0; cindex < ncols; cindex++)
	{
		/*
		 * TODO: needs to separate cases according to the column number.
		 * it shall be informed to kernel function via KPARAM_3
		 */

		/* Load aggregate item */
		l_data[localID].group_id = -1;
		if(localID < localEntry) {
			gpupreagg_data_load(&l_data[localID], &errcode, kds_src, ktoast,
								cindex, rindex[globalID]);
			l_data[localID].group_id = groupID;
		}
		barrier(CLK_LOCAL_MEM_FENCE);

		// Reduction
		for(int unitSize=2; unitSize<=prtSize; unitSize*=2) {
			if(localID % unitSize == unitSize/2  &&  localID < localEntry) {
				cl_int  dstID;

				dstID = localID - unitSize/2;
				if(l_data[localID].group_id == l_data[dstID].group_id) {
					// Marge this aggregate data to lower.
					gpupreagg_aggcalc(&errcode, cindex,
									  &l_data[dstID], &l_data[localID]);
					l_data[localID].group_id = -1;
				}
				barrier(CLK_LOCAL_MEM_FENCE);

				if(l_data[localID].group_id != -1  &&
				   localID + unitSize/2 < localEntry) {
					dstID = localID + unitSize/2;
					if(l_data[localID].group_id == l_data[dstID].group_id) {
						// Marge this aggregate data to upper.
						gpupreagg_aggcalc(&errcode, cindex,
										  &l_data[dstID], &l_data[localID]);
						l_data[localID].group_id = -1;
					}
				}
				barrier(CLK_LOCAL_MEM_FENCE);
			}
		}
		// write back aggregate data
		if(l_data[localID].group_id != -1) {
			gpupreagg_data_store(&l_data[localID], &errcode, kds_dst, ktoast,
								 cindex, base + groupID, l_workbuf);
		}
    }
out:
	kern_writeback_error_status(&kgpreagg->status, errcode, local_workbuf);
}

/*
 * gpupreagg_bitonic_local
 *
 * It tries to apply each steps of bitonic-sorting until its unitsize
 * reaches the workgroup-size (that is expected to power of 2).
 */
__kernel void
gpupreagg_bitonic_local(__global kern_gpupreagg *kgpreagg,
						__global kern_data_store *kds,
						__global kern_toastbuf *ktoast,
						__local void *local_memory)
{
	__global kern_parambuf	*kparams  = KERN_GPUPREAGG_PARAMBUF(kgpreagg);
	__global cl_int			*rindex	  = KERN_GPUPREAGG_SORT_RINDEX(kgpreagg);
	__local  cl_int			*localIdx = local_workbuf;

	cl_int nrows		= kds->nrows;
	cl_int errcode		= StromError_Success;

    cl_int localID		= get_local_id(0);
    cl_int globalID		= get_global_id(0);
    cl_int localSize	= get_local_size(0);

    cl_int prtID		= globalID / localSize; /* partition ID */
    cl_int prtSize		= localSize * 2;		/* partition Size */
    cl_int prtMask		= prtSize - 1;			/* partition Mask */
    cl_int prtPos		= prtID * prtSize;		/* partition Position */

    cl_int localEntry	= ((prtPos + prtSize < nrows)
						   ? prtSize
						   : (nrows - prtPos));

    // create row index and then store to localIdx
    if(localID < localEntry)
		localIdx[localID] = prtPos + localID;

    if(localSize + localID < localEntry)
		localIdx[localSize + localID] = prtPos + localSize + localID;

    barrier(CLK_LOCAL_MEM_FENCE);


	// bitonic sort
	for(int blockSize=2; blockSize<=prtSize; blockSize*=2)
	{
		int blockMask		= blockSize - 1;
		int halfBlockSize	= blockSize / 2;
		int halfBlockMask	= halfBlockSize -1;

		for(int unitSize=blockSize; 2<=unitSize; unitSize/=2)
		{
			int unitMask		= unitSize - 1;
			int halfUnitSize	= unitSize / 2;
			int halfUnitMask	= halfUnitSize - 1;

			bool reversing	= unitSize == blockSize ? true : false;
			int idx0 = ((localID / halfUnitSize) * unitSize
						+ localID % halfUnitSize);
			int idx1 = ((reversing == true)
						? ((idx0 & ~unitMask) | (~idx0 & unitMask))
						: (halfUnitSize + idx0));

			if(idx1 < localEntry) {
				cl_int pos0 = localIdx[idx0];
				cl_int pos1 = localIdx[idx1];
				cl_int rv   = gpupreagg_keycomp(&errcode, kds, ktoast,
												pos0, pos1);

				if(0 < rv) {
					// swap
					localIdx[idx0] = pos1;
					localIdx[idx1] = pos0;
				}
			}
			barrier(CLK_LOCAL_MEM_FENCE);
		}
    }

    if(localID < localEntry)
		rindex[prtPos + localID] = localIdx[localID];

    if(localSize + localID < localEntry)
		rindex[prtPos + localSize + localID] = localIdx[localSize + localID];

	kern_writeback_error_status(&kgpreagg->status, errcode, local_workbuf);
}



/*
 * gpupreagg_bitonic_step
 *
 * It tries to apply individual steps of bitonic-sorting for each step,
 * but does not have restriction of workgroup size. The host code has to
 * control synchronization of each step not to overrun.
 */
__kernel void
gpupreagg_bitonic_step(__global kern_gpupreagg *kgpreagg,
					   cl_int bitonic_unitsz,
					   __global kern_data_store *kds,
					   __global kern_toastbuf *ktoast,
					   __local void *local_memory)
{
	__global kern_parambuf	*kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);
	__global cl_int			*rindex	 = KERN_GPUPREAGG_SORT_RINDEX(kgpreagg);

	cl_int	nrows	  = kds->nrows;
	cl_bool reversing = (bitonic_unitsz < 0 ? true : false);
	size_t	unitsz    = (bitonic_unitsz < 0
						 ? 1U << -bitonic_unitsz
						 : 1U << bitonic_unitsz);
	cl_int	errcode	  = StromError_Success;

	cl_int	threadID		= get_global_id(0);
	cl_int	halfUnitSize	= unitsz / 2;
	cl_int	unitMask		= unitsz - 1;

	cl_int	idx0;
	cl_int	idx1;

	idx0 = (threadID / halfUnitSize) * unitsz + threadID % halfUnitSize;
	idx1 = (reversing
			? ((idx0 & ~unitMask) | (~idx0 & unitMask))
			: (idx0 + halfUnitSize));
	if(nrows <= idx1)
		return;

	cl_int	pos0	= rindex[idx0];
	cl_int	pos1	= rindex[idx1];
	cl_int	rv;

	rv = gpupreagg_keycomp(&errcode, kds, ktoast, pos0, pos1);
	if(0 < rv) {
		/* Swap */
		rindex[idx0] = pos1;
		rindex[idx1] = pos0;
	}

	kern_writeback_error_status(&kgpreagg->status, errcode, local_workbuf);
}

/*
 * gpupreagg_bitonic_merge
 *
 * It handles the merging step of bitonic-sorting if unitsize becomes less
 * than or equal to the workgroup size.
 */
__kernel void
gpupreagg_bitonic_merge(__global kern_gpupreagg *kgpreagg,
						__global kern_data_store *kds,
						__global kern_toastbuf *ktoast,
						__local void *local_memory)
{
	__global kern_parambuf	*kparams = KERN_GPUPREAGG_PARAMBUF(kgpreagg);
	__global cl_int			*rindex	 = KERN_GPUPREAGG_SORT_RINDEX(kgpreagg);
	__local	 cl_int			localIdx = local_workbuf;

	cl_int nrows		= kds->nrows;
	cl_int errcode		= StromError_Success;

    cl_int localID		= get_local_id(0);
    cl_int globalID		= get_global_id(0);
    cl_int localSize	= get_local_size(0);

    cl_int prtID		= globalID / localSize; /* partition ID */
    cl_int prtSize		= localSize * 2;		/* partition Size */
    cl_int prtMask		= prtSize - 1;			/* partition Mask */
    cl_int prtPos		= prtID * prtSize;		/* partition Position */

    cl_int localEntry	= (prtPos+prtSize < nrows) ? prtSize : (nrows-prtPos);


    // load index to localIdx
    if(localID < localEntry)
		localIdx[localID] = rindex[prtPos + localID];

    if(localSize + localID < localEntry)
		localIdx[localSize + localID] = rindex[prtPos + localSize + localID];

    barrier(CLK_LOCAL_MEM_FENCE);


	// marge sorted block
	int blockSize		= prtSize;
	int blockMask		= blockSize - 1;
	int halfBlockSize	= blockSize / 2;
	int halfBlockMask	= halfBlockSize -1;

	for(int unitSize=blockSize; 2<=unitSize; unitSize/=2)
	{
		int unitMask		= unitSize - 1;
		int halfUnitSize	= unitSize / 2;
		int halfUnitMask	= halfUnitSize - 1;

		int idx0 = localID / halfUnitSize * unitSize + localID % halfUnitSize;
		int idx1 = halfUnitSize + idx0;

		if(idx1 < localEntry) {
			cl_int pos0 = localIdx[idx0];
			cl_int pos1 = localIdx[idx1];
			cl_int rv = gpupreagg_keycomp(&errcode, kds, ktoast, pos0, pos1);

			if(0 < rv) {
				// swap
				localIdx[idx0] = pos1;
				localIdx[idx1] = pos0;
			}
		}
		barrier(CLK_LOCAL_MEM_FENCE);
    }

    if(localID < localEntry)
		rindex[prtPos + localID] = localIdx[localID];

    if(localSize + localID < localEntry)
		rindex[prtPos + localSize + localID] = localIdx[localSize + localID];

	kern_writeback_error_status(&kgpreagg->status, errcode, local_workbuf);
}




/*
 * gpupreagg_check_next - decision making whether we need to run next
 * reduction step actually.
 */
__kernel void
gpupreagg_check_next(__global kern_gpupreagg *kgpreagg,
					 __global kern_data_store *kds_old,
					 __global kern_data_store *kds_new)
{
#if 0
	/* used this function ? */
	size_t	nrows_old	= kds_old->nrows;
	size_t	nrows_new	= kds_new->nrows;

	bool	needNextReduction;

	if(nrows_old->flag_needNextReduction == false) {
		needNextReduction = false;
	} else {
		if((nrows_old - nrows_new) < nrows_old / 10) {
			needNextReduction = false;
		} else {
			needNextReduction = true;
		}
	}

	nrows_new->flag_needNextReduction = true;
#endif
}

/*
 * pg_common_vstore_putnull() - utility function to set null bitmask
 * on the appropriate position of the target kern_data_store.
 * It internally uses reduction operation using local memory, so all
 * the work-item has to be called towards this function.
 * It returns width of null-bitmap on the required attribute, may be
 * 0 if column has NOT NULL constratint.
 */
static cl_int
pg_common_vstore_putnull(__private cl_int *errcode,
						 __global kern_data_store *kds,
						 cl_uint colidx,
						 cl_uint rowidx,
						 bool isnull,
						 __local void *local_workbuf)
{
	kern_colmeta	cmeta;
	cl_uint			offset;

	/* only column-store can be written in the kernel space */
	if (!kds->column_form)
	{
		if (!StromErrorIsSignificant(*errcode))
			*errcode = StromError_DataStoreCorruption;
		return -1;
	}

	/* out of range? */
	if (colidx >= kds->ncols)
	{
		if (!StromErrorIsSignificant(*errcode))
			*errcode = StromError_DataStoreOutOfRange;
		return -1;
	}

	/* why do you try to store the datum to this column? */
	cmeta = kds->colmeta[colidx];
	if (!cmeta.attvalid)
		return -1;
	offset = cmeta.cs_offset;

	/*
	 * setting up null-bitmask. we should not use atomic operation here,
	 * because this function shall be called towards all the input records,
	 * so its performance loss is not ignorable.
	 */
	if (cmeta.attnotnull)
	{
		/* NULL is unacceptable with NOT NULL constraint */
		if (isnull)
		{
			if (!StromErrorIsSignificant(*errcode))
				*errcode = StromError_DataStoreCorruption;
			return -1;
		}
	}
	else
	{
		__local cl_uint *nullmask = (__lobal cl_uint *) local_workbuf;
		cl_uint		x;

		nullmask[get_local_id(0)] = (!isnull ? (1 << (rowid & 0x1f)) : 0);
		barrier(CLK_LOCAL_MEM_FENCE);

		for (x=2; x <= sizeof(cl_uint) * BITS_PER_BYTE; x <<= 1)
		{
			if ((get_local_id(0) & (x - 1)) == 0 &&
				(rowid + (x >> 1)) < kds->nitems)
				nullmask[get_local_id(0)]
					|= nullmask[get_local_id(0) + (x >> 1)];
			barrier(CLK_LOCAL_MEM_FENCE);
		}

		if ((get_local_id(0) & 0x001f) == 0 && rowid < kds->nitems)
		{
			__global cl_uint *addr = (__global cl_uint *)
				((__global cl_char *) kds + offset);

			addr[rowid >> 5] = nullmask[get_local_id(0)];
		}
		return STROMALIGN(bitmaplen(kds->nitems));
	}
	return 0;
}

#define STROMCL_SIMPLE_VARSTORE_TEMPLATE(NAME,BASE)			\
	static void												\
	pg_##NAME##_vstore(__global kern_data_store *kds,		\
					   __global kern_toastbuf *ktoast,		\
					   __private int *errcode,				\
					   cl_uint colidx,						\
					   cl_uint rowidx,						\
					   pg_##NAME##_t datum,					\
					   __local void *local_workbuf)			\
	{														\
		kern_colmeta	cmeta;								\
		cl_int			width;								\
		cl_int			cs_offset;							\
		__global BASE  *cs_addr;							\
															\
		width = pg_common_vstore_putnull(errcode, kds,		\
										 colidx, rowidx,	\
										 datum.isnull,		\
										 local_workbuf);	\
		if (width < 0)										\
			return;		/* something error! */				\
		if (rowidx >= kds->nitems)							\
		{													\
			if (!StromErrorIsSignificant(*errcode))			\
				*errcode = StromError_DataStoreOutOfRange;	\
		}													\
		/* OK, let's put fixed-length datum  */				\
		cmeta = kds->colmeta[colidx];						\
		cs_offset = cmeta.cs_offset + width;				\
		cs_addr = (__global BASE *)							\
			((__global char *) kds + cs_offset +			\
			 cmeta.attlen * rowidx);						\
		*cs_addr = datum.value;								\
	}

#define STROMCL_VARLENA_VARSTORE_TEMPLATE(NAME)				\
	static void												\
	pg_##NAME##_vstore(__global kern_data_store *kds,		\
					   __global kern_toastbuf *ktoast,		\
					   __private int *errcode,				\
					   cl_uint colidx,						\
					   cl_uint rowidx,						\
					   pg_##NAME##_t datum,					\
					   __local void *local_workbuf)			\
	{														\
		kern_colmeta		cmeta;							\
		cl_int				width;							\
		cl_int				cs_offset;						\
		cl_int				vl_offset;						\
		__global cl_uint   *cs_addr;						\
															\
		width = pg_common_vstore_putnull(errcode, kds,		\
										 colidx, rowidx,	\
										 datum.isnull,		\
										 local_workbuf);	\
		if (width < 0)										\
			return;		/* something error! */				\
		if (rowidx >= kds->nitems)							\
		{													\
			if (!StromErrorIsSignificant(*errcode))			\
				*errcode = StromError_DataStoreOutOfRange;	\
		}													\
		/* OK, let's put fixed-length datum  */				\
		cmeta = kds->colmeta[colidx];						\
		cs_offset = cmeta.cs_offset + width;				\
		cs_addr = (__global cl_uint *)						\
			((__global char *) kds + cs_offset +			\
			 sizeof(cl_uint) * rowidx);						\
		/* NOTE: right now, varlena datum is supported */	\
		/* as a grouping key only, so its attribute */		\
		/* number should not be changed, thus coldir[] */	\
		/* of toastbuf also should not be changed */		\
		vl_offset = (cl_uint)								\
			((uintptr_t)datum.value -						\
			 (ktoast->length == TOASTBUF_MAGIC				\
			  ? (uintptr_t)ktoast							\
			  : (uintptr_t)ktoast->coldir[colidx]));		\
		*cs_addr = vl_offset;								\
	}
#else
/*
 * special system parameter of gpupreagg
 * KPARAM_2 - kds_head of the source/target kern_data_store
 * KPARAM_3 - array of referenced and aggregated column index
 */
static inline kern_data_store *
KPARAM_GET_KDS_HEAD_DEST(kern_parambuf *kparams)
{
	bytea  *vl_datum = kparam_get_value(kparams, 2);

	if (!vl_datum)
		return NULL;
	return (kern_data_store *)VARDATA_ANY(vl_datum);
}




/* Host side representation of kern_gpupreagg. It can perform as a message
 * object of PG-Strom, has key of OpenCL device program, a source row/column
 * store and a destination kern_data_store.
 */
typedef struct
{
	pgstrom_message		msg;		/* = StromTag_GpuPreAgg */
	Datum				dprog_key;	/* key of device program */
	StromObject		   *rcstore;	/* source row/column store as input */
	kern_data_store	   *kds_dst;	/* result buffer of partial aggregate */
	kern_gpupreagg		kern;		/* kernel portion to be sent */
} pgstrom_gpupreagg;
#endif	/* OPENCL_DEVICE_CODE */
#endif	/* OPENCL_GPUPREAGG_H */
